{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26651ba6-c791-4d28-aa5d-fc848569b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bbdf54-7473-4fa7-9492-4a5bde4dcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7820a39-df35-43e5-81c2-1b6ac4a582c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "train_encoded = pd.get_dummies(train_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "X = train_encoded.drop(columns=[\"ID\", \"수요량\"]).values\n",
    "y = train_encoded[\"수요량\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a582749-8325-498b-8ec6-d39048893c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network architecture\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)  # Optional dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)  # Optional dropout\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96969fc3-a33a-4e0f-aa4c-0c9841f9af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "models = []\n",
    "test_predictions_all = []\n",
    "rmse_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f89745-2980-4b20-a901-996c144f63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 01312: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01323: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01334: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01345: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01356: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01367: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01378: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01389: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01400: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Early stopping in fold 1!\n",
      "Fold 1 RMSE: 172.65132423696747\n",
      "Fold 2\n",
      "Epoch 01085: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01182: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01212: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01272: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01283: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01294: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01305: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01316: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01327: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01338: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 01349: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 01360: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Early stopping in fold 2!\n",
      "Fold 2 RMSE: 162.5112736815714\n",
      "Fold 3\n",
      "Epoch 01210: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01294: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01371: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01382: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01393: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01404: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01415: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01426: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01437: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01448: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 01459: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Early stopping in fold 3!\n",
      "Fold 3 RMSE: 153.13223974064474\n",
      "Fold 4\n",
      "Epoch 01233: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01268: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01279: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01290: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01301: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01312: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01323: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01334: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01345: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01356: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Early stopping in fold 4!\n",
      "Fold 4 RMSE: 142.14970429405258\n",
      "Fold 5\n",
      "Epoch 01043: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01062: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01114: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01125: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01136: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01147: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01158: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01169: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01180: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01191: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 01202: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 01213: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 01224: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 01235: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 01246: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 01257: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Fold 5 RMSE: 145.4031124562744\n",
      "Fold 6\n",
      "Epoch 01111: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01149: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01214: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01247: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01258: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01269: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01280: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01291: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01302: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01313: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 01324: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 01335: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Early stopping in fold 6!\n",
      "Fold 6 RMSE: 157.88100014889633\n",
      "Fold 7\n",
      "Epoch 01293: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01373: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01403: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01458: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01469: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01480: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01491: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01502: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01513: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01524: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 01535: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 01546: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Early stopping in fold 7!\n",
      "Fold 7 RMSE: 152.6806224877295\n",
      "Fold 8\n",
      "Epoch 01356: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01379: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01390: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01401: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01412: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01423: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01434: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01445: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01456: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01467: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Early stopping in fold 8!\n",
      "Fold 8 RMSE: 130.35769285164636\n",
      "Fold 9\n",
      "Epoch 01091: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01279: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01332: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01343: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01354: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01365: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01376: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01387: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01398: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01409: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 01420: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Early stopping in fold 9!\n",
      "Fold 9 RMSE: 144.88751968104467\n",
      "Fold 10\n",
      "Epoch 01248: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 01283: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 01294: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 01305: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 01316: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01327: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01338: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01349: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01360: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01371: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Early stopping in fold 10!\n",
      "Fold 10 RMSE: 164.93866191159464\n",
      "Average RMSE over all folds: 152.65931514904224\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_fold)\n",
    "    y_train_tensor = torch.FloatTensor(y_train_fold)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_fold)\n",
    "    y_val_tensor = torch.FloatTensor(y_val_fold)\n",
    "    \n",
    "    # Model, Loss, optimizer, and scheduler\n",
    "    model = NeuralNet(X_train_tensor.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    # Training with early stopping\n",
    "    num_epochs = 3000\n",
    "    patience = 100\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor).squeeze()\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        \n",
    "        # Print loss for every epoch (optional)\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "        \n",
    "        # Early stopping and learning rate reduction on plateau\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping in fold {fold + 1}!\")\n",
    "                break\n",
    "    \n",
    "    # Save the model for this fold\n",
    "    models.append(model)\n",
    "\n",
    "    # Calculate RMSE for this fold\n",
    "    val_outputs_np = val_outputs.detach().numpy()\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, val_outputs_np))\n",
    "    rmse_values.append(fold_rmse)\n",
    "    print(f\"Fold {fold + 1} RMSE: {fold_rmse}\")\n",
    "    \n",
    "    # Preprocess test data for this fold\n",
    "    test_encoded = pd.get_dummies(test_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "    X_test = test_encoded.drop(columns=[\"ID\"]).values\n",
    "    X_test_fold = scaler.transform(X_test)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_fold)\n",
    "\n",
    "    # Predict on test data for this fold\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(X_test_tensor).squeeze().numpy()\n",
    "    test_predictions_all.append(test_predictions)\n",
    "\n",
    "# You can then average out the predictions from all folds for the test set if needed.\n",
    "final_predictions = np.mean(test_predictions_all, axis=0)\n",
    "\n",
    "# Printing average RMSE across folds\n",
    "print(f\"Average RMSE over all folds: {np.mean(rmse_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4beba7b2-aa35-4b3e-a9fe-788b961665f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging test predictions from all folds\n",
    "final_predictions = np.mean(np.array(test_predictions_all), axis=0)\n",
    "\n",
    "# Generate a submission file\n",
    "submission_dl = pd.DataFrame({'ID': test_data[\"ID\"], '수요량': final_predictions})\n",
    "submission_dl.to_csv(\"./data/submission_dl_kfold.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487cc46-90eb-488a-b642-ddc1752f616e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
