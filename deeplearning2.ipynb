{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf055fbd-1c7f-4ac7-ba7f-01279a95b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Loss: 183299.46875, Val Loss: 190231.671875\n",
      "Epoch 2/3000, Loss: 183261.03125, Val Loss: 190191.203125\n",
      "Epoch 3/3000, Loss: 183220.46875, Val Loss: 190151.046875\n",
      "Epoch 4/3000, Loss: 183181.515625, Val Loss: 190110.71875\n",
      "Epoch 5/3000, Loss: 183142.078125, Val Loss: 190069.671875\n",
      "Epoch 6/3000, Loss: 183101.625, Val Loss: 190027.484375\n",
      "Epoch 7/3000, Loss: 183061.90625, Val Loss: 189983.59375\n",
      "Epoch 8/3000, Loss: 183018.125, Val Loss: 189937.703125\n",
      "Epoch 9/3000, Loss: 182973.625, Val Loss: 189889.28125\n",
      "Epoch 10/3000, Loss: 182928.1875, Val Loss: 189838.03125\n",
      "Epoch 11/3000, Loss: 182879.359375, Val Loss: 189783.578125\n",
      "Epoch 12/3000, Loss: 182823.34375, Val Loss: 189725.5\n",
      "Epoch 13/3000, Loss: 182767.15625, Val Loss: 189663.515625\n",
      "Epoch 14/3000, Loss: 182710.484375, Val Loss: 189597.296875\n",
      "Epoch 15/3000, Loss: 182644.0, Val Loss: 189526.40625\n",
      "Epoch 16/3000, Loss: 182575.65625, Val Loss: 189450.421875\n",
      "Epoch 17/3000, Loss: 182505.0, Val Loss: 189368.875\n",
      "Epoch 18/3000, Loss: 182421.828125, Val Loss: 189281.515625\n",
      "Epoch 19/3000, Loss: 182336.796875, Val Loss: 189187.796875\n",
      "Epoch 20/3000, Loss: 182248.0625, Val Loss: 189087.296875\n",
      "Epoch 21/3000, Loss: 182151.1875, Val Loss: 188979.546875\n",
      "Epoch 22/3000, Loss: 182042.59375, Val Loss: 188864.0\n",
      "Epoch 23/3000, Loss: 181930.75, Val Loss: 188740.1875\n",
      "Epoch 24/3000, Loss: 181813.640625, Val Loss: 188607.640625\n",
      "Epoch 25/3000, Loss: 181681.875, Val Loss: 188465.796875\n",
      "Epoch 26/3000, Loss: 181537.90625, Val Loss: 188314.125\n",
      "Epoch 27/3000, Loss: 181393.109375, Val Loss: 188152.0\n",
      "Epoch 28/3000, Loss: 181241.328125, Val Loss: 187978.90625\n",
      "Epoch 29/3000, Loss: 181074.46875, Val Loss: 187794.15625\n",
      "Epoch 30/3000, Loss: 180886.171875, Val Loss: 187597.1875\n",
      "Epoch 31/3000, Loss: 180688.671875, Val Loss: 187387.234375\n",
      "Epoch 32/3000, Loss: 180488.671875, Val Loss: 187163.703125\n",
      "Epoch 33/3000, Loss: 180264.09375, Val Loss: 186925.8125\n",
      "Epoch 34/3000, Loss: 180040.78125, Val Loss: 186672.75\n",
      "Epoch 35/3000, Loss: 179795.546875, Val Loss: 186403.96875\n",
      "Epoch 36/3000, Loss: 179528.109375, Val Loss: 186118.6875\n",
      "Epoch 37/3000, Loss: 179243.390625, Val Loss: 185816.0625\n",
      "Epoch 38/3000, Loss: 178961.640625, Val Loss: 185495.359375\n",
      "Epoch 39/3000, Loss: 178629.0625, Val Loss: 185155.828125\n",
      "Epoch 40/3000, Loss: 178302.390625, Val Loss: 184796.5\n",
      "Epoch 41/3000, Loss: 177951.875, Val Loss: 184416.578125\n",
      "Epoch 42/3000, Loss: 177578.34375, Val Loss: 184015.3125\n",
      "Epoch 43/3000, Loss: 177177.5, Val Loss: 183591.890625\n",
      "Epoch 44/3000, Loss: 176762.0, Val Loss: 183145.4375\n",
      "Epoch 45/3000, Loss: 176342.6875, Val Loss: 182675.25\n",
      "Epoch 46/3000, Loss: 175859.375, Val Loss: 182180.421875\n",
      "Epoch 47/3000, Loss: 175383.15625, Val Loss: 181660.1875\n",
      "Epoch 48/3000, Loss: 174855.578125, Val Loss: 181113.484375\n",
      "Epoch 49/3000, Loss: 174324.765625, Val Loss: 180539.5625\n",
      "Epoch 50/3000, Loss: 173769.140625, Val Loss: 179937.640625\n",
      "Epoch 51/3000, Loss: 173154.234375, Val Loss: 179306.8125\n",
      "Epoch 52/3000, Loss: 172526.0625, Val Loss: 178646.140625\n",
      "Epoch 53/3000, Loss: 171870.265625, Val Loss: 177954.765625\n",
      "Epoch 54/3000, Loss: 171199.546875, Val Loss: 177231.953125\n",
      "Epoch 55/3000, Loss: 170540.34375, Val Loss: 176476.984375\n",
      "Epoch 56/3000, Loss: 169775.96875, Val Loss: 175689.09375\n",
      "Epoch 57/3000, Loss: 168986.28125, Val Loss: 174867.296875\n",
      "Epoch 58/3000, Loss: 168180.546875, Val Loss: 174010.65625\n",
      "Epoch 59/3000, Loss: 167326.03125, Val Loss: 173118.671875\n",
      "Epoch 60/3000, Loss: 166479.40625, Val Loss: 172191.125\n",
      "Epoch 61/3000, Loss: 165539.203125, Val Loss: 171227.28125\n",
      "Epoch 62/3000, Loss: 164626.015625, Val Loss: 170226.703125\n",
      "Epoch 63/3000, Loss: 163600.5625, Val Loss: 169188.828125\n",
      "Epoch 64/3000, Loss: 162623.0, Val Loss: 168113.3125\n",
      "Epoch 65/3000, Loss: 161496.53125, Val Loss: 166999.4375\n",
      "Epoch 66/3000, Loss: 160455.625, Val Loss: 165847.40625\n",
      "Epoch 67/3000, Loss: 159323.28125, Val Loss: 164657.078125\n",
      "Epoch 68/3000, Loss: 158073.78125, Val Loss: 163428.046875\n",
      "Epoch 69/3000, Loss: 156960.421875, Val Loss: 162160.578125\n",
      "Epoch 70/3000, Loss: 155643.359375, Val Loss: 160854.359375\n",
      "Epoch 71/3000, Loss: 154253.171875, Val Loss: 159509.046875\n",
      "Epoch 72/3000, Loss: 153015.90625, Val Loss: 158125.1875\n",
      "Epoch 73/3000, Loss: 151633.78125, Val Loss: 156703.09375\n",
      "Epoch 74/3000, Loss: 150176.71875, Val Loss: 155242.875\n",
      "Epoch 75/3000, Loss: 148745.84375, Val Loss: 153745.296875\n",
      "Epoch 76/3000, Loss: 147204.8125, Val Loss: 152210.390625\n",
      "Epoch 77/3000, Loss: 145840.609375, Val Loss: 150639.3125\n",
      "Epoch 78/3000, Loss: 144250.734375, Val Loss: 149032.875\n",
      "Epoch 79/3000, Loss: 142793.015625, Val Loss: 147392.5\n",
      "Epoch 80/3000, Loss: 140988.78125, Val Loss: 145718.484375\n",
      "Epoch 81/3000, Loss: 139323.078125, Val Loss: 144012.046875\n",
      "Epoch 82/3000, Loss: 137632.640625, Val Loss: 142274.078125\n",
      "Epoch 83/3000, Loss: 135960.46875, Val Loss: 140505.703125\n",
      "Epoch 84/3000, Loss: 134142.359375, Val Loss: 138708.375\n",
      "Epoch 85/3000, Loss: 132201.3125, Val Loss: 136883.578125\n",
      "Epoch 86/3000, Loss: 130624.0546875, Val Loss: 135033.328125\n",
      "Epoch 87/3000, Loss: 128832.40625, Val Loss: 133159.4375\n",
      "Epoch 88/3000, Loss: 126739.40625, Val Loss: 131263.203125\n",
      "Epoch 89/3000, Loss: 125013.2265625, Val Loss: 129346.9921875\n",
      "Epoch 90/3000, Loss: 123074.640625, Val Loss: 127412.8359375\n",
      "Epoch 91/3000, Loss: 121208.65625, Val Loss: 125463.109375\n",
      "Epoch 92/3000, Loss: 119192.4453125, Val Loss: 123499.53125\n",
      "Epoch 93/3000, Loss: 117215.9375, Val Loss: 121524.546875\n",
      "Epoch 94/3000, Loss: 115606.6328125, Val Loss: 119541.640625\n",
      "Epoch 95/3000, Loss: 113137.4375, Val Loss: 117552.6015625\n",
      "Epoch 96/3000, Loss: 111357.0234375, Val Loss: 115560.3046875\n",
      "Epoch 97/3000, Loss: 109308.578125, Val Loss: 113567.4375\n",
      "Epoch 98/3000, Loss: 107198.1640625, Val Loss: 111576.5390625\n",
      "Epoch 99/3000, Loss: 105230.0859375, Val Loss: 109590.7578125\n",
      "Epoch 100/3000, Loss: 103410.6953125, Val Loss: 107613.0078125\n",
      "Epoch 101/3000, Loss: 101256.296875, Val Loss: 105646.3515625\n",
      "Epoch 102/3000, Loss: 99276.0703125, Val Loss: 103693.4921875\n",
      "Epoch 103/3000, Loss: 97254.09375, Val Loss: 101757.40625\n",
      "Epoch 104/3000, Loss: 95652.53125, Val Loss: 99841.5546875\n",
      "Epoch 105/3000, Loss: 93273.78125, Val Loss: 97948.4296875\n",
      "Epoch 106/3000, Loss: 91541.5234375, Val Loss: 96081.5859375\n",
      "Epoch 107/3000, Loss: 89634.8125, Val Loss: 94243.203125\n",
      "Epoch 108/3000, Loss: 87599.875, Val Loss: 92436.0\n",
      "Epoch 109/3000, Loss: 86050.1015625, Val Loss: 90663.1015625\n",
      "Epoch 110/3000, Loss: 83985.453125, Val Loss: 88926.890625\n",
      "Epoch 111/3000, Loss: 82128.640625, Val Loss: 87229.5078125\n",
      "Epoch 112/3000, Loss: 80246.0390625, Val Loss: 85573.3671875\n",
      "Epoch 113/3000, Loss: 78534.3046875, Val Loss: 83960.8359375\n",
      "Epoch 114/3000, Loss: 76849.9453125, Val Loss: 82394.1640625\n",
      "Epoch 115/3000, Loss: 75761.9375, Val Loss: 80875.8828125\n",
      "Epoch 116/3000, Loss: 74263.9296875, Val Loss: 79407.2578125\n",
      "Epoch 117/3000, Loss: 72772.5625, Val Loss: 77990.21875\n",
      "Epoch 118/3000, Loss: 71084.4453125, Val Loss: 76624.453125\n",
      "Epoch 119/3000, Loss: 69804.5546875, Val Loss: 75311.390625\n",
      "Epoch 120/3000, Loss: 68451.859375, Val Loss: 74051.5859375\n",
      "Epoch 121/3000, Loss: 66983.5546875, Val Loss: 72846.0703125\n",
      "Epoch 122/3000, Loss: 65866.5390625, Val Loss: 71695.390625\n",
      "Epoch 123/3000, Loss: 64350.9296875, Val Loss: 70598.359375\n",
      "Epoch 124/3000, Loss: 63383.8671875, Val Loss: 69555.4453125\n",
      "Epoch 125/3000, Loss: 62273.30859375, Val Loss: 68565.640625\n",
      "Epoch 126/3000, Loss: 61359.09765625, Val Loss: 67629.125\n",
      "Epoch 127/3000, Loss: 60250.64453125, Val Loss: 66743.4140625\n",
      "Epoch 128/3000, Loss: 59352.1796875, Val Loss: 65907.46875\n",
      "Epoch 129/3000, Loss: 58299.60546875, Val Loss: 65120.27734375\n",
      "Epoch 130/3000, Loss: 57710.75, Val Loss: 64380.25\n",
      "Epoch 131/3000, Loss: 56940.37890625, Val Loss: 63685.6171875\n",
      "Epoch 132/3000, Loss: 55873.640625, Val Loss: 63033.62890625\n",
      "Epoch 133/3000, Loss: 55208.453125, Val Loss: 62422.1328125\n",
      "Epoch 134/3000, Loss: 54626.08984375, Val Loss: 61849.1640625\n",
      "Epoch 135/3000, Loss: 54238.09765625, Val Loss: 61312.03515625\n",
      "Epoch 136/3000, Loss: 53621.375, Val Loss: 60808.484375\n",
      "Epoch 137/3000, Loss: 53150.9375, Val Loss: 60335.34765625\n",
      "Epoch 138/3000, Loss: 52502.4765625, Val Loss: 59890.34765625\n",
      "Epoch 139/3000, Loss: 52612.125, Val Loss: 59471.53515625\n",
      "Epoch 140/3000, Loss: 52119.01171875, Val Loss: 59077.203125\n",
      "Epoch 141/3000, Loss: 51508.828125, Val Loss: 58705.15234375\n",
      "Epoch 142/3000, Loss: 51095.3359375, Val Loss: 58352.66796875\n",
      "Epoch 143/3000, Loss: 50222.125, Val Loss: 58018.44921875\n",
      "Epoch 144/3000, Loss: 50307.8828125, Val Loss: 57700.359375\n",
      "Epoch 145/3000, Loss: 49754.76171875, Val Loss: 57396.34375\n",
      "Epoch 146/3000, Loss: 50044.0, Val Loss: 57104.12890625\n",
      "Epoch 147/3000, Loss: 49400.34375, Val Loss: 56823.78125\n",
      "Epoch 148/3000, Loss: 49205.54296875, Val Loss: 56554.21875\n",
      "Epoch 149/3000, Loss: 48800.33984375, Val Loss: 56293.48046875\n",
      "Epoch 150/3000, Loss: 48504.6875, Val Loss: 56041.0078125\n",
      "Epoch 151/3000, Loss: 48405.70703125, Val Loss: 55796.16015625\n",
      "Epoch 152/3000, Loss: 47417.5859375, Val Loss: 55557.35546875\n",
      "Epoch 153/3000, Loss: 47705.734375, Val Loss: 55324.39453125\n",
      "Epoch 154/3000, Loss: 47460.21484375, Val Loss: 55096.19921875\n",
      "Epoch 155/3000, Loss: 46836.0078125, Val Loss: 54873.0234375\n",
      "Epoch 156/3000, Loss: 47211.453125, Val Loss: 54654.1953125\n",
      "Epoch 157/3000, Loss: 47095.859375, Val Loss: 54439.58984375\n",
      "Epoch 158/3000, Loss: 46582.53125, Val Loss: 54230.03125\n",
      "Epoch 159/3000, Loss: 46855.6953125, Val Loss: 54023.96484375\n",
      "Epoch 160/3000, Loss: 46291.6953125, Val Loss: 53821.6484375\n",
      "Epoch 161/3000, Loss: 46304.87890625, Val Loss: 53622.76171875\n",
      "Epoch 162/3000, Loss: 45874.859375, Val Loss: 53427.28515625\n",
      "Epoch 163/3000, Loss: 45780.84375, Val Loss: 53235.65234375\n",
      "Epoch 164/3000, Loss: 46152.44140625, Val Loss: 53047.8125\n",
      "Epoch 165/3000, Loss: 45632.4140625, Val Loss: 52863.859375\n",
      "Epoch 166/3000, Loss: 45101.76171875, Val Loss: 52682.546875\n",
      "Epoch 167/3000, Loss: 45422.125, Val Loss: 52503.95703125\n",
      "Epoch 168/3000, Loss: 44557.66015625, Val Loss: 52328.96484375\n",
      "Epoch 169/3000, Loss: 44713.5859375, Val Loss: 52157.51953125\n",
      "Epoch 170/3000, Loss: 44518.24609375, Val Loss: 51990.17578125\n",
      "Epoch 171/3000, Loss: 44025.80859375, Val Loss: 51826.7890625\n",
      "Epoch 172/3000, Loss: 44103.27734375, Val Loss: 51666.3984375\n",
      "Epoch 173/3000, Loss: 44003.28515625, Val Loss: 51509.39453125\n",
      "Epoch 174/3000, Loss: 44114.19140625, Val Loss: 51355.36328125\n",
      "Epoch 175/3000, Loss: 43662.50390625, Val Loss: 51204.3046875\n",
      "Epoch 176/3000, Loss: 43524.02734375, Val Loss: 51056.09375\n",
      "Epoch 177/3000, Loss: 42964.6171875, Val Loss: 50910.69921875\n",
      "Epoch 178/3000, Loss: 42857.60546875, Val Loss: 50768.37890625\n",
      "Epoch 179/3000, Loss: 43108.30859375, Val Loss: 50629.3828125\n",
      "Epoch 180/3000, Loss: 42856.55859375, Val Loss: 50492.453125\n",
      "Epoch 181/3000, Loss: 42911.36328125, Val Loss: 50357.69140625\n",
      "Epoch 182/3000, Loss: 42520.04296875, Val Loss: 50225.15625\n",
      "Epoch 183/3000, Loss: 42795.9296875, Val Loss: 50094.953125\n",
      "Epoch 184/3000, Loss: 42415.5859375, Val Loss: 49967.53515625\n",
      "Epoch 185/3000, Loss: 42581.2890625, Val Loss: 49843.21484375\n",
      "Epoch 186/3000, Loss: 42528.38671875, Val Loss: 49721.8828125\n",
      "Epoch 187/3000, Loss: 42263.08203125, Val Loss: 49602.77734375\n",
      "Epoch 188/3000, Loss: 41790.79296875, Val Loss: 49485.79296875\n",
      "Epoch 189/3000, Loss: 41750.6796875, Val Loss: 49371.45703125\n",
      "Epoch 190/3000, Loss: 41843.3046875, Val Loss: 49259.74609375\n",
      "Epoch 191/3000, Loss: 41741.41796875, Val Loss: 49150.1953125\n",
      "Epoch 192/3000, Loss: 41416.734375, Val Loss: 49041.89453125\n",
      "Epoch 193/3000, Loss: 41581.7421875, Val Loss: 48935.37109375\n",
      "Epoch 194/3000, Loss: 41222.6875, Val Loss: 48831.53515625\n",
      "Epoch 195/3000, Loss: 40544.078125, Val Loss: 48730.0546875\n",
      "Epoch 196/3000, Loss: 41128.61328125, Val Loss: 48629.84765625\n",
      "Epoch 197/3000, Loss: 40672.0625, Val Loss: 48531.0\n",
      "Epoch 198/3000, Loss: 40836.17578125, Val Loss: 48433.1875\n",
      "Epoch 199/3000, Loss: 40686.09375, Val Loss: 48338.08203125\n",
      "Epoch 200/3000, Loss: 40814.265625, Val Loss: 48245.390625\n",
      "Epoch 201/3000, Loss: 40504.16015625, Val Loss: 48154.515625\n",
      "Epoch 202/3000, Loss: 40646.5625, Val Loss: 48064.71875\n",
      "Epoch 203/3000, Loss: 40552.2109375, Val Loss: 47976.265625\n",
      "Epoch 204/3000, Loss: 39992.984375, Val Loss: 47888.81640625\n",
      "Epoch 205/3000, Loss: 39908.40625, Val Loss: 47803.12890625\n",
      "Epoch 206/3000, Loss: 40415.39453125, Val Loss: 47719.57421875\n",
      "Epoch 207/3000, Loss: 40074.42578125, Val Loss: 47638.21875\n",
      "Epoch 208/3000, Loss: 39723.1953125, Val Loss: 47557.4453125\n",
      "Epoch 209/3000, Loss: 39895.12890625, Val Loss: 47478.63671875\n",
      "Epoch 210/3000, Loss: 39663.15625, Val Loss: 47400.81640625\n",
      "Epoch 211/3000, Loss: 39726.203125, Val Loss: 47323.82421875\n",
      "Epoch 212/3000, Loss: 39676.171875, Val Loss: 47247.66015625\n",
      "Epoch 213/3000, Loss: 39567.73828125, Val Loss: 47173.2421875\n",
      "Epoch 214/3000, Loss: 39871.8515625, Val Loss: 47099.58984375\n",
      "Epoch 215/3000, Loss: 39299.9140625, Val Loss: 47027.7734375\n",
      "Epoch 216/3000, Loss: 39509.80859375, Val Loss: 46957.1640625\n",
      "Epoch 217/3000, Loss: 39330.3359375, Val Loss: 46888.3359375\n",
      "Epoch 218/3000, Loss: 39297.99609375, Val Loss: 46820.09375\n",
      "Epoch 219/3000, Loss: 39101.03125, Val Loss: 46753.66015625\n",
      "Epoch 220/3000, Loss: 39030.79296875, Val Loss: 46688.046875\n",
      "Epoch 221/3000, Loss: 38891.46875, Val Loss: 46623.60546875\n",
      "Epoch 222/3000, Loss: 38888.07421875, Val Loss: 46560.44921875\n",
      "Epoch 223/3000, Loss: 39044.31640625, Val Loss: 46498.6875\n",
      "Epoch 224/3000, Loss: 38699.62109375, Val Loss: 46438.1328125\n",
      "Epoch 225/3000, Loss: 39003.72265625, Val Loss: 46379.25390625\n",
      "Epoch 226/3000, Loss: 38700.1484375, Val Loss: 46321.56640625\n",
      "Epoch 227/3000, Loss: 38301.8359375, Val Loss: 46264.625\n",
      "Epoch 228/3000, Loss: 38943.17578125, Val Loss: 46209.3203125\n",
      "Epoch 229/3000, Loss: 38465.5234375, Val Loss: 46153.89453125\n",
      "Epoch 230/3000, Loss: 38645.859375, Val Loss: 46099.41015625\n",
      "Epoch 231/3000, Loss: 38051.4609375, Val Loss: 46045.33984375\n",
      "Epoch 232/3000, Loss: 38307.75390625, Val Loss: 45992.546875\n",
      "Epoch 233/3000, Loss: 37982.9140625, Val Loss: 45940.03515625\n",
      "Epoch 234/3000, Loss: 37682.76171875, Val Loss: 45887.1640625\n",
      "Epoch 235/3000, Loss: 37983.23046875, Val Loss: 45833.76953125\n",
      "Epoch 236/3000, Loss: 37943.8671875, Val Loss: 45780.77734375\n",
      "Epoch 237/3000, Loss: 38053.16796875, Val Loss: 45729.41015625\n",
      "Epoch 238/3000, Loss: 38082.25390625, Val Loss: 45679.56640625\n",
      "Epoch 239/3000, Loss: 37743.6328125, Val Loss: 45629.828125\n",
      "Epoch 240/3000, Loss: 37871.5859375, Val Loss: 45581.34765625\n",
      "Epoch 241/3000, Loss: 37696.55859375, Val Loss: 45533.8515625\n",
      "Epoch 242/3000, Loss: 38154.296875, Val Loss: 45486.59375\n",
      "Epoch 243/3000, Loss: 37638.40234375, Val Loss: 45440.09375\n",
      "Epoch 244/3000, Loss: 37628.79296875, Val Loss: 45394.50390625\n",
      "Epoch 245/3000, Loss: 37441.15625, Val Loss: 45349.95703125\n",
      "Epoch 246/3000, Loss: 37288.53515625, Val Loss: 45305.83203125\n",
      "Epoch 247/3000, Loss: 37127.515625, Val Loss: 45262.9296875\n",
      "Epoch 248/3000, Loss: 37670.9765625, Val Loss: 45220.69921875\n",
      "Epoch 249/3000, Loss: 37400.98046875, Val Loss: 45179.30859375\n",
      "Epoch 250/3000, Loss: 37005.4921875, Val Loss: 45138.2421875\n",
      "Epoch 251/3000, Loss: 37034.42578125, Val Loss: 45097.31640625\n",
      "Epoch 252/3000, Loss: 37347.68359375, Val Loss: 45056.90625\n",
      "Epoch 253/3000, Loss: 37545.03125, Val Loss: 45017.54296875\n",
      "Epoch 254/3000, Loss: 37275.9140625, Val Loss: 44979.03515625\n",
      "Epoch 255/3000, Loss: 37157.7109375, Val Loss: 44941.05859375\n",
      "Epoch 256/3000, Loss: 36857.9375, Val Loss: 44904.3828125\n",
      "Epoch 257/3000, Loss: 36530.765625, Val Loss: 44867.7265625\n",
      "Epoch 258/3000, Loss: 36897.09765625, Val Loss: 44830.765625\n",
      "Epoch 259/3000, Loss: 36919.2265625, Val Loss: 44793.6015625\n",
      "Epoch 260/3000, Loss: 36661.125, Val Loss: 44756.94140625\n",
      "Epoch 261/3000, Loss: 37077.3046875, Val Loss: 44720.84375\n",
      "Epoch 262/3000, Loss: 36701.26171875, Val Loss: 44684.984375\n",
      "Epoch 263/3000, Loss: 36808.453125, Val Loss: 44649.01953125\n",
      "Epoch 264/3000, Loss: 36688.06640625, Val Loss: 44613.26171875\n",
      "Epoch 265/3000, Loss: 36928.8671875, Val Loss: 44577.8203125\n",
      "Epoch 266/3000, Loss: 36790.890625, Val Loss: 44542.2421875\n",
      "Epoch 267/3000, Loss: 36460.5390625, Val Loss: 44507.48046875\n",
      "Epoch 268/3000, Loss: 36269.0703125, Val Loss: 44474.1875\n",
      "Epoch 269/3000, Loss: 36785.53125, Val Loss: 44442.41015625\n",
      "Epoch 270/3000, Loss: 36763.25, Val Loss: 44410.234375\n",
      "Epoch 271/3000, Loss: 36304.4765625, Val Loss: 44378.109375\n",
      "Epoch 272/3000, Loss: 36343.7890625, Val Loss: 44346.09375\n",
      "Epoch 273/3000, Loss: 36127.40234375, Val Loss: 44313.859375\n",
      "Epoch 274/3000, Loss: 36360.47265625, Val Loss: 44281.125\n",
      "Epoch 275/3000, Loss: 35989.32421875, Val Loss: 44250.18359375\n",
      "Epoch 276/3000, Loss: 36152.94921875, Val Loss: 44219.62109375\n",
      "Epoch 277/3000, Loss: 36139.2421875, Val Loss: 44189.4140625\n",
      "Epoch 278/3000, Loss: 36166.66015625, Val Loss: 44158.5859375\n",
      "Epoch 279/3000, Loss: 35973.78515625, Val Loss: 44128.60546875\n",
      "Epoch 280/3000, Loss: 36357.703125, Val Loss: 44100.5\n",
      "Epoch 281/3000, Loss: 36194.16796875, Val Loss: 44073.3125\n",
      "Epoch 282/3000, Loss: 35903.33984375, Val Loss: 44046.44140625\n",
      "Epoch 283/3000, Loss: 35801.703125, Val Loss: 44019.7421875\n",
      "Epoch 284/3000, Loss: 35549.2109375, Val Loss: 43993.51953125\n",
      "Epoch 285/3000, Loss: 35420.796875, Val Loss: 43967.19921875\n",
      "Epoch 286/3000, Loss: 36000.9453125, Val Loss: 43941.70703125\n",
      "Epoch 287/3000, Loss: 35557.55859375, Val Loss: 43916.703125\n",
      "Epoch 288/3000, Loss: 35279.08203125, Val Loss: 43890.94921875\n",
      "Epoch 289/3000, Loss: 35713.796875, Val Loss: 43864.2265625\n",
      "Epoch 290/3000, Loss: 35356.75, Val Loss: 43836.6015625\n",
      "Epoch 291/3000, Loss: 35811.96484375, Val Loss: 43808.22265625\n",
      "Epoch 292/3000, Loss: 35292.1640625, Val Loss: 43780.75\n",
      "Epoch 293/3000, Loss: 35089.546875, Val Loss: 43753.140625\n",
      "Epoch 294/3000, Loss: 34877.078125, Val Loss: 43725.78125\n",
      "Epoch 295/3000, Loss: 35344.30859375, Val Loss: 43698.2890625\n",
      "Epoch 296/3000, Loss: 34929.86328125, Val Loss: 43669.90234375\n",
      "Epoch 297/3000, Loss: 35439.62109375, Val Loss: 43642.05078125\n",
      "Epoch 298/3000, Loss: 34947.8828125, Val Loss: 43614.31640625\n",
      "Epoch 299/3000, Loss: 34820.48828125, Val Loss: 43587.265625\n",
      "Epoch 300/3000, Loss: 34827.66796875, Val Loss: 43560.05859375\n",
      "Epoch 301/3000, Loss: 35426.046875, Val Loss: 43534.890625\n",
      "Epoch 302/3000, Loss: 35215.6796875, Val Loss: 43510.8203125\n",
      "Epoch 303/3000, Loss: 34640.61328125, Val Loss: 43486.19140625\n",
      "Epoch 304/3000, Loss: 35015.421875, Val Loss: 43461.7578125\n",
      "Epoch 305/3000, Loss: 34577.44921875, Val Loss: 43436.72265625\n",
      "Epoch 306/3000, Loss: 35410.96875, Val Loss: 43411.7734375\n",
      "Epoch 307/3000, Loss: 34836.08984375, Val Loss: 43387.14453125\n",
      "Epoch 308/3000, Loss: 34847.25, Val Loss: 43362.640625\n",
      "Epoch 309/3000, Loss: 34564.59375, Val Loss: 43338.44921875\n",
      "Epoch 310/3000, Loss: 34874.79296875, Val Loss: 43314.015625\n",
      "Epoch 311/3000, Loss: 34765.01953125, Val Loss: 43288.8984375\n",
      "Epoch 312/3000, Loss: 35023.77734375, Val Loss: 43264.30078125\n",
      "Epoch 313/3000, Loss: 34934.4296875, Val Loss: 43239.2890625\n",
      "Epoch 314/3000, Loss: 34596.50390625, Val Loss: 43214.42578125\n",
      "Epoch 315/3000, Loss: 34792.8125, Val Loss: 43188.19140625\n",
      "Epoch 316/3000, Loss: 34456.171875, Val Loss: 43161.44140625\n",
      "Epoch 317/3000, Loss: 34771.0546875, Val Loss: 43134.5859375\n",
      "Epoch 318/3000, Loss: 34587.7421875, Val Loss: 43108.6484375\n",
      "Epoch 319/3000, Loss: 34508.16015625, Val Loss: 43082.14453125\n",
      "Epoch 320/3000, Loss: 34445.16796875, Val Loss: 43056.140625\n",
      "Epoch 321/3000, Loss: 34449.02734375, Val Loss: 43030.84765625\n",
      "Epoch 322/3000, Loss: 34229.3984375, Val Loss: 43006.40234375\n",
      "Epoch 323/3000, Loss: 34301.8828125, Val Loss: 42981.703125\n",
      "Epoch 324/3000, Loss: 34156.6328125, Val Loss: 42956.63671875\n",
      "Epoch 325/3000, Loss: 34171.99609375, Val Loss: 42932.99609375\n",
      "Epoch 326/3000, Loss: 34496.7578125, Val Loss: 42910.56640625\n",
      "Epoch 327/3000, Loss: 34648.17578125, Val Loss: 42888.47265625\n",
      "Epoch 328/3000, Loss: 34108.375, Val Loss: 42865.83203125\n",
      "Epoch 329/3000, Loss: 34205.76171875, Val Loss: 42843.21875\n",
      "Epoch 330/3000, Loss: 33834.43359375, Val Loss: 42820.2421875\n",
      "Epoch 331/3000, Loss: 34173.33203125, Val Loss: 42797.5234375\n",
      "Epoch 332/3000, Loss: 33926.94140625, Val Loss: 42775.1171875\n",
      "Epoch 333/3000, Loss: 34108.12109375, Val Loss: 42753.66796875\n",
      "Epoch 334/3000, Loss: 33977.921875, Val Loss: 42733.36328125\n",
      "Epoch 335/3000, Loss: 33857.6171875, Val Loss: 42712.27734375\n",
      "Epoch 336/3000, Loss: 33924.89453125, Val Loss: 42692.8203125\n",
      "Epoch 337/3000, Loss: 34237.078125, Val Loss: 42674.67578125\n",
      "Epoch 338/3000, Loss: 33554.203125, Val Loss: 42655.921875\n",
      "Epoch 339/3000, Loss: 34305.0625, Val Loss: 42637.171875\n",
      "Epoch 340/3000, Loss: 33854.0234375, Val Loss: 42617.90625\n",
      "Epoch 341/3000, Loss: 34193.45703125, Val Loss: 42599.59765625\n",
      "Epoch 342/3000, Loss: 34144.23046875, Val Loss: 42579.45703125\n",
      "Epoch 343/3000, Loss: 33803.75390625, Val Loss: 42559.171875\n",
      "Epoch 344/3000, Loss: 33924.80859375, Val Loss: 42538.2890625\n",
      "Epoch 345/3000, Loss: 33442.12109375, Val Loss: 42518.07421875\n",
      "Epoch 346/3000, Loss: 33842.9609375, Val Loss: 42498.23828125\n",
      "Epoch 347/3000, Loss: 33573.359375, Val Loss: 42477.42578125\n",
      "Epoch 348/3000, Loss: 33706.4921875, Val Loss: 42455.98046875\n",
      "Epoch 349/3000, Loss: 33603.6484375, Val Loss: 42435.05859375\n",
      "Epoch 350/3000, Loss: 33658.12109375, Val Loss: 42415.07421875\n",
      "Epoch 351/3000, Loss: 33397.5234375, Val Loss: 42394.859375\n",
      "Epoch 352/3000, Loss: 33575.890625, Val Loss: 42374.37890625\n",
      "Epoch 353/3000, Loss: 34231.84375, Val Loss: 42352.921875\n",
      "Epoch 354/3000, Loss: 34016.26171875, Val Loss: 42331.00390625\n",
      "Epoch 355/3000, Loss: 33535.42578125, Val Loss: 42307.65234375\n",
      "Epoch 356/3000, Loss: 33433.62109375, Val Loss: 42285.1328125\n",
      "Epoch 357/3000, Loss: 33269.05859375, Val Loss: 42262.3046875\n",
      "Epoch 358/3000, Loss: 33527.48046875, Val Loss: 42240.23046875\n",
      "Epoch 359/3000, Loss: 33000.43359375, Val Loss: 42218.203125\n",
      "Epoch 360/3000, Loss: 33375.86328125, Val Loss: 42195.54296875\n",
      "Epoch 361/3000, Loss: 33102.484375, Val Loss: 42174.11328125\n",
      "Epoch 362/3000, Loss: 33314.56640625, Val Loss: 42153.0625\n",
      "Epoch 363/3000, Loss: 33696.375, Val Loss: 42133.90234375\n",
      "Epoch 364/3000, Loss: 33083.3828125, Val Loss: 42116.01171875\n",
      "Epoch 365/3000, Loss: 33310.6328125, Val Loss: 42097.41015625\n",
      "Epoch 366/3000, Loss: 33201.6953125, Val Loss: 42078.6796875\n",
      "Epoch 367/3000, Loss: 33101.36328125, Val Loss: 42058.80859375\n",
      "Epoch 368/3000, Loss: 33059.35546875, Val Loss: 42038.93359375\n",
      "Epoch 369/3000, Loss: 33123.06640625, Val Loss: 42019.1484375\n",
      "Epoch 370/3000, Loss: 33142.15625, Val Loss: 41998.8984375\n",
      "Epoch 371/3000, Loss: 33345.53125, Val Loss: 41979.2109375\n",
      "Epoch 372/3000, Loss: 33172.046875, Val Loss: 41959.83203125\n",
      "Epoch 373/3000, Loss: 33169.09375, Val Loss: 41941.0234375\n",
      "Epoch 374/3000, Loss: 33191.00390625, Val Loss: 41922.359375\n",
      "Epoch 375/3000, Loss: 32868.109375, Val Loss: 41904.8515625\n",
      "Epoch 376/3000, Loss: 32888.265625, Val Loss: 41885.984375\n",
      "Epoch 377/3000, Loss: 33270.28515625, Val Loss: 41866.1953125\n",
      "Epoch 378/3000, Loss: 32975.29296875, Val Loss: 41847.03515625\n",
      "Epoch 379/3000, Loss: 32928.546875, Val Loss: 41828.203125\n",
      "Epoch 380/3000, Loss: 32485.59375, Val Loss: 41809.4453125\n",
      "Epoch 381/3000, Loss: 32978.828125, Val Loss: 41790.609375\n",
      "Epoch 382/3000, Loss: 32694.15625, Val Loss: 41771.47265625\n",
      "Epoch 383/3000, Loss: 33320.69140625, Val Loss: 41751.7421875\n",
      "Epoch 384/3000, Loss: 32766.845703125, Val Loss: 41732.94921875\n",
      "Epoch 385/3000, Loss: 32652.77734375, Val Loss: 41714.0859375\n",
      "Epoch 386/3000, Loss: 32770.515625, Val Loss: 41695.7734375\n",
      "Epoch 387/3000, Loss: 32754.275390625, Val Loss: 41677.5390625\n",
      "Epoch 388/3000, Loss: 32790.1875, Val Loss: 41659.2734375\n",
      "Epoch 389/3000, Loss: 32165.876953125, Val Loss: 41640.36328125\n",
      "Epoch 390/3000, Loss: 32694.6875, Val Loss: 41621.5703125\n",
      "Epoch 391/3000, Loss: 32425.32421875, Val Loss: 41602.77734375\n",
      "Epoch 392/3000, Loss: 32394.662109375, Val Loss: 41584.1640625\n",
      "Epoch 393/3000, Loss: 32823.8125, Val Loss: 41565.23046875\n",
      "Epoch 394/3000, Loss: 32733.431640625, Val Loss: 41546.4453125\n",
      "Epoch 395/3000, Loss: 32503.095703125, Val Loss: 41526.859375\n",
      "Epoch 396/3000, Loss: 32969.43359375, Val Loss: 41508.1640625\n",
      "Epoch 397/3000, Loss: 32803.9296875, Val Loss: 41490.2265625\n",
      "Epoch 398/3000, Loss: 32614.798828125, Val Loss: 41473.64453125\n",
      "Epoch 399/3000, Loss: 32774.03125, Val Loss: 41458.36328125\n",
      "Epoch 400/3000, Loss: 32555.884765625, Val Loss: 41442.421875\n",
      "Epoch 401/3000, Loss: 32878.2265625, Val Loss: 41425.57421875\n",
      "Epoch 402/3000, Loss: 32714.1484375, Val Loss: 41407.34765625\n",
      "Epoch 403/3000, Loss: 32343.53515625, Val Loss: 41389.39453125\n",
      "Epoch 404/3000, Loss: 32433.615234375, Val Loss: 41371.3125\n",
      "Epoch 405/3000, Loss: 32485.69140625, Val Loss: 41353.51953125\n",
      "Epoch 406/3000, Loss: 32124.3984375, Val Loss: 41335.85546875\n",
      "Epoch 407/3000, Loss: 32454.90234375, Val Loss: 41318.015625\n",
      "Epoch 408/3000, Loss: 32714.076171875, Val Loss: 41301.34375\n",
      "Epoch 409/3000, Loss: 32459.578125, Val Loss: 41284.5390625\n",
      "Epoch 410/3000, Loss: 32445.212890625, Val Loss: 41269.08203125\n",
      "Epoch 411/3000, Loss: 32430.654296875, Val Loss: 41253.9375\n",
      "Epoch 412/3000, Loss: 32169.3984375, Val Loss: 41239.24609375\n",
      "Epoch 413/3000, Loss: 32220.27734375, Val Loss: 41224.2265625\n",
      "Epoch 414/3000, Loss: 32299.65625, Val Loss: 41208.625\n",
      "Epoch 415/3000, Loss: 32116.892578125, Val Loss: 41192.66796875\n",
      "Epoch 416/3000, Loss: 32298.638671875, Val Loss: 41175.61328125\n",
      "Epoch 417/3000, Loss: 31934.1953125, Val Loss: 41158.87109375\n",
      "Epoch 418/3000, Loss: 32201.265625, Val Loss: 41143.1953125\n",
      "Epoch 419/3000, Loss: 31559.494140625, Val Loss: 41126.82421875\n",
      "Epoch 420/3000, Loss: 32196.6015625, Val Loss: 41109.3359375\n",
      "Epoch 421/3000, Loss: 32275.337890625, Val Loss: 41090.6796875\n",
      "Epoch 422/3000, Loss: 32272.326171875, Val Loss: 41073.1875\n",
      "Epoch 423/3000, Loss: 32522.4140625, Val Loss: 41056.58203125\n",
      "Epoch 424/3000, Loss: 32030.75390625, Val Loss: 41039.38671875\n",
      "Epoch 425/3000, Loss: 31999.39453125, Val Loss: 41022.04296875\n",
      "Epoch 426/3000, Loss: 32119.919921875, Val Loss: 41003.79296875\n",
      "Epoch 427/3000, Loss: 31951.451171875, Val Loss: 40986.45703125\n",
      "Epoch 428/3000, Loss: 31637.35546875, Val Loss: 40968.8046875\n",
      "Epoch 429/3000, Loss: 32194.654296875, Val Loss: 40952.05859375\n",
      "Epoch 430/3000, Loss: 31388.7734375, Val Loss: 40935.64453125\n",
      "Epoch 431/3000, Loss: 31760.20703125, Val Loss: 40918.91796875\n",
      "Epoch 432/3000, Loss: 31697.232421875, Val Loss: 40901.78125\n",
      "Epoch 433/3000, Loss: 31627.353515625, Val Loss: 40885.234375\n",
      "Epoch 434/3000, Loss: 31361.451171875, Val Loss: 40869.23046875\n",
      "Epoch 435/3000, Loss: 31468.8515625, Val Loss: 40854.4296875\n",
      "Epoch 436/3000, Loss: 31639.333984375, Val Loss: 40839.375\n",
      "Epoch 437/3000, Loss: 31230.654296875, Val Loss: 40824.81640625\n",
      "Epoch 438/3000, Loss: 31639.279296875, Val Loss: 40809.96875\n",
      "Epoch 439/3000, Loss: 31879.388671875, Val Loss: 40794.6796875\n",
      "Epoch 440/3000, Loss: 31448.73046875, Val Loss: 40779.828125\n",
      "Epoch 441/3000, Loss: 31621.796875, Val Loss: 40764.29296875\n",
      "Epoch 442/3000, Loss: 31454.3984375, Val Loss: 40749.01953125\n",
      "Epoch 443/3000, Loss: 31464.28125, Val Loss: 40734.21875\n",
      "Epoch 444/3000, Loss: 31553.208984375, Val Loss: 40717.7265625\n",
      "Epoch 445/3000, Loss: 30996.837890625, Val Loss: 40700.99609375\n",
      "Epoch 446/3000, Loss: 31344.884765625, Val Loss: 40684.75390625\n",
      "Epoch 447/3000, Loss: 31730.970703125, Val Loss: 40667.19140625\n",
      "Epoch 448/3000, Loss: 31213.025390625, Val Loss: 40650.640625\n",
      "Epoch 449/3000, Loss: 31317.75390625, Val Loss: 40634.82421875\n",
      "Epoch 450/3000, Loss: 31499.005859375, Val Loss: 40619.83203125\n",
      "Epoch 451/3000, Loss: 31347.548828125, Val Loss: 40605.01953125\n",
      "Epoch 452/3000, Loss: 31631.794921875, Val Loss: 40589.6953125\n",
      "Epoch 453/3000, Loss: 31469.70703125, Val Loss: 40573.09375\n",
      "Epoch 454/3000, Loss: 31143.443359375, Val Loss: 40556.72265625\n",
      "Epoch 455/3000, Loss: 31507.904296875, Val Loss: 40540.51953125\n",
      "Epoch 456/3000, Loss: 31170.12890625, Val Loss: 40524.4453125\n",
      "Epoch 457/3000, Loss: 31166.501953125, Val Loss: 40508.609375\n",
      "Epoch 458/3000, Loss: 31528.611328125, Val Loss: 40493.2421875\n",
      "Epoch 459/3000, Loss: 31178.24609375, Val Loss: 40478.06640625\n",
      "Epoch 460/3000, Loss: 31572.486328125, Val Loss: 40464.12890625\n",
      "Epoch 461/3000, Loss: 31365.150390625, Val Loss: 40450.38671875\n",
      "Epoch 462/3000, Loss: 31037.623046875, Val Loss: 40437.13671875\n",
      "Epoch 463/3000, Loss: 31025.111328125, Val Loss: 40424.33203125\n",
      "Epoch 464/3000, Loss: 31314.619140625, Val Loss: 40412.046875\n",
      "Epoch 465/3000, Loss: 31120.044921875, Val Loss: 40400.10546875\n",
      "Epoch 466/3000, Loss: 31082.25390625, Val Loss: 40389.0078125\n",
      "Epoch 467/3000, Loss: 31439.046875, Val Loss: 40377.95703125\n",
      "Epoch 468/3000, Loss: 30977.68359375, Val Loss: 40366.984375\n",
      "Epoch 469/3000, Loss: 30855.021484375, Val Loss: 40355.86328125\n",
      "Epoch 470/3000, Loss: 30734.23828125, Val Loss: 40344.05078125\n",
      "Epoch 471/3000, Loss: 31056.84765625, Val Loss: 40331.75\n",
      "Epoch 472/3000, Loss: 31032.099609375, Val Loss: 40320.4921875\n",
      "Epoch 473/3000, Loss: 31248.787109375, Val Loss: 40308.33984375\n",
      "Epoch 474/3000, Loss: 30893.4765625, Val Loss: 40296.046875\n",
      "Epoch 475/3000, Loss: 30612.4453125, Val Loss: 40282.9921875\n",
      "Epoch 476/3000, Loss: 30659.978515625, Val Loss: 40269.97265625\n",
      "Epoch 477/3000, Loss: 30677.54296875, Val Loss: 40257.4296875\n",
      "Epoch 478/3000, Loss: 30950.0234375, Val Loss: 40244.16796875\n",
      "Epoch 479/3000, Loss: 30815.349609375, Val Loss: 40231.234375\n",
      "Epoch 480/3000, Loss: 30865.87109375, Val Loss: 40218.34375\n",
      "Epoch 481/3000, Loss: 30618.080078125, Val Loss: 40205.03515625\n",
      "Epoch 482/3000, Loss: 30666.69921875, Val Loss: 40191.30078125\n",
      "Epoch 483/3000, Loss: 30882.013671875, Val Loss: 40176.26171875\n",
      "Epoch 484/3000, Loss: 30790.634765625, Val Loss: 40161.15234375\n",
      "Epoch 485/3000, Loss: 30737.474609375, Val Loss: 40146.33203125\n",
      "Epoch 486/3000, Loss: 30792.830078125, Val Loss: 40132.5703125\n",
      "Epoch 487/3000, Loss: 31073.703125, Val Loss: 40119.45703125\n",
      "Epoch 488/3000, Loss: 30840.798828125, Val Loss: 40103.81640625\n",
      "Epoch 489/3000, Loss: 30831.4609375, Val Loss: 40088.42578125\n",
      "Epoch 490/3000, Loss: 30567.6953125, Val Loss: 40073.80859375\n",
      "Epoch 491/3000, Loss: 30407.462890625, Val Loss: 40058.46875\n",
      "Epoch 492/3000, Loss: 30845.71484375, Val Loss: 40041.98046875\n",
      "Epoch 493/3000, Loss: 30558.03125, Val Loss: 40025.01953125\n",
      "Epoch 494/3000, Loss: 30156.1171875, Val Loss: 40007.82421875\n",
      "Epoch 495/3000, Loss: 30672.923828125, Val Loss: 39990.4375\n",
      "Epoch 496/3000, Loss: 30371.30859375, Val Loss: 39974.20703125\n",
      "Epoch 497/3000, Loss: 30338.888671875, Val Loss: 39959.13671875\n",
      "Epoch 498/3000, Loss: 30206.375, Val Loss: 39944.984375\n",
      "Epoch 499/3000, Loss: 30621.130859375, Val Loss: 39931.3828125\n",
      "Epoch 500/3000, Loss: 30377.4765625, Val Loss: 39918.59765625\n",
      "Epoch 501/3000, Loss: 30553.93359375, Val Loss: 39906.16015625\n",
      "Epoch 502/3000, Loss: 30478.568359375, Val Loss: 39893.80859375\n",
      "Epoch 503/3000, Loss: 29831.009765625, Val Loss: 39881.0859375\n",
      "Epoch 504/3000, Loss: 29896.822265625, Val Loss: 39870.15625\n",
      "Epoch 505/3000, Loss: 30764.654296875, Val Loss: 39860.7578125\n",
      "Epoch 506/3000, Loss: 30499.4921875, Val Loss: 39850.48046875\n",
      "Epoch 507/3000, Loss: 30159.216796875, Val Loss: 39841.3984375\n",
      "Epoch 508/3000, Loss: 29967.291015625, Val Loss: 39832.40625\n",
      "Epoch 509/3000, Loss: 30046.232421875, Val Loss: 39823.30859375\n",
      "Epoch 510/3000, Loss: 30302.123046875, Val Loss: 39813.09375\n",
      "Epoch 511/3000, Loss: 29878.634765625, Val Loss: 39804.16015625\n",
      "Epoch 512/3000, Loss: 30499.001953125, Val Loss: 39795.42578125\n",
      "Epoch 513/3000, Loss: 30118.087890625, Val Loss: 39785.40625\n",
      "Epoch 514/3000, Loss: 30403.720703125, Val Loss: 39775.4765625\n",
      "Epoch 515/3000, Loss: 30079.509765625, Val Loss: 39765.4921875\n",
      "Epoch 516/3000, Loss: 29848.978515625, Val Loss: 39754.51171875\n",
      "Epoch 517/3000, Loss: 30179.38671875, Val Loss: 39742.7421875\n",
      "Epoch 518/3000, Loss: 30103.341796875, Val Loss: 39730.77734375\n",
      "Epoch 519/3000, Loss: 30194.216796875, Val Loss: 39718.19921875\n",
      "Epoch 520/3000, Loss: 30011.056640625, Val Loss: 39706.18359375\n",
      "Epoch 521/3000, Loss: 30338.76171875, Val Loss: 39694.421875\n",
      "Epoch 522/3000, Loss: 30366.474609375, Val Loss: 39680.67578125\n",
      "Epoch 523/3000, Loss: 30563.25390625, Val Loss: 39664.86328125\n",
      "Epoch 524/3000, Loss: 30418.439453125, Val Loss: 39649.6796875\n",
      "Epoch 525/3000, Loss: 29732.78515625, Val Loss: 39634.34765625\n",
      "Epoch 526/3000, Loss: 29977.138671875, Val Loss: 39618.39453125\n",
      "Epoch 527/3000, Loss: 30121.166015625, Val Loss: 39601.86328125\n",
      "Epoch 528/3000, Loss: 29597.12109375, Val Loss: 39585.95703125\n",
      "Epoch 529/3000, Loss: 29724.708984375, Val Loss: 39570.09765625\n",
      "Epoch 530/3000, Loss: 30024.91796875, Val Loss: 39554.85546875\n",
      "Epoch 531/3000, Loss: 30077.58203125, Val Loss: 39540.77734375\n",
      "Epoch 532/3000, Loss: 30125.4765625, Val Loss: 39526.046875\n",
      "Epoch 533/3000, Loss: 29957.525390625, Val Loss: 39512.22265625\n",
      "Epoch 534/3000, Loss: 29664.55859375, Val Loss: 39497.63671875\n",
      "Epoch 535/3000, Loss: 29663.966796875, Val Loss: 39483.74609375\n",
      "Epoch 536/3000, Loss: 29644.14453125, Val Loss: 39469.81640625\n",
      "Epoch 537/3000, Loss: 29186.541015625, Val Loss: 39456.765625\n",
      "Epoch 538/3000, Loss: 29494.212890625, Val Loss: 39443.9453125\n",
      "Epoch 539/3000, Loss: 29730.6640625, Val Loss: 39432.390625\n",
      "Epoch 540/3000, Loss: 29980.1875, Val Loss: 39419.9296875\n",
      "Epoch 541/3000, Loss: 29738.630859375, Val Loss: 39409.69921875\n",
      "Epoch 542/3000, Loss: 29742.078125, Val Loss: 39399.359375\n",
      "Epoch 543/3000, Loss: 29945.74609375, Val Loss: 39389.97265625\n",
      "Epoch 544/3000, Loss: 29622.70703125, Val Loss: 39381.8125\n",
      "Epoch 545/3000, Loss: 29490.416015625, Val Loss: 39373.546875\n",
      "Epoch 546/3000, Loss: 29814.560546875, Val Loss: 39364.79296875\n",
      "Epoch 547/3000, Loss: 29700.87109375, Val Loss: 39354.80859375\n",
      "Epoch 548/3000, Loss: 29422.74609375, Val Loss: 39345.5\n",
      "Epoch 549/3000, Loss: 29703.3671875, Val Loss: 39334.10546875\n",
      "Epoch 550/3000, Loss: 29364.912109375, Val Loss: 39321.3359375\n",
      "Epoch 551/3000, Loss: 29440.87109375, Val Loss: 39308.3046875\n",
      "Epoch 552/3000, Loss: 29336.751953125, Val Loss: 39295.17578125\n",
      "Epoch 553/3000, Loss: 29559.525390625, Val Loss: 39282.12890625\n",
      "Epoch 554/3000, Loss: 29004.423828125, Val Loss: 39269.7578125\n",
      "Epoch 555/3000, Loss: 29873.216796875, Val Loss: 39258.19140625\n",
      "Epoch 556/3000, Loss: 29204.3125, Val Loss: 39244.984375\n",
      "Epoch 557/3000, Loss: 29772.654296875, Val Loss: 39231.18359375\n",
      "Epoch 558/3000, Loss: 29435.201171875, Val Loss: 39217.20703125\n",
      "Epoch 559/3000, Loss: 29429.767578125, Val Loss: 39202.76953125\n",
      "Epoch 560/3000, Loss: 29762.0234375, Val Loss: 39187.2265625\n",
      "Epoch 561/3000, Loss: 29661.560546875, Val Loss: 39170.3984375\n",
      "Epoch 562/3000, Loss: 29421.28515625, Val Loss: 39154.3359375\n",
      "Epoch 563/3000, Loss: 29241.837890625, Val Loss: 39137.7421875\n",
      "Epoch 564/3000, Loss: 29725.826171875, Val Loss: 39123.49609375\n",
      "Epoch 565/3000, Loss: 29294.2109375, Val Loss: 39110.42578125\n",
      "Epoch 566/3000, Loss: 29391.880859375, Val Loss: 39099.11328125\n",
      "Epoch 567/3000, Loss: 29448.509765625, Val Loss: 39086.671875\n",
      "Epoch 568/3000, Loss: 29622.720703125, Val Loss: 39075.21875\n",
      "Epoch 569/3000, Loss: 29152.375, Val Loss: 39063.92578125\n",
      "Epoch 570/3000, Loss: 29478.986328125, Val Loss: 39052.93359375\n",
      "Epoch 571/3000, Loss: 28945.525390625, Val Loss: 39040.828125\n",
      "Epoch 572/3000, Loss: 29350.423828125, Val Loss: 39029.734375\n",
      "Epoch 573/3000, Loss: 29230.091796875, Val Loss: 39019.91015625\n",
      "Epoch 574/3000, Loss: 28990.828125, Val Loss: 39008.71875\n",
      "Epoch 575/3000, Loss: 29253.103515625, Val Loss: 38998.484375\n",
      "Epoch 576/3000, Loss: 29219.78125, Val Loss: 38988.50390625\n",
      "Epoch 577/3000, Loss: 29119.591796875, Val Loss: 38977.2109375\n",
      "Epoch 578/3000, Loss: 29572.62890625, Val Loss: 38966.3984375\n",
      "Epoch 579/3000, Loss: 29401.2890625, Val Loss: 38954.99609375\n",
      "Epoch 580/3000, Loss: 28837.91015625, Val Loss: 38943.5625\n",
      "Epoch 581/3000, Loss: 29221.814453125, Val Loss: 38932.67578125\n",
      "Epoch 582/3000, Loss: 29028.31640625, Val Loss: 38921.98828125\n",
      "Epoch 583/3000, Loss: 29035.384765625, Val Loss: 38910.2109375\n",
      "Epoch 584/3000, Loss: 29284.427734375, Val Loss: 38897.01953125\n",
      "Epoch 585/3000, Loss: 28910.34765625, Val Loss: 38883.69140625\n",
      "Epoch 586/3000, Loss: 29242.029296875, Val Loss: 38869.64453125\n",
      "Epoch 587/3000, Loss: 29020.9921875, Val Loss: 38854.53125\n",
      "Epoch 588/3000, Loss: 29160.9921875, Val Loss: 38838.92578125\n",
      "Epoch 589/3000, Loss: 29218.17578125, Val Loss: 38823.45703125\n",
      "Epoch 590/3000, Loss: 28477.3359375, Val Loss: 38806.828125\n",
      "Epoch 591/3000, Loss: 28531.455078125, Val Loss: 38791.5390625\n",
      "Epoch 592/3000, Loss: 29058.326171875, Val Loss: 38775.6328125\n",
      "Epoch 593/3000, Loss: 29138.142578125, Val Loss: 38760.35546875\n",
      "Epoch 594/3000, Loss: 28955.197265625, Val Loss: 38745.8671875\n",
      "Epoch 595/3000, Loss: 29046.380859375, Val Loss: 38729.8359375\n",
      "Epoch 596/3000, Loss: 28911.5, Val Loss: 38715.0078125\n",
      "Epoch 597/3000, Loss: 28875.892578125, Val Loss: 38703.07421875\n",
      "Epoch 598/3000, Loss: 28926.64453125, Val Loss: 38691.06640625\n",
      "Epoch 599/3000, Loss: 28521.431640625, Val Loss: 38681.42578125\n",
      "Epoch 600/3000, Loss: 28964.220703125, Val Loss: 38671.3515625\n",
      "Epoch 601/3000, Loss: 28380.1484375, Val Loss: 38662.11328125\n",
      "Epoch 602/3000, Loss: 28864.4765625, Val Loss: 38652.0625\n",
      "Epoch 603/3000, Loss: 28532.611328125, Val Loss: 38642.26171875\n",
      "Epoch 604/3000, Loss: 28630.8359375, Val Loss: 38631.66796875\n",
      "Epoch 605/3000, Loss: 28689.263671875, Val Loss: 38618.95703125\n",
      "Epoch 606/3000, Loss: 28522.814453125, Val Loss: 38605.20703125\n",
      "Epoch 607/3000, Loss: 28572.580078125, Val Loss: 38591.84765625\n",
      "Epoch 608/3000, Loss: 28945.740234375, Val Loss: 38579.40234375\n",
      "Epoch 609/3000, Loss: 28974.228515625, Val Loss: 38565.83984375\n",
      "Epoch 610/3000, Loss: 28281.76953125, Val Loss: 38554.234375\n",
      "Epoch 611/3000, Loss: 28091.4765625, Val Loss: 38542.1875\n",
      "Epoch 612/3000, Loss: 29171.478515625, Val Loss: 38527.98828125\n",
      "Epoch 613/3000, Loss: 28389.0, Val Loss: 38511.7734375\n",
      "Epoch 614/3000, Loss: 28100.21484375, Val Loss: 38495.6015625\n",
      "Epoch 615/3000, Loss: 28809.9296875, Val Loss: 38479.0625\n",
      "Epoch 616/3000, Loss: 28098.91015625, Val Loss: 38462.78125\n",
      "Epoch 617/3000, Loss: 27885.650390625, Val Loss: 38448.52734375\n",
      "Epoch 618/3000, Loss: 28756.31640625, Val Loss: 38436.421875\n",
      "Epoch 619/3000, Loss: 28274.349609375, Val Loss: 38426.03125\n",
      "Epoch 620/3000, Loss: 28616.6796875, Val Loss: 38415.375\n",
      "Epoch 621/3000, Loss: 28260.892578125, Val Loss: 38404.5859375\n",
      "Epoch 622/3000, Loss: 28596.400390625, Val Loss: 38394.02734375\n",
      "Epoch 623/3000, Loss: 28425.48046875, Val Loss: 38382.78125\n",
      "Epoch 624/3000, Loss: 28207.548828125, Val Loss: 38370.62890625\n",
      "Epoch 625/3000, Loss: 28357.58203125, Val Loss: 38357.16796875\n",
      "Epoch 626/3000, Loss: 28391.7890625, Val Loss: 38346.47265625\n",
      "Epoch 627/3000, Loss: 28394.91796875, Val Loss: 38337.6015625\n",
      "Epoch 628/3000, Loss: 28041.669921875, Val Loss: 38326.6328125\n",
      "Epoch 629/3000, Loss: 28234.072265625, Val Loss: 38314.6171875\n",
      "Epoch 630/3000, Loss: 28439.552734375, Val Loss: 38300.9296875\n",
      "Epoch 631/3000, Loss: 28335.517578125, Val Loss: 38285.265625\n",
      "Epoch 632/3000, Loss: 28315.36328125, Val Loss: 38269.5234375\n",
      "Epoch 633/3000, Loss: 28080.060546875, Val Loss: 38256.0546875\n",
      "Epoch 634/3000, Loss: 28061.17578125, Val Loss: 38243.34765625\n",
      "Epoch 635/3000, Loss: 28058.775390625, Val Loss: 38229.265625\n",
      "Epoch 636/3000, Loss: 28088.044921875, Val Loss: 38214.05859375\n",
      "Epoch 637/3000, Loss: 27810.875, Val Loss: 38199.59765625\n",
      "Epoch 638/3000, Loss: 28492.865234375, Val Loss: 38186.39453125\n",
      "Epoch 639/3000, Loss: 28199.53515625, Val Loss: 38174.44140625\n",
      "Epoch 640/3000, Loss: 27758.12109375, Val Loss: 38162.23046875\n",
      "Epoch 641/3000, Loss: 28169.306640625, Val Loss: 38150.609375\n",
      "Epoch 642/3000, Loss: 27806.52734375, Val Loss: 38138.0078125\n",
      "Epoch 643/3000, Loss: 28216.4140625, Val Loss: 38124.9921875\n",
      "Epoch 644/3000, Loss: 27766.96875, Val Loss: 38112.12109375\n",
      "Epoch 645/3000, Loss: 27968.658203125, Val Loss: 38099.23046875\n",
      "Epoch 646/3000, Loss: 28388.78515625, Val Loss: 38086.96484375\n",
      "Epoch 647/3000, Loss: 27867.564453125, Val Loss: 38075.84375\n",
      "Epoch 648/3000, Loss: 27887.078125, Val Loss: 38064.97265625\n",
      "Epoch 649/3000, Loss: 27985.509765625, Val Loss: 38054.36328125\n",
      "Epoch 650/3000, Loss: 27878.5078125, Val Loss: 38045.171875\n",
      "Epoch 651/3000, Loss: 27920.62109375, Val Loss: 38036.1640625\n",
      "Epoch 652/3000, Loss: 27884.47265625, Val Loss: 38028.40234375\n",
      "Epoch 653/3000, Loss: 27821.576171875, Val Loss: 38022.3984375\n",
      "Epoch 654/3000, Loss: 27765.431640625, Val Loss: 38016.41015625\n",
      "Epoch 655/3000, Loss: 27857.4375, Val Loss: 38009.3359375\n",
      "Epoch 656/3000, Loss: 27829.603515625, Val Loss: 38001.48828125\n",
      "Epoch 657/3000, Loss: 27870.623046875, Val Loss: 37993.125\n",
      "Epoch 658/3000, Loss: 27797.623046875, Val Loss: 37984.59765625\n",
      "Epoch 659/3000, Loss: 27558.912109375, Val Loss: 37976.01171875\n",
      "Epoch 660/3000, Loss: 27900.958984375, Val Loss: 37966.98046875\n",
      "Epoch 661/3000, Loss: 27891.291015625, Val Loss: 37956.91015625\n",
      "Epoch 662/3000, Loss: 27276.40625, Val Loss: 37947.62109375\n",
      "Epoch 663/3000, Loss: 27266.02734375, Val Loss: 37937.66015625\n",
      "Epoch 664/3000, Loss: 27356.78125, Val Loss: 37927.078125\n",
      "Epoch 665/3000, Loss: 27796.912109375, Val Loss: 37915.0390625\n",
      "Epoch 666/3000, Loss: 27625.578125, Val Loss: 37900.14453125\n",
      "Epoch 667/3000, Loss: 27434.67578125, Val Loss: 37884.453125\n",
      "Epoch 668/3000, Loss: 27596.126953125, Val Loss: 37868.10546875\n",
      "Epoch 669/3000, Loss: 27673.138671875, Val Loss: 37852.5\n",
      "Epoch 670/3000, Loss: 27514.72265625, Val Loss: 37837.78125\n",
      "Epoch 671/3000, Loss: 27640.005859375, Val Loss: 37821.2578125\n",
      "Epoch 672/3000, Loss: 27800.900390625, Val Loss: 37805.3671875\n",
      "Epoch 673/3000, Loss: 27218.216796875, Val Loss: 37789.78515625\n",
      "Epoch 674/3000, Loss: 26805.44140625, Val Loss: 37773.265625\n",
      "Epoch 675/3000, Loss: 27429.96875, Val Loss: 37757.46875\n",
      "Epoch 676/3000, Loss: 27673.830078125, Val Loss: 37742.9453125\n",
      "Epoch 677/3000, Loss: 27449.173828125, Val Loss: 37728.40234375\n",
      "Epoch 678/3000, Loss: 27224.294921875, Val Loss: 37714.15625\n",
      "Epoch 679/3000, Loss: 27357.509765625, Val Loss: 37699.15625\n",
      "Epoch 680/3000, Loss: 27621.35546875, Val Loss: 37684.16015625\n",
      "Epoch 681/3000, Loss: 27291.732421875, Val Loss: 37668.53515625\n",
      "Epoch 682/3000, Loss: 26958.6171875, Val Loss: 37652.92578125\n",
      "Epoch 683/3000, Loss: 27195.12890625, Val Loss: 37636.984375\n",
      "Epoch 684/3000, Loss: 27131.02734375, Val Loss: 37623.26953125\n",
      "Epoch 685/3000, Loss: 26935.34765625, Val Loss: 37609.01953125\n",
      "Epoch 686/3000, Loss: 27762.41796875, Val Loss: 37593.15625\n",
      "Epoch 687/3000, Loss: 27119.42578125, Val Loss: 37576.5859375\n",
      "Epoch 688/3000, Loss: 27059.427734375, Val Loss: 37561.8515625\n",
      "Epoch 689/3000, Loss: 26951.369140625, Val Loss: 37547.58203125\n",
      "Epoch 690/3000, Loss: 27642.798828125, Val Loss: 37535.27734375\n",
      "Epoch 691/3000, Loss: 27369.458984375, Val Loss: 37522.76953125\n",
      "Epoch 692/3000, Loss: 27476.1015625, Val Loss: 37511.15625\n",
      "Epoch 693/3000, Loss: 26885.84765625, Val Loss: 37499.8203125\n",
      "Epoch 694/3000, Loss: 27079.078125, Val Loss: 37489.55078125\n",
      "Epoch 695/3000, Loss: 27038.75, Val Loss: 37481.10546875\n",
      "Epoch 696/3000, Loss: 26960.9609375, Val Loss: 37473.765625\n",
      "Epoch 697/3000, Loss: 27061.3359375, Val Loss: 37462.7578125\n",
      "Epoch 698/3000, Loss: 27024.650390625, Val Loss: 37451.98828125\n",
      "Epoch 699/3000, Loss: 27179.27734375, Val Loss: 37437.9609375\n",
      "Epoch 700/3000, Loss: 27333.89453125, Val Loss: 37422.58203125\n",
      "Epoch 701/3000, Loss: 27321.2578125, Val Loss: 37408.1015625\n",
      "Epoch 702/3000, Loss: 26923.28125, Val Loss: 37393.97265625\n",
      "Epoch 703/3000, Loss: 26922.30859375, Val Loss: 37377.20703125\n",
      "Epoch 704/3000, Loss: 26694.501953125, Val Loss: 37359.90234375\n",
      "Epoch 705/3000, Loss: 26688.77734375, Val Loss: 37340.98828125\n",
      "Epoch 706/3000, Loss: 26500.529296875, Val Loss: 37323.0078125\n",
      "Epoch 707/3000, Loss: 26713.650390625, Val Loss: 37305.5703125\n",
      "Epoch 708/3000, Loss: 26744.416015625, Val Loss: 37287.46484375\n",
      "Epoch 709/3000, Loss: 26620.51171875, Val Loss: 37269.62890625\n",
      "Epoch 710/3000, Loss: 26931.470703125, Val Loss: 37254.1953125\n",
      "Epoch 711/3000, Loss: 27004.365234375, Val Loss: 37239.2890625\n",
      "Epoch 712/3000, Loss: 27030.6640625, Val Loss: 37224.859375\n",
      "Epoch 713/3000, Loss: 27210.447265625, Val Loss: 37211.4296875\n",
      "Epoch 714/3000, Loss: 26483.326171875, Val Loss: 37199.51171875\n",
      "Epoch 715/3000, Loss: 26883.201171875, Val Loss: 37186.8203125\n",
      "Epoch 716/3000, Loss: 26637.70703125, Val Loss: 37174.51953125\n",
      "Epoch 717/3000, Loss: 26627.080078125, Val Loss: 37163.64453125\n",
      "Epoch 718/3000, Loss: 26603.71484375, Val Loss: 37153.9765625\n",
      "Epoch 719/3000, Loss: 26858.0625, Val Loss: 37144.5234375\n",
      "Epoch 720/3000, Loss: 26623.3125, Val Loss: 37135.25\n",
      "Epoch 721/3000, Loss: 26597.388671875, Val Loss: 37124.06640625\n",
      "Epoch 722/3000, Loss: 26733.99609375, Val Loss: 37109.37890625\n",
      "Epoch 723/3000, Loss: 26724.150390625, Val Loss: 37094.64453125\n",
      "Epoch 724/3000, Loss: 26663.984375, Val Loss: 37078.8203125\n",
      "Epoch 725/3000, Loss: 26652.330078125, Val Loss: 37062.265625\n",
      "Epoch 726/3000, Loss: 26428.931640625, Val Loss: 37045.703125\n",
      "Epoch 727/3000, Loss: 26650.99609375, Val Loss: 37029.3515625\n",
      "Epoch 728/3000, Loss: 26153.53515625, Val Loss: 37014.61328125\n",
      "Epoch 729/3000, Loss: 26701.412109375, Val Loss: 36999.62109375\n",
      "Epoch 730/3000, Loss: 26809.373046875, Val Loss: 36985.76953125\n",
      "Epoch 731/3000, Loss: 26816.978515625, Val Loss: 36973.8046875\n",
      "Epoch 732/3000, Loss: 26487.62109375, Val Loss: 36961.71484375\n",
      "Epoch 733/3000, Loss: 26339.38671875, Val Loss: 36950.29296875\n",
      "Epoch 734/3000, Loss: 26551.60546875, Val Loss: 36939.546875\n",
      "Epoch 735/3000, Loss: 26474.359375, Val Loss: 36927.85546875\n",
      "Epoch 736/3000, Loss: 26502.662109375, Val Loss: 36915.91796875\n",
      "Epoch 737/3000, Loss: 26365.7578125, Val Loss: 36903.3359375\n",
      "Epoch 738/3000, Loss: 26575.421875, Val Loss: 36891.3828125\n",
      "Epoch 739/3000, Loss: 26462.703125, Val Loss: 36879.50390625\n",
      "Epoch 740/3000, Loss: 26376.875, Val Loss: 36868.78125\n",
      "Epoch 741/3000, Loss: 26456.306640625, Val Loss: 36859.16796875\n",
      "Epoch 742/3000, Loss: 26406.02734375, Val Loss: 36851.50390625\n",
      "Epoch 743/3000, Loss: 25446.18359375, Val Loss: 36840.30859375\n",
      "Epoch 744/3000, Loss: 26301.744140625, Val Loss: 36828.0546875\n",
      "Epoch 745/3000, Loss: 26480.13671875, Val Loss: 36820.859375\n",
      "Epoch 746/3000, Loss: 25893.29296875, Val Loss: 36814.1640625\n",
      "Epoch 747/3000, Loss: 26103.21875, Val Loss: 36805.0625\n",
      "Epoch 748/3000, Loss: 26090.341796875, Val Loss: 36793.3046875\n",
      "Epoch 749/3000, Loss: 26629.794921875, Val Loss: 36780.703125\n",
      "Epoch 750/3000, Loss: 26151.93359375, Val Loss: 36766.94921875\n",
      "Epoch 751/3000, Loss: 25879.369140625, Val Loss: 36751.7734375\n",
      "Epoch 752/3000, Loss: 26273.34765625, Val Loss: 36737.10546875\n",
      "Epoch 753/3000, Loss: 25801.607421875, Val Loss: 36720.98046875\n",
      "Epoch 754/3000, Loss: 26352.40625, Val Loss: 36704.6328125\n",
      "Epoch 755/3000, Loss: 26135.935546875, Val Loss: 36686.5546875\n",
      "Epoch 756/3000, Loss: 25900.623046875, Val Loss: 36666.90625\n",
      "Epoch 757/3000, Loss: 26116.453125, Val Loss: 36647.6875\n",
      "Epoch 758/3000, Loss: 26034.681640625, Val Loss: 36629.81640625\n",
      "Epoch 759/3000, Loss: 25880.44921875, Val Loss: 36611.41015625\n",
      "Epoch 760/3000, Loss: 25814.478515625, Val Loss: 36596.69140625\n",
      "Epoch 761/3000, Loss: 25519.162109375, Val Loss: 36580.96484375\n",
      "Epoch 762/3000, Loss: 25459.876953125, Val Loss: 36566.3984375\n",
      "Epoch 763/3000, Loss: 26099.515625, Val Loss: 36553.8671875\n",
      "Epoch 764/3000, Loss: 25563.671875, Val Loss: 36543.3125\n",
      "Epoch 765/3000, Loss: 25946.322265625, Val Loss: 36531.078125\n",
      "Epoch 766/3000, Loss: 25790.15234375, Val Loss: 36517.23828125\n",
      "Epoch 767/3000, Loss: 25939.689453125, Val Loss: 36503.09375\n",
      "Epoch 768/3000, Loss: 25969.88671875, Val Loss: 36489.796875\n",
      "Epoch 769/3000, Loss: 25884.689453125, Val Loss: 36476.7265625\n",
      "Epoch 770/3000, Loss: 25992.236328125, Val Loss: 36462.9140625\n",
      "Epoch 771/3000, Loss: 25715.109375, Val Loss: 36449.015625\n",
      "Epoch 772/3000, Loss: 25957.78125, Val Loss: 36435.2578125\n",
      "Epoch 773/3000, Loss: 26055.744140625, Val Loss: 36424.9296875\n",
      "Epoch 774/3000, Loss: 25368.3515625, Val Loss: 36412.671875\n",
      "Epoch 775/3000, Loss: 25738.51953125, Val Loss: 36400.8671875\n",
      "Epoch 776/3000, Loss: 25769.919921875, Val Loss: 36392.39453125\n",
      "Epoch 777/3000, Loss: 25864.765625, Val Loss: 36383.22265625\n",
      "Epoch 778/3000, Loss: 25469.84765625, Val Loss: 36371.62109375\n",
      "Epoch 779/3000, Loss: 25784.890625, Val Loss: 36358.296875\n",
      "Epoch 780/3000, Loss: 25363.517578125, Val Loss: 36345.15625\n",
      "Epoch 781/3000, Loss: 25465.892578125, Val Loss: 36330.59375\n",
      "Epoch 782/3000, Loss: 25998.896484375, Val Loss: 36317.11328125\n",
      "Epoch 783/3000, Loss: 25178.556640625, Val Loss: 36302.46875\n",
      "Epoch 784/3000, Loss: 25492.65234375, Val Loss: 36287.01953125\n",
      "Epoch 785/3000, Loss: 25494.26171875, Val Loss: 36270.71875\n",
      "Epoch 786/3000, Loss: 25433.087890625, Val Loss: 36253.71484375\n",
      "Epoch 787/3000, Loss: 25418.7734375, Val Loss: 36236.296875\n",
      "Epoch 788/3000, Loss: 25557.31640625, Val Loss: 36216.77734375\n",
      "Epoch 789/3000, Loss: 25438.37109375, Val Loss: 36196.2109375\n",
      "Epoch 790/3000, Loss: 25349.201171875, Val Loss: 36175.26171875\n",
      "Epoch 791/3000, Loss: 25415.109375, Val Loss: 36154.16796875\n",
      "Epoch 792/3000, Loss: 25285.525390625, Val Loss: 36133.703125\n",
      "Epoch 793/3000, Loss: 25789.67578125, Val Loss: 36112.5703125\n",
      "Epoch 794/3000, Loss: 25323.853515625, Val Loss: 36092.06640625\n",
      "Epoch 795/3000, Loss: 25194.154296875, Val Loss: 36071.1015625\n",
      "Epoch 796/3000, Loss: 25414.67578125, Val Loss: 36051.671875\n",
      "Epoch 797/3000, Loss: 25603.87109375, Val Loss: 36036.59765625\n",
      "Epoch 798/3000, Loss: 25633.970703125, Val Loss: 36020.6015625\n",
      "Epoch 799/3000, Loss: 24755.1875, Val Loss: 36006.2109375\n",
      "Epoch 800/3000, Loss: 25278.203125, Val Loss: 35991.5625\n",
      "Epoch 801/3000, Loss: 25473.96875, Val Loss: 35979.06640625\n",
      "Epoch 802/3000, Loss: 25053.84765625, Val Loss: 35966.84765625\n",
      "Epoch 803/3000, Loss: 25261.99609375, Val Loss: 35958.4296875\n",
      "Epoch 804/3000, Loss: 25169.203125, Val Loss: 35951.609375\n",
      "Epoch 805/3000, Loss: 25422.6328125, Val Loss: 35948.06640625\n",
      "Epoch 806/3000, Loss: 25503.28125, Val Loss: 35944.22265625\n",
      "Epoch 807/3000, Loss: 25043.6015625, Val Loss: 35938.50390625\n",
      "Epoch 808/3000, Loss: 25120.044921875, Val Loss: 35935.59375\n",
      "Epoch 809/3000, Loss: 24668.78125, Val Loss: 35929.3125\n",
      "Epoch 810/3000, Loss: 25512.923828125, Val Loss: 35919.984375\n",
      "Epoch 811/3000, Loss: 24694.443359375, Val Loss: 35904.234375\n",
      "Epoch 812/3000, Loss: 24484.041015625, Val Loss: 35884.078125\n",
      "Epoch 813/3000, Loss: 24357.484375, Val Loss: 35861.81640625\n",
      "Epoch 814/3000, Loss: 24498.1796875, Val Loss: 35835.796875\n",
      "Epoch 815/3000, Loss: 24462.2421875, Val Loss: 35812.33203125\n",
      "Epoch 816/3000, Loss: 24836.353515625, Val Loss: 35790.63671875\n",
      "Epoch 817/3000, Loss: 25314.375, Val Loss: 35767.2578125\n",
      "Epoch 818/3000, Loss: 24643.21875, Val Loss: 35743.703125\n",
      "Epoch 819/3000, Loss: 25001.85546875, Val Loss: 35722.4296875\n",
      "Epoch 820/3000, Loss: 24953.630859375, Val Loss: 35702.3125\n",
      "Epoch 821/3000, Loss: 24406.142578125, Val Loss: 35680.1171875\n",
      "Epoch 822/3000, Loss: 24935.0078125, Val Loss: 35659.41015625\n",
      "Epoch 823/3000, Loss: 24391.474609375, Val Loss: 35638.95703125\n",
      "Epoch 824/3000, Loss: 24811.388671875, Val Loss: 35619.16796875\n",
      "Epoch 825/3000, Loss: 24499.876953125, Val Loss: 35601.49609375\n",
      "Epoch 826/3000, Loss: 24429.08984375, Val Loss: 35584.171875\n",
      "Epoch 827/3000, Loss: 24454.1875, Val Loss: 35564.9140625\n",
      "Epoch 828/3000, Loss: 24082.76953125, Val Loss: 35545.60546875\n",
      "Epoch 829/3000, Loss: 24356.4765625, Val Loss: 35526.0625\n",
      "Epoch 830/3000, Loss: 24494.98828125, Val Loss: 35509.9453125\n",
      "Epoch 831/3000, Loss: 24382.361328125, Val Loss: 35497.5546875\n",
      "Epoch 832/3000, Loss: 24517.904296875, Val Loss: 35485.7421875\n",
      "Epoch 833/3000, Loss: 24487.11328125, Val Loss: 35471.40234375\n",
      "Epoch 834/3000, Loss: 24367.998046875, Val Loss: 35456.48046875\n",
      "Epoch 835/3000, Loss: 24508.919921875, Val Loss: 35445.49609375\n",
      "Epoch 836/3000, Loss: 24179.587890625, Val Loss: 35432.8125\n",
      "Epoch 837/3000, Loss: 24304.884765625, Val Loss: 35421.72265625\n",
      "Epoch 838/3000, Loss: 23835.18359375, Val Loss: 35411.23046875\n",
      "Epoch 839/3000, Loss: 24028.111328125, Val Loss: 35400.765625\n",
      "Epoch 840/3000, Loss: 24289.91796875, Val Loss: 35391.85546875\n",
      "Epoch 841/3000, Loss: 24083.314453125, Val Loss: 35382.3515625\n",
      "Epoch 842/3000, Loss: 24308.36328125, Val Loss: 35371.3984375\n",
      "Epoch 843/3000, Loss: 24269.052734375, Val Loss: 35359.44921875\n",
      "Epoch 844/3000, Loss: 23975.232421875, Val Loss: 35346.89453125\n",
      "Epoch 845/3000, Loss: 23868.423828125, Val Loss: 35332.38671875\n",
      "Epoch 846/3000, Loss: 24487.677734375, Val Loss: 35315.78125\n",
      "Epoch 847/3000, Loss: 24058.0625, Val Loss: 35297.60546875\n",
      "Epoch 848/3000, Loss: 24216.953125, Val Loss: 35275.39453125\n",
      "Epoch 849/3000, Loss: 24051.7421875, Val Loss: 35252.234375\n",
      "Epoch 850/3000, Loss: 23984.783203125, Val Loss: 35229.32421875\n",
      "Epoch 851/3000, Loss: 23875.5625, Val Loss: 35203.80078125\n",
      "Epoch 852/3000, Loss: 23829.494140625, Val Loss: 35174.44921875\n",
      "Epoch 853/3000, Loss: 24061.849609375, Val Loss: 35145.6875\n",
      "Epoch 854/3000, Loss: 24682.23828125, Val Loss: 35113.2578125\n",
      "Epoch 855/3000, Loss: 23903.673828125, Val Loss: 35082.3828125\n",
      "Epoch 856/3000, Loss: 23916.068359375, Val Loss: 35053.1796875\n",
      "Epoch 857/3000, Loss: 23596.5703125, Val Loss: 35024.796875\n",
      "Epoch 858/3000, Loss: 23850.453125, Val Loss: 35001.74609375\n",
      "Epoch 859/3000, Loss: 23729.443359375, Val Loss: 34985.07421875\n",
      "Epoch 860/3000, Loss: 24217.337890625, Val Loss: 34970.44921875\n",
      "Epoch 861/3000, Loss: 23719.9140625, Val Loss: 34955.12109375\n",
      "Epoch 862/3000, Loss: 23636.265625, Val Loss: 34941.75\n",
      "Epoch 863/3000, Loss: 23616.576171875, Val Loss: 34930.16015625\n",
      "Epoch 864/3000, Loss: 23737.1171875, Val Loss: 34922.984375\n",
      "Epoch 865/3000, Loss: 23714.8203125, Val Loss: 34914.21875\n",
      "Epoch 866/3000, Loss: 23702.580078125, Val Loss: 34903.17578125\n",
      "Epoch 867/3000, Loss: 23870.408203125, Val Loss: 34889.5234375\n",
      "Epoch 868/3000, Loss: 24037.92578125, Val Loss: 34869.89453125\n",
      "Epoch 869/3000, Loss: 23254.25, Val Loss: 34846.59765625\n",
      "Epoch 870/3000, Loss: 24073.62109375, Val Loss: 34822.99609375\n",
      "Epoch 871/3000, Loss: 23279.083984375, Val Loss: 34797.34375\n",
      "Epoch 872/3000, Loss: 23368.8359375, Val Loss: 34770.6796875\n",
      "Epoch 873/3000, Loss: 23530.138671875, Val Loss: 34747.59375\n",
      "Epoch 874/3000, Loss: 22761.2265625, Val Loss: 34726.40234375\n",
      "Epoch 875/3000, Loss: 23271.98828125, Val Loss: 34703.7734375\n",
      "Epoch 876/3000, Loss: 23381.392578125, Val Loss: 34680.96875\n",
      "Epoch 877/3000, Loss: 23862.125, Val Loss: 34661.43359375\n",
      "Epoch 878/3000, Loss: 23317.017578125, Val Loss: 34641.12890625\n",
      "Epoch 879/3000, Loss: 23466.98046875, Val Loss: 34622.42578125\n",
      "Epoch 880/3000, Loss: 23270.798828125, Val Loss: 34604.75390625\n",
      "Epoch 881/3000, Loss: 23214.861328125, Val Loss: 34594.13671875\n",
      "Epoch 882/3000, Loss: 23376.984375, Val Loss: 34585.9453125\n",
      "Epoch 883/3000, Loss: 23449.34375, Val Loss: 34575.984375\n",
      "Epoch 884/3000, Loss: 23391.693359375, Val Loss: 34557.3125\n",
      "Epoch 885/3000, Loss: 23046.060546875, Val Loss: 34537.2265625\n",
      "Epoch 886/3000, Loss: 23214.84375, Val Loss: 34514.765625\n",
      "Epoch 887/3000, Loss: 23014.9296875, Val Loss: 34487.7578125\n",
      "Epoch 888/3000, Loss: 23608.501953125, Val Loss: 34461.8125\n",
      "Epoch 889/3000, Loss: 23263.833984375, Val Loss: 34436.59375\n",
      "Epoch 890/3000, Loss: 22994.615234375, Val Loss: 34411.10546875\n",
      "Epoch 891/3000, Loss: 22852.12890625, Val Loss: 34388.02734375\n",
      "Epoch 892/3000, Loss: 23254.5, Val Loss: 34362.41796875\n",
      "Epoch 893/3000, Loss: 22800.8984375, Val Loss: 34334.9921875\n",
      "Epoch 894/3000, Loss: 22842.65625, Val Loss: 34312.296875\n",
      "Epoch 895/3000, Loss: 23136.201171875, Val Loss: 34289.65234375\n",
      "Epoch 896/3000, Loss: 23248.58984375, Val Loss: 34264.78125\n",
      "Epoch 897/3000, Loss: 23107.982421875, Val Loss: 34242.1328125\n",
      "Epoch 898/3000, Loss: 23069.38671875, Val Loss: 34221.6953125\n",
      "Epoch 899/3000, Loss: 22946.474609375, Val Loss: 34199.328125\n",
      "Epoch 900/3000, Loss: 22499.7890625, Val Loss: 34177.0\n",
      "Epoch 901/3000, Loss: 23086.310546875, Val Loss: 34158.76171875\n",
      "Epoch 902/3000, Loss: 22956.181640625, Val Loss: 34142.33203125\n",
      "Epoch 903/3000, Loss: 22222.63671875, Val Loss: 34129.62890625\n",
      "Epoch 904/3000, Loss: 23065.326171875, Val Loss: 34115.92578125\n",
      "Epoch 905/3000, Loss: 22645.974609375, Val Loss: 34102.6796875\n",
      "Epoch 906/3000, Loss: 23095.283203125, Val Loss: 34089.2578125\n",
      "Epoch 907/3000, Loss: 22402.078125, Val Loss: 34079.86328125\n",
      "Epoch 908/3000, Loss: 22542.517578125, Val Loss: 34066.36328125\n",
      "Epoch 909/3000, Loss: 22498.796875, Val Loss: 34051.72265625\n",
      "Epoch 910/3000, Loss: 22567.705078125, Val Loss: 34036.26171875\n",
      "Epoch 911/3000, Loss: 22421.578125, Val Loss: 34013.7421875\n",
      "Epoch 912/3000, Loss: 22525.24609375, Val Loss: 33988.76953125\n",
      "Epoch 913/3000, Loss: 22431.9765625, Val Loss: 33960.7421875\n",
      "Epoch 914/3000, Loss: 22385.69921875, Val Loss: 33930.92578125\n",
      "Epoch 915/3000, Loss: 22249.12109375, Val Loss: 33895.48828125\n",
      "Epoch 916/3000, Loss: 22673.955078125, Val Loss: 33856.41015625\n",
      "Epoch 917/3000, Loss: 22542.146484375, Val Loss: 33818.3984375\n",
      "Epoch 918/3000, Loss: 22336.734375, Val Loss: 33787.0703125\n",
      "Epoch 919/3000, Loss: 22179.720703125, Val Loss: 33760.63671875\n",
      "Epoch 920/3000, Loss: 22477.646484375, Val Loss: 33737.6484375\n",
      "Epoch 921/3000, Loss: 22527.009765625, Val Loss: 33718.81640625\n",
      "Epoch 922/3000, Loss: 22314.78515625, Val Loss: 33706.73828125\n",
      "Epoch 923/3000, Loss: 21711.955078125, Val Loss: 33690.3125\n",
      "Epoch 924/3000, Loss: 22343.634765625, Val Loss: 33683.12890625\n",
      "Epoch 925/3000, Loss: 21797.73828125, Val Loss: 33669.8671875\n",
      "Epoch 926/3000, Loss: 22256.103515625, Val Loss: 33655.23046875\n",
      "Epoch 927/3000, Loss: 22069.783203125, Val Loss: 33638.40234375\n",
      "Epoch 928/3000, Loss: 22007.880859375, Val Loss: 33620.08203125\n",
      "Epoch 929/3000, Loss: 22189.32421875, Val Loss: 33600.43359375\n",
      "Epoch 930/3000, Loss: 22398.423828125, Val Loss: 33575.85546875\n",
      "Epoch 931/3000, Loss: 21717.953125, Val Loss: 33551.45703125\n",
      "Epoch 932/3000, Loss: 21205.84375, Val Loss: 33523.55078125\n",
      "Epoch 933/3000, Loss: 22037.767578125, Val Loss: 33492.92578125\n",
      "Epoch 934/3000, Loss: 22064.88671875, Val Loss: 33467.203125\n",
      "Epoch 935/3000, Loss: 22062.216796875, Val Loss: 33444.52734375\n",
      "Epoch 936/3000, Loss: 22154.1640625, Val Loss: 33419.30078125\n",
      "Epoch 937/3000, Loss: 22077.357421875, Val Loss: 33393.421875\n",
      "Epoch 938/3000, Loss: 21849.193359375, Val Loss: 33368.0625\n",
      "Epoch 939/3000, Loss: 21662.267578125, Val Loss: 33342.76171875\n",
      "Epoch 940/3000, Loss: 21858.39453125, Val Loss: 33322.7734375\n",
      "Epoch 941/3000, Loss: 21352.00390625, Val Loss: 33301.515625\n",
      "Epoch 942/3000, Loss: 21667.75390625, Val Loss: 33284.62109375\n",
      "Epoch 943/3000, Loss: 21801.8984375, Val Loss: 33273.4921875\n",
      "Epoch 944/3000, Loss: 21776.69921875, Val Loss: 33264.71484375\n",
      "Epoch 945/3000, Loss: 21785.05078125, Val Loss: 33256.1796875\n",
      "Epoch 946/3000, Loss: 22357.123046875, Val Loss: 33252.9609375\n",
      "Epoch 947/3000, Loss: 21573.62890625, Val Loss: 33251.43359375\n",
      "Epoch 948/3000, Loss: 21622.388671875, Val Loss: 33246.9609375\n",
      "Epoch 949/3000, Loss: 21878.28125, Val Loss: 33238.98828125\n",
      "Epoch 950/3000, Loss: 21312.435546875, Val Loss: 33226.953125\n",
      "Epoch 951/3000, Loss: 21007.138671875, Val Loss: 33206.70703125\n",
      "Epoch 952/3000, Loss: 21280.9375, Val Loss: 33177.76171875\n",
      "Epoch 953/3000, Loss: 21686.193359375, Val Loss: 33151.31640625\n",
      "Epoch 954/3000, Loss: 21270.3671875, Val Loss: 33123.45703125\n",
      "Epoch 955/3000, Loss: 21056.857421875, Val Loss: 33090.7890625\n",
      "Epoch 956/3000, Loss: 20859.841796875, Val Loss: 33054.2421875\n",
      "Epoch 957/3000, Loss: 21233.95703125, Val Loss: 33018.359375\n",
      "Epoch 958/3000, Loss: 21479.404296875, Val Loss: 32981.859375\n",
      "Epoch 959/3000, Loss: 21214.0625, Val Loss: 32953.671875\n",
      "Epoch 960/3000, Loss: 21255.873046875, Val Loss: 32929.8046875\n",
      "Epoch 961/3000, Loss: 21230.3828125, Val Loss: 32911.671875\n",
      "Epoch 962/3000, Loss: 21144.705078125, Val Loss: 32894.171875\n",
      "Epoch 963/3000, Loss: 20608.412109375, Val Loss: 32873.01953125\n",
      "Epoch 964/3000, Loss: 21042.802734375, Val Loss: 32848.15234375\n",
      "Epoch 965/3000, Loss: 20712.67578125, Val Loss: 32823.02734375\n",
      "Epoch 966/3000, Loss: 21586.62890625, Val Loss: 32800.46484375\n",
      "Epoch 967/3000, Loss: 21001.19140625, Val Loss: 32784.06640625\n",
      "Epoch 968/3000, Loss: 21016.537109375, Val Loss: 32772.4296875\n",
      "Epoch 969/3000, Loss: 21051.1875, Val Loss: 32753.953125\n",
      "Epoch 970/3000, Loss: 20715.755859375, Val Loss: 32733.185546875\n",
      "Epoch 971/3000, Loss: 20917.556640625, Val Loss: 32709.77734375\n",
      "Epoch 972/3000, Loss: 20543.806640625, Val Loss: 32682.9140625\n",
      "Epoch 973/3000, Loss: 21528.2890625, Val Loss: 32655.62890625\n",
      "Epoch 974/3000, Loss: 20637.599609375, Val Loss: 32627.62890625\n",
      "Epoch 975/3000, Loss: 21019.283203125, Val Loss: 32599.83984375\n",
      "Epoch 976/3000, Loss: 20979.26171875, Val Loss: 32579.5703125\n",
      "Epoch 977/3000, Loss: 20706.220703125, Val Loss: 32559.248046875\n",
      "Epoch 978/3000, Loss: 20953.939453125, Val Loss: 32535.4765625\n",
      "Epoch 979/3000, Loss: 20660.169921875, Val Loss: 32516.5\n",
      "Epoch 980/3000, Loss: 20602.0703125, Val Loss: 32499.15234375\n",
      "Epoch 981/3000, Loss: 20698.791015625, Val Loss: 32479.755859375\n",
      "Epoch 982/3000, Loss: 20553.947265625, Val Loss: 32466.580078125\n",
      "Epoch 983/3000, Loss: 20276.11328125, Val Loss: 32449.498046875\n",
      "Epoch 984/3000, Loss: 20566.025390625, Val Loss: 32435.890625\n",
      "Epoch 985/3000, Loss: 20405.443359375, Val Loss: 32426.166015625\n",
      "Epoch 986/3000, Loss: 20380.48828125, Val Loss: 32415.0\n",
      "Epoch 987/3000, Loss: 20317.18359375, Val Loss: 32410.533203125\n",
      "Epoch 988/3000, Loss: 20459.810546875, Val Loss: 32397.8515625\n",
      "Epoch 989/3000, Loss: 20139.609375, Val Loss: 32379.04296875\n",
      "Epoch 990/3000, Loss: 20563.765625, Val Loss: 32359.595703125\n",
      "Epoch 991/3000, Loss: 20695.458984375, Val Loss: 32336.275390625\n",
      "Epoch 992/3000, Loss: 20105.037109375, Val Loss: 32311.091796875\n",
      "Epoch 993/3000, Loss: 20281.74609375, Val Loss: 32282.49609375\n",
      "Epoch 994/3000, Loss: 20444.650390625, Val Loss: 32251.876953125\n",
      "Epoch 995/3000, Loss: 20361.9140625, Val Loss: 32218.41015625\n",
      "Epoch 996/3000, Loss: 20159.294921875, Val Loss: 32178.232421875\n",
      "Epoch 997/3000, Loss: 20245.8203125, Val Loss: 32140.279296875\n",
      "Epoch 998/3000, Loss: 20044.49609375, Val Loss: 32105.0390625\n",
      "Epoch 999/3000, Loss: 19949.978515625, Val Loss: 32071.89453125\n",
      "Epoch 1000/3000, Loss: 20148.94921875, Val Loss: 32043.646484375\n",
      "Epoch 1001/3000, Loss: 20279.240234375, Val Loss: 32015.056640625\n",
      "Epoch 1002/3000, Loss: 19964.984375, Val Loss: 31986.900390625\n",
      "Epoch 1003/3000, Loss: 20058.9375, Val Loss: 31960.884765625\n",
      "Epoch 1004/3000, Loss: 20055.599609375, Val Loss: 31936.5234375\n",
      "Epoch 1005/3000, Loss: 19528.78125, Val Loss: 31912.490234375\n",
      "Epoch 1006/3000, Loss: 19444.970703125, Val Loss: 31886.17578125\n",
      "Epoch 1007/3000, Loss: 19892.974609375, Val Loss: 31860.623046875\n",
      "Epoch 1008/3000, Loss: 20194.19140625, Val Loss: 31833.59375\n",
      "Epoch 1009/3000, Loss: 19405.51171875, Val Loss: 31801.23828125\n",
      "Epoch 1010/3000, Loss: 20205.23046875, Val Loss: 31773.13671875\n",
      "Epoch 1011/3000, Loss: 19567.24609375, Val Loss: 31745.130859375\n",
      "Epoch 1012/3000, Loss: 19508.814453125, Val Loss: 31724.12890625\n",
      "Epoch 1013/3000, Loss: 19624.736328125, Val Loss: 31708.15234375\n",
      "Epoch 1014/3000, Loss: 19080.673828125, Val Loss: 31694.919921875\n",
      "Epoch 1015/3000, Loss: 19455.87890625, Val Loss: 31681.333984375\n",
      "Epoch 1016/3000, Loss: 19115.62109375, Val Loss: 31663.037109375\n",
      "Epoch 1017/3000, Loss: 19234.5546875, Val Loss: 31643.3359375\n",
      "Epoch 1018/3000, Loss: 19980.19140625, Val Loss: 31630.005859375\n",
      "Epoch 1019/3000, Loss: 19366.048828125, Val Loss: 31610.6015625\n",
      "Epoch 1020/3000, Loss: 19379.25390625, Val Loss: 31584.8984375\n",
      "Epoch 1021/3000, Loss: 18988.9296875, Val Loss: 31552.732421875\n",
      "Epoch 1022/3000, Loss: 19938.71875, Val Loss: 31521.923828125\n",
      "Epoch 1023/3000, Loss: 19661.97265625, Val Loss: 31494.7265625\n",
      "Epoch 1024/3000, Loss: 19152.98046875, Val Loss: 31470.30859375\n",
      "Epoch 1025/3000, Loss: 19451.001953125, Val Loss: 31445.388671875\n",
      "Epoch 1026/3000, Loss: 18789.740234375, Val Loss: 31419.3046875\n",
      "Epoch 1027/3000, Loss: 19500.572265625, Val Loss: 31393.23828125\n",
      "Epoch 1028/3000, Loss: 19234.373046875, Val Loss: 31364.4296875\n",
      "Epoch 1029/3000, Loss: 19393.62890625, Val Loss: 31332.90234375\n",
      "Epoch 1030/3000, Loss: 19209.125, Val Loss: 31304.220703125\n",
      "Epoch 1031/3000, Loss: 18937.576171875, Val Loss: 31275.138671875\n",
      "Epoch 1032/3000, Loss: 19018.8515625, Val Loss: 31249.171875\n",
      "Epoch 1033/3000, Loss: 19297.916015625, Val Loss: 31223.884765625\n",
      "Epoch 1034/3000, Loss: 18951.626953125, Val Loss: 31203.185546875\n",
      "Epoch 1035/3000, Loss: 18949.15625, Val Loss: 31183.52734375\n",
      "Epoch 1036/3000, Loss: 18676.673828125, Val Loss: 31157.4375\n",
      "Epoch 1037/3000, Loss: 19320.39453125, Val Loss: 31130.09375\n",
      "Epoch 1038/3000, Loss: 18722.78515625, Val Loss: 31103.509765625\n",
      "Epoch 1039/3000, Loss: 18539.806640625, Val Loss: 31076.376953125\n",
      "Epoch 1040/3000, Loss: 18729.107421875, Val Loss: 31051.33203125\n",
      "Epoch 1041/3000, Loss: 18451.97265625, Val Loss: 31024.142578125\n",
      "Epoch 1042/3000, Loss: 18477.755859375, Val Loss: 30999.3359375\n",
      "Epoch 1043/3000, Loss: 18610.501953125, Val Loss: 30978.91015625\n",
      "Epoch 1044/3000, Loss: 19115.009765625, Val Loss: 30958.322265625\n",
      "Epoch 1045/3000, Loss: 19129.74609375, Val Loss: 30943.185546875\n",
      "Epoch 1046/3000, Loss: 18675.166015625, Val Loss: 30925.732421875\n",
      "Epoch 1047/3000, Loss: 18079.30078125, Val Loss: 30908.02734375\n",
      "Epoch 1048/3000, Loss: 18268.33984375, Val Loss: 30886.443359375\n",
      "Epoch 1049/3000, Loss: 18542.74609375, Val Loss: 30859.84765625\n",
      "Epoch 1050/3000, Loss: 18761.564453125, Val Loss: 30831.25390625\n",
      "Epoch 1051/3000, Loss: 18203.98828125, Val Loss: 30799.97265625\n",
      "Epoch 1052/3000, Loss: 18329.6875, Val Loss: 30768.541015625\n",
      "Epoch 1053/3000, Loss: 18249.9765625, Val Loss: 30748.705078125\n",
      "Epoch 1054/3000, Loss: 18656.287109375, Val Loss: 30730.841796875\n",
      "Epoch 1055/3000, Loss: 17772.625, Val Loss: 30714.947265625\n",
      "Epoch 1056/3000, Loss: 18667.52734375, Val Loss: 30706.443359375\n",
      "Epoch 1057/3000, Loss: 18263.6640625, Val Loss: 30695.833984375\n",
      "Epoch 1058/3000, Loss: 18083.109375, Val Loss: 30686.310546875\n",
      "Epoch 1059/3000, Loss: 17852.466796875, Val Loss: 30678.8125\n",
      "Epoch 1060/3000, Loss: 18194.970703125, Val Loss: 30668.009765625\n",
      "Epoch 1061/3000, Loss: 17981.96484375, Val Loss: 30658.328125\n",
      "Epoch 1062/3000, Loss: 17950.18359375, Val Loss: 30645.8203125\n",
      "Epoch 1063/3000, Loss: 18286.150390625, Val Loss: 30632.1328125\n",
      "Epoch 1064/3000, Loss: 18360.591796875, Val Loss: 30615.50390625\n",
      "Epoch 1065/3000, Loss: 17489.5625, Val Loss: 30594.841796875\n",
      "Epoch 1066/3000, Loss: 18134.623046875, Val Loss: 30571.0703125\n",
      "Epoch 1067/3000, Loss: 17793.8828125, Val Loss: 30545.03515625\n",
      "Epoch 1068/3000, Loss: 18004.728515625, Val Loss: 30520.73828125\n",
      "Epoch 1069/3000, Loss: 17408.525390625, Val Loss: 30493.970703125\n",
      "Epoch 1070/3000, Loss: 18297.357421875, Val Loss: 30473.501953125\n",
      "Epoch 1071/3000, Loss: 18190.83984375, Val Loss: 30459.779296875\n",
      "Epoch 1072/3000, Loss: 17521.25, Val Loss: 30448.37109375\n",
      "Epoch 1073/3000, Loss: 17787.9453125, Val Loss: 30434.642578125\n",
      "Epoch 1074/3000, Loss: 17622.36328125, Val Loss: 30416.6953125\n",
      "Epoch 1075/3000, Loss: 17758.62890625, Val Loss: 30387.1328125\n",
      "Epoch 1076/3000, Loss: 17825.869140625, Val Loss: 30356.228515625\n",
      "Epoch 1077/3000, Loss: 18052.240234375, Val Loss: 30325.84765625\n",
      "Epoch 1078/3000, Loss: 17805.68359375, Val Loss: 30293.978515625\n",
      "Epoch 1079/3000, Loss: 17367.37109375, Val Loss: 30262.080078125\n",
      "Epoch 1080/3000, Loss: 17427.8515625, Val Loss: 30239.89453125\n",
      "Epoch 1081/3000, Loss: 17419.111328125, Val Loss: 30215.986328125\n",
      "Epoch 1082/3000, Loss: 17585.251953125, Val Loss: 30198.849609375\n",
      "Epoch 1083/3000, Loss: 17513.3125, Val Loss: 30179.486328125\n",
      "Epoch 1084/3000, Loss: 17331.716796875, Val Loss: 30163.19921875\n",
      "Epoch 1085/3000, Loss: 17394.892578125, Val Loss: 30147.458984375\n",
      "Epoch 1086/3000, Loss: 17922.34765625, Val Loss: 30132.150390625\n",
      "Epoch 1087/3000, Loss: 17465.251953125, Val Loss: 30118.685546875\n",
      "Epoch 1088/3000, Loss: 17241.931640625, Val Loss: 30098.859375\n",
      "Epoch 1089/3000, Loss: 17052.576171875, Val Loss: 30076.23828125\n",
      "Epoch 1090/3000, Loss: 17550.337890625, Val Loss: 30052.380859375\n",
      "Epoch 1091/3000, Loss: 16946.697265625, Val Loss: 30027.908203125\n",
      "Epoch 1092/3000, Loss: 17491.681640625, Val Loss: 30000.650390625\n",
      "Epoch 1093/3000, Loss: 17472.466796875, Val Loss: 29968.943359375\n",
      "Epoch 1094/3000, Loss: 17041.75, Val Loss: 29932.7109375\n",
      "Epoch 1095/3000, Loss: 17348.9453125, Val Loss: 29898.955078125\n",
      "Epoch 1096/3000, Loss: 17089.568359375, Val Loss: 29860.830078125\n",
      "Epoch 1097/3000, Loss: 17102.90625, Val Loss: 29813.98046875\n",
      "Epoch 1098/3000, Loss: 17021.115234375, Val Loss: 29775.775390625\n",
      "Epoch 1099/3000, Loss: 16909.2734375, Val Loss: 29739.44140625\n",
      "Epoch 1100/3000, Loss: 16973.78125, Val Loss: 29703.3046875\n",
      "Epoch 1101/3000, Loss: 17149.2578125, Val Loss: 29663.775390625\n",
      "Epoch 1102/3000, Loss: 17058.30078125, Val Loss: 29636.65625\n",
      "Epoch 1103/3000, Loss: 16867.623046875, Val Loss: 29608.296875\n",
      "Epoch 1104/3000, Loss: 16528.859375, Val Loss: 29576.55078125\n",
      "Epoch 1105/3000, Loss: 16969.677734375, Val Loss: 29556.53125\n",
      "Epoch 1106/3000, Loss: 16421.171875, Val Loss: 29542.419921875\n",
      "Epoch 1107/3000, Loss: 16784.951171875, Val Loss: 29530.78125\n",
      "Epoch 1108/3000, Loss: 16699.078125, Val Loss: 29527.830078125\n",
      "Epoch 1109/3000, Loss: 16550.728515625, Val Loss: 29516.787109375\n",
      "Epoch 1110/3000, Loss: 17195.68359375, Val Loss: 29501.728515625\n",
      "Epoch 1111/3000, Loss: 16379.7314453125, Val Loss: 29485.689453125\n",
      "Epoch 1112/3000, Loss: 16484.505859375, Val Loss: 29466.814453125\n",
      "Epoch 1113/3000, Loss: 16360.787109375, Val Loss: 29442.189453125\n",
      "Epoch 1114/3000, Loss: 16758.486328125, Val Loss: 29413.708984375\n",
      "Epoch 1115/3000, Loss: 16408.39453125, Val Loss: 29385.93359375\n",
      "Epoch 1116/3000, Loss: 16441.6015625, Val Loss: 29356.1796875\n",
      "Epoch 1117/3000, Loss: 16332.482421875, Val Loss: 29332.47265625\n",
      "Epoch 1118/3000, Loss: 16543.359375, Val Loss: 29310.56640625\n",
      "Epoch 1119/3000, Loss: 16151.0517578125, Val Loss: 29285.041015625\n",
      "Epoch 1120/3000, Loss: 15999.5166015625, Val Loss: 29259.326171875\n",
      "Epoch 1121/3000, Loss: 16336.6826171875, Val Loss: 29238.232421875\n",
      "Epoch 1122/3000, Loss: 16415.314453125, Val Loss: 29221.173828125\n",
      "Epoch 1123/3000, Loss: 15865.384765625, Val Loss: 29205.279296875\n",
      "Epoch 1124/3000, Loss: 16060.875, Val Loss: 29186.59375\n",
      "Epoch 1125/3000, Loss: 16260.4658203125, Val Loss: 29166.6953125\n",
      "Epoch 1126/3000, Loss: 16308.4150390625, Val Loss: 29142.70703125\n",
      "Epoch 1127/3000, Loss: 16402.400390625, Val Loss: 29124.435546875\n",
      "Epoch 1128/3000, Loss: 15834.3779296875, Val Loss: 29107.99609375\n",
      "Epoch 1129/3000, Loss: 15428.755859375, Val Loss: 29083.244140625\n",
      "Epoch 1130/3000, Loss: 15946.1474609375, Val Loss: 29052.8203125\n",
      "Epoch 1131/3000, Loss: 15821.6533203125, Val Loss: 29032.458984375\n",
      "Epoch 1132/3000, Loss: 15838.9306640625, Val Loss: 29005.78125\n",
      "Epoch 1133/3000, Loss: 15897.904296875, Val Loss: 28982.427734375\n",
      "Epoch 1134/3000, Loss: 15860.9423828125, Val Loss: 28965.498046875\n",
      "Epoch 1135/3000, Loss: 15957.4365234375, Val Loss: 28937.685546875\n",
      "Epoch 1136/3000, Loss: 16106.99609375, Val Loss: 28906.83984375\n",
      "Epoch 1137/3000, Loss: 15606.1064453125, Val Loss: 28868.921875\n",
      "Epoch 1138/3000, Loss: 15251.30078125, Val Loss: 28833.416015625\n",
      "Epoch 1139/3000, Loss: 15539.421875, Val Loss: 28787.390625\n",
      "Epoch 1140/3000, Loss: 15937.716796875, Val Loss: 28745.10546875\n",
      "Epoch 1141/3000, Loss: 15934.306640625, Val Loss: 28708.564453125\n",
      "Epoch 1142/3000, Loss: 15653.203125, Val Loss: 28677.052734375\n",
      "Epoch 1143/3000, Loss: 15555.0302734375, Val Loss: 28655.744140625\n",
      "Epoch 1144/3000, Loss: 15581.1279296875, Val Loss: 28633.501953125\n",
      "Epoch 1145/3000, Loss: 15151.9775390625, Val Loss: 28616.1015625\n",
      "Epoch 1146/3000, Loss: 15256.828125, Val Loss: 28607.166015625\n",
      "Epoch 1147/3000, Loss: 15502.02734375, Val Loss: 28602.1953125\n",
      "Epoch 1148/3000, Loss: 15071.8837890625, Val Loss: 28593.58203125\n",
      "Epoch 1149/3000, Loss: 15679.5537109375, Val Loss: 28590.515625\n",
      "Epoch 1150/3000, Loss: 15445.0419921875, Val Loss: 28591.44921875\n",
      "Epoch 1151/3000, Loss: 15555.2177734375, Val Loss: 28596.404296875\n",
      "Epoch 1152/3000, Loss: 15045.69921875, Val Loss: 28596.681640625\n",
      "Epoch 1153/3000, Loss: 15146.080078125, Val Loss: 28588.28515625\n",
      "Epoch 1154/3000, Loss: 15622.6630859375, Val Loss: 28586.900390625\n",
      "Epoch 1155/3000, Loss: 15283.5205078125, Val Loss: 28579.271484375\n",
      "Epoch 1156/3000, Loss: 15275.7451171875, Val Loss: 28564.884765625\n",
      "Epoch 1157/3000, Loss: 15119.38671875, Val Loss: 28539.96875\n",
      "Epoch 1158/3000, Loss: 15044.474609375, Val Loss: 28507.607421875\n",
      "Epoch 1159/3000, Loss: 15295.7431640625, Val Loss: 28476.615234375\n",
      "Epoch 1160/3000, Loss: 14891.072265625, Val Loss: 28455.78125\n",
      "Epoch 1161/3000, Loss: 15351.69140625, Val Loss: 28438.93359375\n",
      "Epoch 1162/3000, Loss: 14818.8974609375, Val Loss: 28423.259765625\n",
      "Epoch 1163/3000, Loss: 14684.4404296875, Val Loss: 28408.30859375\n",
      "Epoch 1164/3000, Loss: 15049.076171875, Val Loss: 28390.73828125\n",
      "Epoch 1165/3000, Loss: 15196.072265625, Val Loss: 28360.5859375\n",
      "Epoch 1166/3000, Loss: 14465.544921875, Val Loss: 28330.46484375\n",
      "Epoch 1167/3000, Loss: 14885.087890625, Val Loss: 28303.1328125\n",
      "Epoch 1168/3000, Loss: 14630.404296875, Val Loss: 28276.22265625\n",
      "Epoch 1169/3000, Loss: 15110.55078125, Val Loss: 28246.14453125\n",
      "Epoch 1170/3000, Loss: 14571.623046875, Val Loss: 28197.830078125\n",
      "Epoch 1171/3000, Loss: 14405.0927734375, Val Loss: 28147.794921875\n",
      "Epoch 1172/3000, Loss: 14403.9501953125, Val Loss: 28103.826171875\n",
      "Epoch 1173/3000, Loss: 15036.259765625, Val Loss: 28067.509765625\n",
      "Epoch 1174/3000, Loss: 15190.9453125, Val Loss: 28048.884765625\n",
      "Epoch 1175/3000, Loss: 14810.234375, Val Loss: 28042.185546875\n",
      "Epoch 1176/3000, Loss: 14252.6806640625, Val Loss: 28047.58984375\n",
      "Epoch 1177/3000, Loss: 14171.048828125, Val Loss: 28048.009765625\n",
      "Epoch 1178/3000, Loss: 14994.865234375, Val Loss: 28051.404296875\n",
      "Epoch 1179/3000, Loss: 13808.0, Val Loss: 28041.91015625\n",
      "Epoch 1180/3000, Loss: 13779.0810546875, Val Loss: 28037.533203125\n",
      "Epoch 1181/3000, Loss: 14292.484375, Val Loss: 28021.203125\n",
      "Epoch 1182/3000, Loss: 14270.630859375, Val Loss: 28009.0546875\n",
      "Epoch 1183/3000, Loss: 14315.5888671875, Val Loss: 27993.287109375\n",
      "Epoch 1184/3000, Loss: 14096.08984375, Val Loss: 27982.392578125\n",
      "Epoch 1185/3000, Loss: 13713.3564453125, Val Loss: 27962.86328125\n",
      "Epoch 1186/3000, Loss: 13870.810546875, Val Loss: 27941.013671875\n",
      "Epoch 1187/3000, Loss: 14525.0283203125, Val Loss: 27919.607421875\n",
      "Epoch 1188/3000, Loss: 14437.32421875, Val Loss: 27890.5625\n",
      "Epoch 1189/3000, Loss: 14085.75390625, Val Loss: 27864.265625\n",
      "Epoch 1190/3000, Loss: 13870.37890625, Val Loss: 27829.6640625\n",
      "Epoch 1191/3000, Loss: 14244.8193359375, Val Loss: 27801.755859375\n",
      "Epoch 1192/3000, Loss: 14322.9345703125, Val Loss: 27780.05859375\n",
      "Epoch 1193/3000, Loss: 13943.5986328125, Val Loss: 27751.291015625\n",
      "Epoch 1194/3000, Loss: 13543.4541015625, Val Loss: 27712.892578125\n",
      "Epoch 1195/3000, Loss: 13843.8759765625, Val Loss: 27677.236328125\n",
      "Epoch 1196/3000, Loss: 14102.9033203125, Val Loss: 27646.330078125\n",
      "Epoch 1197/3000, Loss: 13704.7685546875, Val Loss: 27611.298828125\n",
      "Epoch 1198/3000, Loss: 13978.8623046875, Val Loss: 27584.142578125\n",
      "Epoch 1199/3000, Loss: 13413.2802734375, Val Loss: 27561.521484375\n",
      "Epoch 1200/3000, Loss: 13672.7802734375, Val Loss: 27542.4296875\n",
      "Epoch 1201/3000, Loss: 14197.953125, Val Loss: 27534.876953125\n",
      "Epoch 1202/3000, Loss: 13498.2939453125, Val Loss: 27533.84765625\n",
      "Epoch 1203/3000, Loss: 13574.611328125, Val Loss: 27513.509765625\n",
      "Epoch 1204/3000, Loss: 13749.7607421875, Val Loss: 27483.123046875\n",
      "Epoch 1205/3000, Loss: 13408.330078125, Val Loss: 27449.125\n",
      "Epoch 1206/3000, Loss: 13203.8984375, Val Loss: 27420.2109375\n",
      "Epoch 1207/3000, Loss: 14317.4453125, Val Loss: 27407.912109375\n",
      "Epoch 1208/3000, Loss: 13956.9208984375, Val Loss: 27404.8125\n",
      "Epoch 1209/3000, Loss: 13610.283203125, Val Loss: 27388.80078125\n",
      "Epoch 1210/3000, Loss: 13694.017578125, Val Loss: 27377.4765625\n",
      "Epoch 1211/3000, Loss: 13445.5283203125, Val Loss: 27369.265625\n",
      "Epoch 1212/3000, Loss: 13403.1806640625, Val Loss: 27359.83203125\n",
      "Epoch 1213/3000, Loss: 13832.3525390625, Val Loss: 27335.1875\n",
      "Epoch 1214/3000, Loss: 13526.615234375, Val Loss: 27301.609375\n",
      "Epoch 1215/3000, Loss: 13223.603515625, Val Loss: 27274.224609375\n",
      "Epoch 1216/3000, Loss: 13590.30078125, Val Loss: 27252.556640625\n",
      "Epoch 1217/3000, Loss: 13403.9052734375, Val Loss: 27220.6484375\n",
      "Epoch 1218/3000, Loss: 13375.06640625, Val Loss: 27196.76953125\n",
      "Epoch 1219/3000, Loss: 13304.5029296875, Val Loss: 27175.607421875\n",
      "Epoch 1220/3000, Loss: 13317.611328125, Val Loss: 27157.58984375\n",
      "Epoch 1221/3000, Loss: 13025.64453125, Val Loss: 27152.8515625\n",
      "Epoch 1222/3000, Loss: 13343.95703125, Val Loss: 27147.697265625\n",
      "Epoch 1223/3000, Loss: 13430.8466796875, Val Loss: 27133.388671875\n",
      "Epoch 1224/3000, Loss: 12571.681640625, Val Loss: 27113.375\n",
      "Epoch 1225/3000, Loss: 13064.994140625, Val Loss: 27076.0\n",
      "Epoch 1226/3000, Loss: 13265.7900390625, Val Loss: 27033.072265625\n",
      "Epoch 1227/3000, Loss: 13119.91796875, Val Loss: 26999.5703125\n",
      "Epoch 1228/3000, Loss: 13388.9248046875, Val Loss: 26974.759765625\n",
      "Epoch 1229/3000, Loss: 13393.4189453125, Val Loss: 26953.0078125\n",
      "Epoch 1230/3000, Loss: 12426.0556640625, Val Loss: 26935.96875\n",
      "Epoch 1231/3000, Loss: 12961.5234375, Val Loss: 26907.787109375\n",
      "Epoch 1232/3000, Loss: 12607.6923828125, Val Loss: 26887.7421875\n",
      "Epoch 1233/3000, Loss: 12811.4375, Val Loss: 26863.19140625\n",
      "Epoch 1234/3000, Loss: 13289.8486328125, Val Loss: 26845.69140625\n",
      "Epoch 1235/3000, Loss: 13098.478515625, Val Loss: 26830.986328125\n",
      "Epoch 1236/3000, Loss: 13329.1240234375, Val Loss: 26815.447265625\n",
      "Epoch 1237/3000, Loss: 12877.6728515625, Val Loss: 26802.16015625\n",
      "Epoch 1238/3000, Loss: 12709.8935546875, Val Loss: 26778.80078125\n",
      "Epoch 1239/3000, Loss: 13521.755859375, Val Loss: 26767.681640625\n",
      "Epoch 1240/3000, Loss: 12444.1572265625, Val Loss: 26754.033203125\n",
      "Epoch 1241/3000, Loss: 13010.8759765625, Val Loss: 26741.1328125\n",
      "Epoch 1242/3000, Loss: 12719.6455078125, Val Loss: 26720.63671875\n",
      "Epoch 1243/3000, Loss: 12920.4091796875, Val Loss: 26686.4296875\n",
      "Epoch 1244/3000, Loss: 12428.8447265625, Val Loss: 26658.791015625\n",
      "Epoch 1245/3000, Loss: 12692.3828125, Val Loss: 26624.05078125\n",
      "Epoch 1246/3000, Loss: 13279.177734375, Val Loss: 26601.544921875\n",
      "Epoch 1247/3000, Loss: 12785.8701171875, Val Loss: 26580.5625\n",
      "Epoch 1248/3000, Loss: 12753.73046875, Val Loss: 26562.748046875\n",
      "Epoch 1249/3000, Loss: 12401.4775390625, Val Loss: 26547.900390625\n",
      "Epoch 1250/3000, Loss: 12292.9892578125, Val Loss: 26538.21484375\n",
      "Epoch 1251/3000, Loss: 12449.4072265625, Val Loss: 26529.751953125\n",
      "Epoch 1252/3000, Loss: 12782.9521484375, Val Loss: 26520.244140625\n",
      "Epoch 1253/3000, Loss: 12740.1611328125, Val Loss: 26504.298828125\n",
      "Epoch 1254/3000, Loss: 12494.4345703125, Val Loss: 26503.697265625\n",
      "Epoch 1255/3000, Loss: 12268.1181640625, Val Loss: 26498.001953125\n",
      "Epoch 1256/3000, Loss: 11903.9453125, Val Loss: 26480.822265625\n",
      "Epoch 1257/3000, Loss: 12572.43359375, Val Loss: 26454.369140625\n",
      "Epoch 1258/3000, Loss: 12553.2158203125, Val Loss: 26422.34765625\n",
      "Epoch 1259/3000, Loss: 12502.982421875, Val Loss: 26388.111328125\n",
      "Epoch 1260/3000, Loss: 12626.2314453125, Val Loss: 26361.54296875\n",
      "Epoch 1261/3000, Loss: 12708.4931640625, Val Loss: 26322.328125\n",
      "Epoch 1262/3000, Loss: 12273.9111328125, Val Loss: 26301.33984375\n",
      "Epoch 1263/3000, Loss: 12158.8515625, Val Loss: 26293.701171875\n",
      "Epoch 1264/3000, Loss: 12527.484375, Val Loss: 26294.734375\n",
      "Epoch 1265/3000, Loss: 12966.39453125, Val Loss: 26303.28515625\n",
      "Epoch 1266/3000, Loss: 13033.16015625, Val Loss: 26318.333984375\n",
      "Epoch 1267/3000, Loss: 12891.603515625, Val Loss: 26321.939453125\n",
      "Epoch 1268/3000, Loss: 12486.37890625, Val Loss: 26321.6484375\n",
      "Epoch 1269/3000, Loss: 12238.28515625, Val Loss: 26315.57421875\n",
      "Epoch 1270/3000, Loss: 12277.5146484375, Val Loss: 26303.65625\n",
      "Epoch 1271/3000, Loss: 11892.6181640625, Val Loss: 26294.6796875\n",
      "Epoch 1272/3000, Loss: 12099.591796875, Val Loss: 26286.849609375\n",
      "Epoch 1273/3000, Loss: 11394.4677734375, Val Loss: 26278.390625\n",
      "Epoch 1274/3000, Loss: 12307.943359375, Val Loss: 26276.248046875\n",
      "Epoch 1275/3000, Loss: 12124.1611328125, Val Loss: 26254.244140625\n",
      "Epoch 1276/3000, Loss: 12438.6279296875, Val Loss: 26235.56640625\n",
      "Epoch 1277/3000, Loss: 11854.318359375, Val Loss: 26216.333984375\n",
      "Epoch 1278/3000, Loss: 12213.591796875, Val Loss: 26203.943359375\n",
      "Epoch 1279/3000, Loss: 12048.4296875, Val Loss: 26187.279296875\n",
      "Epoch 1280/3000, Loss: 11857.484375, Val Loss: 26164.859375\n",
      "Epoch 1281/3000, Loss: 11948.3212890625, Val Loss: 26146.017578125\n",
      "Epoch 1282/3000, Loss: 11935.876953125, Val Loss: 26119.37109375\n",
      "Epoch 1283/3000, Loss: 11910.56640625, Val Loss: 26098.8046875\n",
      "Epoch 1284/3000, Loss: 11662.8671875, Val Loss: 26070.76171875\n",
      "Epoch 1285/3000, Loss: 11993.3056640625, Val Loss: 26033.51171875\n",
      "Epoch 1286/3000, Loss: 11984.45703125, Val Loss: 26006.068359375\n",
      "Epoch 1287/3000, Loss: 12231.7158203125, Val Loss: 25989.322265625\n",
      "Epoch 1288/3000, Loss: 11660.818359375, Val Loss: 25982.02734375\n",
      "Epoch 1289/3000, Loss: 12266.83203125, Val Loss: 25966.240234375\n",
      "Epoch 1290/3000, Loss: 11920.6064453125, Val Loss: 25945.6875\n",
      "Epoch 1291/3000, Loss: 11753.9462890625, Val Loss: 25911.154296875\n",
      "Epoch 1292/3000, Loss: 12114.0400390625, Val Loss: 25892.201171875\n",
      "Epoch 1293/3000, Loss: 11562.912109375, Val Loss: 25872.583984375\n",
      "Epoch 1294/3000, Loss: 11811.990234375, Val Loss: 25865.787109375\n",
      "Epoch 1295/3000, Loss: 11452.509765625, Val Loss: 25851.98046875\n",
      "Epoch 1296/3000, Loss: 11248.583984375, Val Loss: 25834.19921875\n",
      "Epoch 1297/3000, Loss: 11590.87109375, Val Loss: 25822.880859375\n",
      "Epoch 1298/3000, Loss: 11949.4521484375, Val Loss: 25816.755859375\n",
      "Epoch 1299/3000, Loss: 11322.65625, Val Loss: 25799.603515625\n",
      "Epoch 1300/3000, Loss: 11497.2529296875, Val Loss: 25770.59375\n",
      "Epoch 1301/3000, Loss: 11391.7421875, Val Loss: 25731.419921875\n",
      "Epoch 1302/3000, Loss: 11259.0869140625, Val Loss: 25688.11328125\n",
      "Epoch 1303/3000, Loss: 11483.33984375, Val Loss: 25645.23828125\n",
      "Epoch 1304/3000, Loss: 12110.3662109375, Val Loss: 25610.40625\n",
      "Epoch 1305/3000, Loss: 11996.6357421875, Val Loss: 25590.474609375\n",
      "Epoch 1306/3000, Loss: 10938.8935546875, Val Loss: 25569.310546875\n",
      "Epoch 1307/3000, Loss: 11041.037109375, Val Loss: 25538.4375\n",
      "Epoch 1308/3000, Loss: 10980.6181640625, Val Loss: 25514.873046875\n",
      "Epoch 1309/3000, Loss: 11249.9130859375, Val Loss: 25499.17578125\n",
      "Epoch 1310/3000, Loss: 11119.013671875, Val Loss: 25476.439453125\n",
      "Epoch 1311/3000, Loss: 11801.5341796875, Val Loss: 25455.080078125\n",
      "Epoch 1312/3000, Loss: 11195.1494140625, Val Loss: 25445.115234375\n",
      "Epoch 1313/3000, Loss: 11705.1962890625, Val Loss: 25443.380859375\n",
      "Epoch 1314/3000, Loss: 10771.958984375, Val Loss: 25443.716796875\n",
      "Epoch 1315/3000, Loss: 11810.43359375, Val Loss: 25449.716796875\n",
      "Epoch 1316/3000, Loss: 11103.443359375, Val Loss: 25433.375\n",
      "Epoch 1317/3000, Loss: 11256.8076171875, Val Loss: 25391.162109375\n",
      "Epoch 1318/3000, Loss: 11304.5537109375, Val Loss: 25356.23828125\n",
      "Epoch 1319/3000, Loss: 11007.802734375, Val Loss: 25321.53125\n",
      "Epoch 1320/3000, Loss: 11833.9287109375, Val Loss: 25296.263671875\n",
      "Epoch 1321/3000, Loss: 11191.705078125, Val Loss: 25283.2578125\n",
      "Epoch 1322/3000, Loss: 11096.8447265625, Val Loss: 25285.806640625\n",
      "Epoch 1323/3000, Loss: 10455.3076171875, Val Loss: 25298.634765625\n",
      "Epoch 1324/3000, Loss: 10741.6904296875, Val Loss: 25304.734375\n",
      "Epoch 1325/3000, Loss: 10983.7978515625, Val Loss: 25320.748046875\n",
      "Epoch 1326/3000, Loss: 11159.9248046875, Val Loss: 25338.099609375\n",
      "Epoch 1327/3000, Loss: 10705.3388671875, Val Loss: 25358.529296875\n",
      "Epoch 1328/3000, Loss: 11273.1328125, Val Loss: 25367.779296875\n",
      "Epoch 1329/3000, Loss: 11255.109375, Val Loss: 25371.396484375\n",
      "Epoch 1330/3000, Loss: 10451.50390625, Val Loss: 25363.875\n",
      "Epoch 1331/3000, Loss: 11304.9091796875, Val Loss: 25332.59375\n",
      "Epoch 1332/3000, Loss: 10695.734375, Val Loss: 25292.515625\n",
      "Epoch 01332: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 1333/3000, Loss: 10721.05859375, Val Loss: 25272.279296875\n",
      "Epoch 1334/3000, Loss: 10245.2685546875, Val Loss: 25247.142578125\n",
      "Epoch 1335/3000, Loss: 10624.4638671875, Val Loss: 25211.873046875\n",
      "Epoch 1336/3000, Loss: 11142.6806640625, Val Loss: 25181.37890625\n",
      "Epoch 1337/3000, Loss: 10464.73828125, Val Loss: 25148.53515625\n",
      "Epoch 1338/3000, Loss: 10466.595703125, Val Loss: 25115.90625\n",
      "Epoch 1339/3000, Loss: 11540.2021484375, Val Loss: 25090.283203125\n",
      "Epoch 1340/3000, Loss: 10664.064453125, Val Loss: 25075.23828125\n",
      "Epoch 1341/3000, Loss: 10710.4150390625, Val Loss: 25067.078125\n",
      "Epoch 1342/3000, Loss: 10412.396484375, Val Loss: 25066.84765625\n",
      "Epoch 1343/3000, Loss: 10696.1376953125, Val Loss: 25069.15234375\n",
      "Epoch 1344/3000, Loss: 11008.234375, Val Loss: 25074.611328125\n",
      "Epoch 1345/3000, Loss: 10405.1884765625, Val Loss: 25066.72265625\n",
      "Epoch 1346/3000, Loss: 10937.3544921875, Val Loss: 25060.865234375\n",
      "Epoch 1347/3000, Loss: 10395.5546875, Val Loss: 25053.607421875\n",
      "Epoch 1348/3000, Loss: 10281.970703125, Val Loss: 25046.94140625\n",
      "Epoch 1349/3000, Loss: 10565.1298828125, Val Loss: 25042.89453125\n",
      "Epoch 1350/3000, Loss: 10401.0791015625, Val Loss: 25039.158203125\n",
      "Epoch 1351/3000, Loss: 10766.4853515625, Val Loss: 25038.97265625\n",
      "Epoch 1352/3000, Loss: 10906.486328125, Val Loss: 25040.3984375\n",
      "Epoch 1353/3000, Loss: 10384.69921875, Val Loss: 25041.35546875\n",
      "Epoch 1354/3000, Loss: 10727.7509765625, Val Loss: 25038.41796875\n",
      "Epoch 1355/3000, Loss: 10904.8681640625, Val Loss: 25044.5859375\n",
      "Epoch 1356/3000, Loss: 10957.67578125, Val Loss: 25053.7109375\n",
      "Epoch 1357/3000, Loss: 10867.3515625, Val Loss: 25061.09375\n",
      "Epoch 1358/3000, Loss: 11071.1552734375, Val Loss: 25059.41015625\n",
      "Epoch 1359/3000, Loss: 11002.4638671875, Val Loss: 25059.560546875\n",
      "Epoch 1360/3000, Loss: 10784.4482421875, Val Loss: 25057.326171875\n",
      "Epoch 1361/3000, Loss: 10349.7275390625, Val Loss: 25050.46875\n",
      "Epoch 01361: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 1362/3000, Loss: 10727.2958984375, Val Loss: 25046.61328125\n",
      "Epoch 1363/3000, Loss: 10230.7958984375, Val Loss: 25039.408203125\n",
      "Epoch 1364/3000, Loss: 10707.6884765625, Val Loss: 25029.00390625\n",
      "Epoch 1365/3000, Loss: 10748.31640625, Val Loss: 25019.8984375\n",
      "Epoch 1366/3000, Loss: 10622.2197265625, Val Loss: 25010.623046875\n",
      "Epoch 1367/3000, Loss: 10439.2021484375, Val Loss: 25000.36328125\n",
      "Epoch 1368/3000, Loss: 10618.28515625, Val Loss: 24986.607421875\n",
      "Epoch 1369/3000, Loss: 11151.2744140625, Val Loss: 24975.46875\n",
      "Epoch 1370/3000, Loss: 9905.740234375, Val Loss: 24969.13671875\n",
      "Epoch 1371/3000, Loss: 10270.7607421875, Val Loss: 24961.6484375\n",
      "Epoch 1372/3000, Loss: 10254.1591796875, Val Loss: 24953.166015625\n",
      "Epoch 1373/3000, Loss: 10307.03125, Val Loss: 24946.17578125\n",
      "Epoch 1374/3000, Loss: 10399.4638671875, Val Loss: 24939.884765625\n",
      "Epoch 1375/3000, Loss: 10146.7724609375, Val Loss: 24934.955078125\n",
      "Epoch 1376/3000, Loss: 10230.5556640625, Val Loss: 24933.701171875\n",
      "Epoch 1377/3000, Loss: 10612.4521484375, Val Loss: 24936.44140625\n",
      "Epoch 1378/3000, Loss: 10763.6962890625, Val Loss: 24937.611328125\n",
      "Epoch 1379/3000, Loss: 10342.0625, Val Loss: 24937.279296875\n",
      "Epoch 1380/3000, Loss: 10576.869140625, Val Loss: 24937.6640625\n",
      "Epoch 1381/3000, Loss: 10421.6806640625, Val Loss: 24937.875\n",
      "Epoch 1382/3000, Loss: 10715.1181640625, Val Loss: 24941.685546875\n",
      "Epoch 1383/3000, Loss: 10529.5771484375, Val Loss: 24944.837890625\n",
      "Epoch 1384/3000, Loss: 10224.8017578125, Val Loss: 24950.623046875\n",
      "Epoch 1385/3000, Loss: 10445.1943359375, Val Loss: 24956.603515625\n",
      "Epoch 1386/3000, Loss: 9695.23046875, Val Loss: 24958.28515625\n",
      "Epoch 01386: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 1387/3000, Loss: 10288.7431640625, Val Loss: 24959.201171875\n",
      "Epoch 1388/3000, Loss: 10786.3857421875, Val Loss: 24957.76953125\n",
      "Epoch 1389/3000, Loss: 10624.7119140625, Val Loss: 24954.9453125\n",
      "Epoch 1390/3000, Loss: 10399.5673828125, Val Loss: 24952.4375\n",
      "Epoch 1391/3000, Loss: 10387.861328125, Val Loss: 24951.150390625\n",
      "Epoch 1392/3000, Loss: 10492.0205078125, Val Loss: 24950.037109375\n",
      "Epoch 1393/3000, Loss: 10854.4453125, Val Loss: 24947.201171875\n",
      "Epoch 1394/3000, Loss: 10517.5693359375, Val Loss: 24942.759765625\n",
      "Epoch 1395/3000, Loss: 10578.4326171875, Val Loss: 24938.625\n",
      "Epoch 1396/3000, Loss: 10782.7734375, Val Loss: 24933.796875\n",
      "Epoch 1397/3000, Loss: 11094.9169921875, Val Loss: 24928.880859375\n",
      "Epoch 1398/3000, Loss: 9982.2294921875, Val Loss: 24922.6328125\n",
      "Epoch 1399/3000, Loss: 10616.548828125, Val Loss: 24917.0859375\n",
      "Epoch 1400/3000, Loss: 10487.38671875, Val Loss: 24912.580078125\n",
      "Epoch 1401/3000, Loss: 10290.861328125, Val Loss: 24906.609375\n",
      "Epoch 1402/3000, Loss: 9750.8857421875, Val Loss: 24900.65234375\n",
      "Epoch 1403/3000, Loss: 10266.60546875, Val Loss: 24896.638671875\n",
      "Epoch 1404/3000, Loss: 11009.5771484375, Val Loss: 24893.580078125\n",
      "Epoch 1405/3000, Loss: 9987.6162109375, Val Loss: 24890.90234375\n",
      "Epoch 1406/3000, Loss: 10510.55859375, Val Loss: 24889.673828125\n",
      "Epoch 1407/3000, Loss: 10606.2119140625, Val Loss: 24888.18359375\n",
      "Epoch 1408/3000, Loss: 10939.857421875, Val Loss: 24888.234375\n",
      "Epoch 1409/3000, Loss: 10598.587890625, Val Loss: 24888.931640625\n",
      "Epoch 1410/3000, Loss: 10468.9833984375, Val Loss: 24890.216796875\n",
      "Epoch 1411/3000, Loss: 10377.8056640625, Val Loss: 24892.046875\n",
      "Epoch 1412/3000, Loss: 10311.208984375, Val Loss: 24894.4140625\n",
      "Epoch 1413/3000, Loss: 10484.3310546875, Val Loss: 24897.201171875\n",
      "Epoch 1414/3000, Loss: 10238.7646484375, Val Loss: 24900.46875\n",
      "Epoch 1415/3000, Loss: 11002.7568359375, Val Loss: 24903.1171875\n",
      "Epoch 1416/3000, Loss: 10622.361328125, Val Loss: 24905.98046875\n",
      "Epoch 1417/3000, Loss: 9982.7841796875, Val Loss: 24909.33203125\n",
      "Epoch 1418/3000, Loss: 10920.61328125, Val Loss: 24911.671875\n",
      "Epoch 01418: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 1419/3000, Loss: 10449.318359375, Val Loss: 24912.005859375\n",
      "Epoch 1420/3000, Loss: 10610.064453125, Val Loss: 24912.03125\n",
      "Epoch 1421/3000, Loss: 10485.4404296875, Val Loss: 24912.171875\n",
      "Epoch 1422/3000, Loss: 10182.29296875, Val Loss: 24911.9609375\n",
      "Epoch 1423/3000, Loss: 10367.5625, Val Loss: 24910.2578125\n",
      "Epoch 1424/3000, Loss: 10696.7421875, Val Loss: 24908.830078125\n",
      "Epoch 1425/3000, Loss: 10118.66796875, Val Loss: 24907.091796875\n",
      "Epoch 1426/3000, Loss: 10935.8466796875, Val Loss: 24906.80078125\n",
      "Epoch 1427/3000, Loss: 10628.0380859375, Val Loss: 24906.2421875\n",
      "Epoch 1428/3000, Loss: 10455.638671875, Val Loss: 24906.099609375\n",
      "Epoch 1429/3000, Loss: 10823.44921875, Val Loss: 24905.1796875\n",
      "Epoch 01429: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 1430/3000, Loss: 10384.1142578125, Val Loss: 24904.533203125\n",
      "Epoch 1431/3000, Loss: 10333.5947265625, Val Loss: 24904.1796875\n",
      "Epoch 1432/3000, Loss: 10381.2802734375, Val Loss: 24903.52734375\n",
      "Epoch 1433/3000, Loss: 10648.0478515625, Val Loss: 24903.037109375\n",
      "Epoch 1434/3000, Loss: 10375.2841796875, Val Loss: 24902.12109375\n",
      "Epoch 1435/3000, Loss: 10493.9287109375, Val Loss: 24901.576171875\n",
      "Epoch 1436/3000, Loss: 10505.08984375, Val Loss: 24901.126953125\n",
      "Epoch 1437/3000, Loss: 10220.595703125, Val Loss: 24900.12109375\n",
      "Epoch 1438/3000, Loss: 10574.705078125, Val Loss: 24899.408203125\n",
      "Epoch 1439/3000, Loss: 9926.9814453125, Val Loss: 24898.662109375\n",
      "Epoch 1440/3000, Loss: 10421.90234375, Val Loss: 24897.484375\n",
      "Epoch 01440: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 1441/3000, Loss: 10584.79296875, Val Loss: 24896.64453125\n",
      "Epoch 1442/3000, Loss: 9923.2431640625, Val Loss: 24895.794921875\n",
      "Epoch 1443/3000, Loss: 10339.732421875, Val Loss: 24895.115234375\n",
      "Epoch 1444/3000, Loss: 10679.42578125, Val Loss: 24894.392578125\n",
      "Epoch 1445/3000, Loss: 10709.3876953125, Val Loss: 24893.515625\n",
      "Epoch 1446/3000, Loss: 10462.525390625, Val Loss: 24892.642578125\n",
      "Epoch 1447/3000, Loss: 10636.4013671875, Val Loss: 24891.68359375\n",
      "Epoch 1448/3000, Loss: 10627.3271484375, Val Loss: 24890.71484375\n",
      "Epoch 1449/3000, Loss: 10464.541015625, Val Loss: 24889.8828125\n",
      "Epoch 1450/3000, Loss: 9949.5283203125, Val Loss: 24889.017578125\n",
      "Epoch 1451/3000, Loss: 11027.1025390625, Val Loss: 24888.390625\n",
      "Epoch 01451: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 1452/3000, Loss: 10460.9423828125, Val Loss: 24888.072265625\n",
      "Epoch 1453/3000, Loss: 10154.728515625, Val Loss: 24887.771484375\n",
      "Epoch 1454/3000, Loss: 10063.6552734375, Val Loss: 24887.5\n",
      "Epoch 1455/3000, Loss: 10031.1796875, Val Loss: 24887.220703125\n",
      "Epoch 1456/3000, Loss: 10768.8291015625, Val Loss: 24886.94140625\n",
      "Epoch 1457/3000, Loss: 10442.2998046875, Val Loss: 24886.6875\n",
      "Epoch 1458/3000, Loss: 11057.90625, Val Loss: 24886.390625\n",
      "Epoch 1459/3000, Loss: 10279.9130859375, Val Loss: 24886.232421875\n",
      "Epoch 1460/3000, Loss: 10070.3740234375, Val Loss: 24886.06640625\n",
      "Epoch 1461/3000, Loss: 10538.2412109375, Val Loss: 24886.0078125\n",
      "Epoch 1462/3000, Loss: 10851.6416015625, Val Loss: 24886.025390625\n",
      "Epoch 01462: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 1463/3000, Loss: 10608.779296875, Val Loss: 24886.06640625\n",
      "Epoch 1464/3000, Loss: 9739.7412109375, Val Loss: 24886.029296875\n",
      "Epoch 1465/3000, Loss: 10121.025390625, Val Loss: 24885.89453125\n",
      "Epoch 1466/3000, Loss: 10459.998046875, Val Loss: 24885.767578125\n",
      "Epoch 1467/3000, Loss: 9817.3095703125, Val Loss: 24885.646484375\n",
      "Epoch 1468/3000, Loss: 10573.7548828125, Val Loss: 24885.53515625\n",
      "Epoch 1469/3000, Loss: 10328.2451171875, Val Loss: 24885.43359375\n",
      "Epoch 1470/3000, Loss: 11677.6015625, Val Loss: 24885.314453125\n",
      "Epoch 1471/3000, Loss: 10449.640625, Val Loss: 24885.181640625\n",
      "Epoch 1472/3000, Loss: 9931.8056640625, Val Loss: 24885.080078125\n",
      "Epoch 1473/3000, Loss: 10350.7587890625, Val Loss: 24885.0078125\n",
      "Epoch 1474/3000, Loss: 10238.5927734375, Val Loss: 24884.865234375\n",
      "Epoch 1475/3000, Loss: 10566.150390625, Val Loss: 24884.72265625\n",
      "Epoch 1476/3000, Loss: 10577.38671875, Val Loss: 24884.595703125\n",
      "Epoch 1477/3000, Loss: 10460.044921875, Val Loss: 24884.404296875\n",
      "Epoch 1478/3000, Loss: 10842.4326171875, Val Loss: 24884.228515625\n",
      "Epoch 01478: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 1479/3000, Loss: 10890.8056640625, Val Loss: 24884.1796875\n",
      "Epoch 1480/3000, Loss: 10837.2607421875, Val Loss: 24884.154296875\n",
      "Epoch 1481/3000, Loss: 10977.896484375, Val Loss: 24884.119140625\n",
      "Epoch 1482/3000, Loss: 10482.470703125, Val Loss: 24884.07421875\n",
      "Epoch 1483/3000, Loss: 10478.3759765625, Val Loss: 24884.033203125\n",
      "Epoch 1484/3000, Loss: 10115.1494140625, Val Loss: 24884.0078125\n",
      "Epoch 1485/3000, Loss: 9983.2421875, Val Loss: 24884.0\n",
      "Epoch 1486/3000, Loss: 10401.2880859375, Val Loss: 24883.9765625\n",
      "Epoch 1487/3000, Loss: 10550.5126953125, Val Loss: 24883.98828125\n",
      "Epoch 1488/3000, Loss: 10354.994140625, Val Loss: 24884.021484375\n",
      "Epoch 1489/3000, Loss: 10169.2890625, Val Loss: 24884.0546875\n",
      "Epoch 01489: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 1490/3000, Loss: 9895.384765625, Val Loss: 24884.0546875\n",
      "Epoch 1491/3000, Loss: 10581.427734375, Val Loss: 24884.0625\n",
      "Epoch 1492/3000, Loss: 10153.8681640625, Val Loss: 24884.0703125\n",
      "Epoch 1493/3000, Loss: 10246.7919921875, Val Loss: 24884.068359375\n",
      "Epoch 1494/3000, Loss: 10611.9453125, Val Loss: 24884.068359375\n",
      "Epoch 1495/3000, Loss: 10682.25390625, Val Loss: 24884.056640625\n",
      "Epoch 1496/3000, Loss: 10290.677734375, Val Loss: 24884.017578125\n",
      "Epoch 1497/3000, Loss: 10531.8662109375, Val Loss: 24883.98046875\n",
      "Epoch 1498/3000, Loss: 10506.8203125, Val Loss: 24883.962890625\n",
      "Epoch 1499/3000, Loss: 9972.04296875, Val Loss: 24883.943359375\n",
      "Epoch 1500/3000, Loss: 10015.71875, Val Loss: 24883.921875\n",
      "Epoch 01500: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 1501/3000, Loss: 10364.814453125, Val Loss: 24883.9140625\n",
      "Epoch 1502/3000, Loss: 10211.7109375, Val Loss: 24883.90234375\n",
      "Epoch 1503/3000, Loss: 10510.974609375, Val Loss: 24883.890625\n",
      "Epoch 1504/3000, Loss: 10676.4296875, Val Loss: 24883.875\n",
      "Epoch 1505/3000, Loss: 10162.30859375, Val Loss: 24883.85546875\n",
      "Epoch 1506/3000, Loss: 10364.8974609375, Val Loss: 24883.8359375\n",
      "Epoch 1507/3000, Loss: 10609.8837890625, Val Loss: 24883.814453125\n",
      "Epoch 1508/3000, Loss: 9702.3203125, Val Loss: 24883.794921875\n",
      "Epoch 1509/3000, Loss: 10473.2294921875, Val Loss: 24883.763671875\n",
      "Epoch 1510/3000, Loss: 10412.779296875, Val Loss: 24883.736328125\n",
      "Epoch 1511/3000, Loss: 10827.5625, Val Loss: 24883.71875\n",
      "Epoch 01511: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 1512/3000, Loss: 10632.5556640625, Val Loss: 24883.712890625\n",
      "Epoch 1513/3000, Loss: 10187.01171875, Val Loss: 24883.70703125\n",
      "Epoch 1514/3000, Loss: 10139.2509765625, Val Loss: 24883.697265625\n",
      "Epoch 1515/3000, Loss: 11063.2294921875, Val Loss: 24883.689453125\n",
      "Epoch 1516/3000, Loss: 10067.859375, Val Loss: 24883.6796875\n",
      "Epoch 1517/3000, Loss: 10964.9013671875, Val Loss: 24883.669921875\n",
      "Epoch 1518/3000, Loss: 10247.025390625, Val Loss: 24883.65625\n",
      "Epoch 1519/3000, Loss: 10621.505859375, Val Loss: 24883.64453125\n",
      "Epoch 1520/3000, Loss: 10606.974609375, Val Loss: 24883.63671875\n",
      "Epoch 1521/3000, Loss: 10385.0419921875, Val Loss: 24883.626953125\n",
      "Epoch 1522/3000, Loss: 10548.1494140625, Val Loss: 24883.619140625\n",
      "Epoch 01522: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 1523/3000, Loss: 10023.7373046875, Val Loss: 24883.61328125\n",
      "Epoch 1524/3000, Loss: 9908.369140625, Val Loss: 24883.611328125\n",
      "Epoch 1525/3000, Loss: 10426.0078125, Val Loss: 24883.609375\n",
      "Epoch 1526/3000, Loss: 10003.3662109375, Val Loss: 24883.599609375\n",
      "Epoch 1527/3000, Loss: 9834.19921875, Val Loss: 24883.595703125\n",
      "Epoch 1528/3000, Loss: 10150.21875, Val Loss: 24883.587890625\n",
      "Epoch 1529/3000, Loss: 10968.72265625, Val Loss: 24883.58203125\n",
      "Epoch 1530/3000, Loss: 10677.609375, Val Loss: 24883.576171875\n",
      "Epoch 1531/3000, Loss: 10278.6015625, Val Loss: 24883.5703125\n",
      "Epoch 1532/3000, Loss: 10295.1796875, Val Loss: 24883.568359375\n",
      "Epoch 1533/3000, Loss: 10730.2802734375, Val Loss: 24883.560546875\n",
      "Epoch 01533: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 1534/3000, Loss: 10040.236328125, Val Loss: 24883.55859375\n",
      "Epoch 1535/3000, Loss: 10332.7177734375, Val Loss: 24883.556640625\n",
      "Epoch 1536/3000, Loss: 10450.78515625, Val Loss: 24883.556640625\n",
      "Epoch 1537/3000, Loss: 9737.1904296875, Val Loss: 24883.5546875\n",
      "Epoch 1538/3000, Loss: 10277.5888671875, Val Loss: 24883.552734375\n",
      "Epoch 1539/3000, Loss: 10263.375, Val Loss: 24883.552734375\n",
      "Epoch 1540/3000, Loss: 10897.640625, Val Loss: 24883.54296875\n",
      "Epoch 1541/3000, Loss: 10569.48046875, Val Loss: 24883.54296875\n",
      "Epoch 1542/3000, Loss: 9771.9794921875, Val Loss: 24883.537109375\n",
      "Epoch 1543/3000, Loss: 10359.8427734375, Val Loss: 24883.53515625\n",
      "Epoch 1544/3000, Loss: 10204.9306640625, Val Loss: 24883.53125\n",
      "Epoch 01544: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 1545/3000, Loss: 10825.5283203125, Val Loss: 24883.53125\n",
      "Epoch 1546/3000, Loss: 10675.3173828125, Val Loss: 24883.529296875\n",
      "Epoch 1547/3000, Loss: 10428.7001953125, Val Loss: 24883.52734375\n",
      "Epoch 1548/3000, Loss: 10397.662109375, Val Loss: 24883.525390625\n",
      "Epoch 1549/3000, Loss: 10352.68359375, Val Loss: 24883.525390625\n",
      "Epoch 1550/3000, Loss: 10327.0380859375, Val Loss: 24883.52734375\n",
      "Epoch 1551/3000, Loss: 10324.5546875, Val Loss: 24883.5234375\n",
      "Epoch 1552/3000, Loss: 10675.2529296875, Val Loss: 24883.51953125\n",
      "Epoch 1553/3000, Loss: 10591.3076171875, Val Loss: 24883.51953125\n",
      "Epoch 1554/3000, Loss: 10724.30859375, Val Loss: 24883.51953125\n",
      "Epoch 1555/3000, Loss: 10480.2646484375, Val Loss: 24883.51953125\n",
      "Epoch 01555: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch 1556/3000, Loss: 10151.171875, Val Loss: 24883.51953125\n",
      "Epoch 1557/3000, Loss: 10289.7880859375, Val Loss: 24883.51953125\n",
      "Epoch 1558/3000, Loss: 10264.2421875, Val Loss: 24883.51953125\n",
      "Epoch 1559/3000, Loss: 10156.4306640625, Val Loss: 24883.51953125\n",
      "Epoch 1560/3000, Loss: 10315.63671875, Val Loss: 24883.51953125\n",
      "Epoch 1561/3000, Loss: 10524.0517578125, Val Loss: 24883.51953125\n",
      "Epoch 1562/3000, Loss: 10140.3515625, Val Loss: 24883.517578125\n",
      "Epoch 1563/3000, Loss: 10578.076171875, Val Loss: 24883.51953125\n",
      "Epoch 1564/3000, Loss: 10513.4716796875, Val Loss: 24883.51953125\n",
      "Epoch 1565/3000, Loss: 10303.3173828125, Val Loss: 24883.51953125\n",
      "Epoch 1566/3000, Loss: 10402.853515625, Val Loss: 24883.51953125\n",
      "Epoch 1567/3000, Loss: 10279.06640625, Val Loss: 24883.51953125\n",
      "Epoch 1568/3000, Loss: 10838.572265625, Val Loss: 24883.51953125\n",
      "Epoch 1569/3000, Loss: 10225.5927734375, Val Loss: 24883.51953125\n",
      "Epoch 1570/3000, Loss: 10432.8017578125, Val Loss: 24883.517578125\n",
      "Epoch 1571/3000, Loss: 10009.3017578125, Val Loss: 24883.517578125\n",
      "Epoch 1572/3000, Loss: 10343.244140625, Val Loss: 24883.517578125\n",
      "Epoch 1573/3000, Loss: 10060.923828125, Val Loss: 24883.517578125\n",
      "Epoch 1574/3000, Loss: 10468.701171875, Val Loss: 24883.515625\n",
      "Epoch 1575/3000, Loss: 10919.296875, Val Loss: 24883.51953125\n",
      "Epoch 1576/3000, Loss: 9811.833984375, Val Loss: 24883.515625\n",
      "Epoch 1577/3000, Loss: 10041.71875, Val Loss: 24883.515625\n",
      "Epoch 1578/3000, Loss: 10141.76171875, Val Loss: 24883.515625\n",
      "Epoch 1579/3000, Loss: 10745.314453125, Val Loss: 24883.515625\n",
      "Epoch 1580/3000, Loss: 10412.73828125, Val Loss: 24883.513671875\n",
      "Epoch 1581/3000, Loss: 10301.2802734375, Val Loss: 24883.513671875\n",
      "Epoch 1582/3000, Loss: 10926.8671875, Val Loss: 24883.513671875\n",
      "Epoch 1583/3000, Loss: 10747.7265625, Val Loss: 24883.513671875\n",
      "Epoch 1584/3000, Loss: 9941.4794921875, Val Loss: 24883.513671875\n",
      "Epoch 1585/3000, Loss: 9947.1708984375, Val Loss: 24883.513671875\n",
      "Epoch 1586/3000, Loss: 10206.828125, Val Loss: 24883.513671875\n",
      "Epoch 1587/3000, Loss: 10494.2314453125, Val Loss: 24883.513671875\n",
      "Epoch 1588/3000, Loss: 10844.888671875, Val Loss: 24883.513671875\n",
      "Epoch 1589/3000, Loss: 10904.2763671875, Val Loss: 24883.513671875\n",
      "Epoch 1590/3000, Loss: 10139.056640625, Val Loss: 24883.513671875\n",
      "Epoch 1591/3000, Loss: 11074.4521484375, Val Loss: 24883.513671875\n",
      "Epoch 1592/3000, Loss: 10190.267578125, Val Loss: 24883.509765625\n",
      "Epoch 1593/3000, Loss: 10191.6455078125, Val Loss: 24883.509765625\n",
      "Epoch 1594/3000, Loss: 10366.0419921875, Val Loss: 24883.513671875\n",
      "Epoch 1595/3000, Loss: 10706.6220703125, Val Loss: 24883.513671875\n",
      "Epoch 1596/3000, Loss: 10200.4423828125, Val Loss: 24883.513671875\n",
      "Epoch 1597/3000, Loss: 10005.353515625, Val Loss: 24883.513671875\n",
      "Epoch 1598/3000, Loss: 10188.0751953125, Val Loss: 24883.509765625\n",
      "Epoch 1599/3000, Loss: 10686.4521484375, Val Loss: 24883.509765625\n",
      "Epoch 1600/3000, Loss: 10411.3642578125, Val Loss: 24883.509765625\n",
      "Epoch 1601/3000, Loss: 11219.3642578125, Val Loss: 24883.509765625\n",
      "Epoch 1602/3000, Loss: 10791.203125, Val Loss: 24883.51171875\n",
      "Epoch 1603/3000, Loss: 10542.060546875, Val Loss: 24883.509765625\n",
      "Epoch 1604/3000, Loss: 10183.4501953125, Val Loss: 24883.509765625\n",
      "Epoch 1605/3000, Loss: 10756.9287109375, Val Loss: 24883.509765625\n",
      "Epoch 1606/3000, Loss: 10184.400390625, Val Loss: 24883.51171875\n",
      "Epoch 1607/3000, Loss: 10454.8251953125, Val Loss: 24883.513671875\n",
      "Epoch 1608/3000, Loss: 10221.9736328125, Val Loss: 24883.513671875\n",
      "Epoch 1609/3000, Loss: 10272.357421875, Val Loss: 24883.51171875\n",
      "Epoch 1610/3000, Loss: 10307.1865234375, Val Loss: 24883.513671875\n",
      "Epoch 1611/3000, Loss: 10724.25, Val Loss: 24883.509765625\n",
      "Epoch 1612/3000, Loss: 10117.5869140625, Val Loss: 24883.5078125\n",
      "Epoch 1613/3000, Loss: 10432.3515625, Val Loss: 24883.509765625\n",
      "Epoch 1614/3000, Loss: 10218.373046875, Val Loss: 24883.51171875\n",
      "Epoch 1615/3000, Loss: 10030.80078125, Val Loss: 24883.5078125\n",
      "Epoch 1616/3000, Loss: 10675.9296875, Val Loss: 24883.5078125\n",
      "Epoch 1617/3000, Loss: 10379.0322265625, Val Loss: 24883.5078125\n",
      "Epoch 1618/3000, Loss: 10648.5576171875, Val Loss: 24883.5078125\n",
      "Epoch 1619/3000, Loss: 10511.0830078125, Val Loss: 24883.505859375\n",
      "Epoch 1620/3000, Loss: 10292.1455078125, Val Loss: 24883.505859375\n",
      "Epoch 1621/3000, Loss: 10455.37890625, Val Loss: 24883.50390625\n",
      "Epoch 1622/3000, Loss: 9927.0244140625, Val Loss: 24883.5078125\n",
      "Epoch 1623/3000, Loss: 9766.609375, Val Loss: 24883.50390625\n",
      "Epoch 1624/3000, Loss: 10715.916015625, Val Loss: 24883.505859375\n",
      "Epoch 1625/3000, Loss: 9969.6279296875, Val Loss: 24883.50390625\n",
      "Epoch 1626/3000, Loss: 10322.421875, Val Loss: 24883.50390625\n",
      "Epoch 1627/3000, Loss: 9817.970703125, Val Loss: 24883.50390625\n",
      "Epoch 1628/3000, Loss: 10328.0830078125, Val Loss: 24883.501953125\n",
      "Epoch 1629/3000, Loss: 10758.3076171875, Val Loss: 24883.501953125\n",
      "Epoch 1630/3000, Loss: 10050.904296875, Val Loss: 24883.501953125\n",
      "Epoch 1631/3000, Loss: 10418.3486328125, Val Loss: 24883.501953125\n",
      "Epoch 1632/3000, Loss: 10253.634765625, Val Loss: 24883.501953125\n",
      "Epoch 1633/3000, Loss: 10313.3359375, Val Loss: 24883.5\n",
      "Epoch 1634/3000, Loss: 10368.3095703125, Val Loss: 24883.5\n",
      "Epoch 1635/3000, Loss: 10532.1982421875, Val Loss: 24883.5\n",
      "Epoch 1636/3000, Loss: 10461.26953125, Val Loss: 24883.49609375\n",
      "Epoch 1637/3000, Loss: 10012.7080078125, Val Loss: 24883.49609375\n",
      "Epoch 1638/3000, Loss: 9668.8447265625, Val Loss: 24883.49609375\n",
      "Epoch 1639/3000, Loss: 10627.92578125, Val Loss: 24883.49609375\n",
      "Epoch 1640/3000, Loss: 10822.8310546875, Val Loss: 24883.5\n",
      "Epoch 1641/3000, Loss: 10544.9619140625, Val Loss: 24883.49609375\n",
      "Epoch 1642/3000, Loss: 9888.3056640625, Val Loss: 24883.498046875\n",
      "Epoch 1643/3000, Loss: 10663.2109375, Val Loss: 24883.49609375\n",
      "Epoch 1644/3000, Loss: 11205.23828125, Val Loss: 24883.49609375\n",
      "Epoch 1645/3000, Loss: 10352.6181640625, Val Loss: 24883.49609375\n",
      "Epoch 1646/3000, Loss: 10526.7822265625, Val Loss: 24883.49609375\n",
      "Epoch 1647/3000, Loss: 10052.98828125, Val Loss: 24883.49609375\n",
      "Epoch 1648/3000, Loss: 10535.4541015625, Val Loss: 24883.494140625\n",
      "Epoch 1649/3000, Loss: 10226.4755859375, Val Loss: 24883.494140625\n",
      "Epoch 1650/3000, Loss: 11113.677734375, Val Loss: 24883.494140625\n",
      "Epoch 1651/3000, Loss: 10435.8759765625, Val Loss: 24883.494140625\n",
      "Epoch 1652/3000, Loss: 9920.31640625, Val Loss: 24883.4921875\n",
      "Epoch 1653/3000, Loss: 9966.48828125, Val Loss: 24883.4921875\n",
      "Epoch 1654/3000, Loss: 10387.1767578125, Val Loss: 24883.494140625\n",
      "Epoch 1655/3000, Loss: 10111.376953125, Val Loss: 24883.494140625\n",
      "Epoch 1656/3000, Loss: 10449.2216796875, Val Loss: 24883.490234375\n",
      "Epoch 1657/3000, Loss: 10787.9296875, Val Loss: 24883.490234375\n",
      "Epoch 1658/3000, Loss: 10412.05859375, Val Loss: 24883.490234375\n",
      "Epoch 1659/3000, Loss: 10170.7900390625, Val Loss: 24883.490234375\n",
      "Epoch 1660/3000, Loss: 10259.517578125, Val Loss: 24883.490234375\n",
      "Epoch 1661/3000, Loss: 10516.6142578125, Val Loss: 24883.490234375\n",
      "Epoch 1662/3000, Loss: 10596.7900390625, Val Loss: 24883.48828125\n",
      "Epoch 1663/3000, Loss: 11166.65234375, Val Loss: 24883.486328125\n",
      "Epoch 1664/3000, Loss: 10281.576171875, Val Loss: 24883.48828125\n",
      "Epoch 1665/3000, Loss: 10405.986328125, Val Loss: 24883.48828125\n",
      "Epoch 1666/3000, Loss: 10129.9931640625, Val Loss: 24883.486328125\n",
      "Epoch 1667/3000, Loss: 10684.6689453125, Val Loss: 24883.486328125\n",
      "Epoch 1668/3000, Loss: 10142.662109375, Val Loss: 24883.486328125\n",
      "Epoch 1669/3000, Loss: 10678.2607421875, Val Loss: 24883.486328125\n",
      "Epoch 1670/3000, Loss: 10415.24609375, Val Loss: 24883.486328125\n",
      "Epoch 1671/3000, Loss: 10859.72265625, Val Loss: 24883.486328125\n",
      "Epoch 1672/3000, Loss: 10747.3994140625, Val Loss: 24883.486328125\n",
      "Epoch 1673/3000, Loss: 10231.919921875, Val Loss: 24883.484375\n",
      "Epoch 1674/3000, Loss: 10276.3310546875, Val Loss: 24883.484375\n",
      "Epoch 1675/3000, Loss: 11095.7412109375, Val Loss: 24883.48046875\n",
      "Epoch 1676/3000, Loss: 10217.220703125, Val Loss: 24883.48046875\n",
      "Epoch 1677/3000, Loss: 10962.34375, Val Loss: 24883.48046875\n",
      "Epoch 1678/3000, Loss: 10975.4296875, Val Loss: 24883.478515625\n",
      "Epoch 1679/3000, Loss: 10063.0, Val Loss: 24883.478515625\n",
      "Epoch 1680/3000, Loss: 10732.88671875, Val Loss: 24883.478515625\n",
      "Epoch 1681/3000, Loss: 10453.8427734375, Val Loss: 24883.48046875\n",
      "Epoch 1682/3000, Loss: 10376.861328125, Val Loss: 24883.48046875\n",
      "Epoch 1683/3000, Loss: 10540.787109375, Val Loss: 24883.48046875\n",
      "Epoch 1684/3000, Loss: 10420.9287109375, Val Loss: 24883.48046875\n",
      "Epoch 1685/3000, Loss: 10950.1728515625, Val Loss: 24883.48046875\n",
      "Epoch 1686/3000, Loss: 10546.1416015625, Val Loss: 24883.4765625\n",
      "Epoch 1687/3000, Loss: 9876.7451171875, Val Loss: 24883.4765625\n",
      "Epoch 1688/3000, Loss: 10547.9599609375, Val Loss: 24883.4765625\n",
      "Epoch 1689/3000, Loss: 10395.921875, Val Loss: 24883.48046875\n",
      "Epoch 1690/3000, Loss: 10261.1474609375, Val Loss: 24883.48046875\n",
      "Epoch 1691/3000, Loss: 10624.7529296875, Val Loss: 24883.4765625\n",
      "Epoch 1692/3000, Loss: 10780.7568359375, Val Loss: 24883.4765625\n",
      "Epoch 1693/3000, Loss: 10408.666015625, Val Loss: 24883.4765625\n",
      "Epoch 1694/3000, Loss: 10513.322265625, Val Loss: 24883.4765625\n",
      "Epoch 1695/3000, Loss: 10588.3828125, Val Loss: 24883.4765625\n",
      "Epoch 1696/3000, Loss: 10275.1044921875, Val Loss: 24883.478515625\n",
      "Epoch 1697/3000, Loss: 10238.1376953125, Val Loss: 24883.4765625\n",
      "Epoch 1698/3000, Loss: 10235.5078125, Val Loss: 24883.4765625\n",
      "Epoch 1699/3000, Loss: 10995.9892578125, Val Loss: 24883.4765625\n",
      "Epoch 1700/3000, Loss: 10645.1455078125, Val Loss: 24883.4765625\n",
      "Epoch 1701/3000, Loss: 10013.7587890625, Val Loss: 24883.4765625\n",
      "Epoch 1702/3000, Loss: 10981.953125, Val Loss: 24883.4765625\n",
      "Epoch 1703/3000, Loss: 10174.681640625, Val Loss: 24883.4765625\n",
      "Epoch 1704/3000, Loss: 10404.0498046875, Val Loss: 24883.4765625\n",
      "Epoch 1705/3000, Loss: 10437.6552734375, Val Loss: 24883.4765625\n",
      "Epoch 1706/3000, Loss: 10543.8232421875, Val Loss: 24883.4765625\n",
      "Epoch 1707/3000, Loss: 10192.2158203125, Val Loss: 24883.4765625\n",
      "Epoch 1708/3000, Loss: 10541.240234375, Val Loss: 24883.4765625\n",
      "Epoch 1709/3000, Loss: 10567.4501953125, Val Loss: 24883.474609375\n",
      "Epoch 1710/3000, Loss: 10431.1416015625, Val Loss: 24883.4765625\n",
      "Epoch 1711/3000, Loss: 10270.1494140625, Val Loss: 24883.4765625\n",
      "Epoch 1712/3000, Loss: 10505.4345703125, Val Loss: 24883.4765625\n",
      "Epoch 1713/3000, Loss: 10506.5703125, Val Loss: 24883.47265625\n",
      "Epoch 1714/3000, Loss: 9900.5166015625, Val Loss: 24883.474609375\n",
      "Epoch 1715/3000, Loss: 10463.7783203125, Val Loss: 24883.474609375\n",
      "Epoch 1716/3000, Loss: 10197.7861328125, Val Loss: 24883.470703125\n",
      "Epoch 1717/3000, Loss: 10630.076171875, Val Loss: 24883.470703125\n",
      "Epoch 1718/3000, Loss: 10405.1455078125, Val Loss: 24883.474609375\n",
      "Epoch 1719/3000, Loss: 10914.8193359375, Val Loss: 24883.46875\n",
      "Epoch 1720/3000, Loss: 9936.314453125, Val Loss: 24883.46875\n",
      "Epoch 1721/3000, Loss: 10510.650390625, Val Loss: 24883.46875\n",
      "Epoch 1722/3000, Loss: 10845.9814453125, Val Loss: 24883.46875\n",
      "Epoch 1723/3000, Loss: 10083.94921875, Val Loss: 24883.46875\n",
      "Epoch 1724/3000, Loss: 10280.3798828125, Val Loss: 24883.46875\n",
      "Epoch 1725/3000, Loss: 10583.3740234375, Val Loss: 24883.46875\n",
      "Epoch 1726/3000, Loss: 10046.484375, Val Loss: 24883.46875\n",
      "Epoch 1727/3000, Loss: 10225.0419921875, Val Loss: 24883.46875\n",
      "Epoch 1728/3000, Loss: 10232.6865234375, Val Loss: 24883.46875\n",
      "Epoch 1729/3000, Loss: 10187.5302734375, Val Loss: 24883.46875\n",
      "Epoch 1730/3000, Loss: 10344.3681640625, Val Loss: 24883.46875\n",
      "Epoch 1731/3000, Loss: 10516.203125, Val Loss: 24883.46875\n",
      "Epoch 1732/3000, Loss: 10444.83984375, Val Loss: 24883.466796875\n",
      "Epoch 1733/3000, Loss: 9661.3046875, Val Loss: 24883.46875\n",
      "Epoch 1734/3000, Loss: 10749.0556640625, Val Loss: 24883.466796875\n",
      "Epoch 1735/3000, Loss: 10350.740234375, Val Loss: 24883.46484375\n",
      "Epoch 1736/3000, Loss: 10238.9619140625, Val Loss: 24883.46484375\n",
      "Epoch 1737/3000, Loss: 9852.8369140625, Val Loss: 24883.466796875\n",
      "Epoch 1738/3000, Loss: 10298.185546875, Val Loss: 24883.46484375\n",
      "Epoch 1739/3000, Loss: 10384.19921875, Val Loss: 24883.46484375\n",
      "Epoch 1740/3000, Loss: 10672.9990234375, Val Loss: 24883.46484375\n",
      "Epoch 1741/3000, Loss: 10263.7216796875, Val Loss: 24883.46484375\n",
      "Epoch 1742/3000, Loss: 10379.2685546875, Val Loss: 24883.46484375\n",
      "Epoch 1743/3000, Loss: 10237.0302734375, Val Loss: 24883.46484375\n",
      "Epoch 1744/3000, Loss: 9879.365234375, Val Loss: 24883.46484375\n",
      "Epoch 1745/3000, Loss: 10531.5537109375, Val Loss: 24883.46484375\n",
      "Epoch 1746/3000, Loss: 10043.1669921875, Val Loss: 24883.462890625\n",
      "Epoch 1747/3000, Loss: 10573.48828125, Val Loss: 24883.462890625\n",
      "Epoch 1748/3000, Loss: 10348.77734375, Val Loss: 24883.462890625\n",
      "Epoch 1749/3000, Loss: 10029.779296875, Val Loss: 24883.462890625\n",
      "Epoch 1750/3000, Loss: 10777.62109375, Val Loss: 24883.462890625\n",
      "Epoch 1751/3000, Loss: 10182.68359375, Val Loss: 24883.462890625\n",
      "Epoch 1752/3000, Loss: 10425.3369140625, Val Loss: 24883.462890625\n",
      "Epoch 1753/3000, Loss: 11024.1767578125, Val Loss: 24883.462890625\n",
      "Epoch 1754/3000, Loss: 10243.38671875, Val Loss: 24883.462890625\n",
      "Epoch 1755/3000, Loss: 10044.1767578125, Val Loss: 24883.462890625\n",
      "Epoch 1756/3000, Loss: 10452.08984375, Val Loss: 24883.462890625\n",
      "Epoch 1757/3000, Loss: 10185.6767578125, Val Loss: 24883.46484375\n",
      "Epoch 1758/3000, Loss: 10833.0595703125, Val Loss: 24883.462890625\n",
      "Epoch 1759/3000, Loss: 10517.552734375, Val Loss: 24883.46484375\n",
      "Epoch 1760/3000, Loss: 10674.0947265625, Val Loss: 24883.462890625\n",
      "Epoch 1761/3000, Loss: 9801.0615234375, Val Loss: 24883.462890625\n",
      "Epoch 1762/3000, Loss: 10236.1162109375, Val Loss: 24883.462890625\n",
      "Epoch 1763/3000, Loss: 10438.146484375, Val Loss: 24883.462890625\n",
      "Epoch 1764/3000, Loss: 10014.6513671875, Val Loss: 24883.462890625\n",
      "Epoch 1765/3000, Loss: 10479.4150390625, Val Loss: 24883.462890625\n",
      "Epoch 1766/3000, Loss: 10863.7236328125, Val Loss: 24883.462890625\n",
      "Epoch 1767/3000, Loss: 10166.9033203125, Val Loss: 24883.4609375\n",
      "Epoch 1768/3000, Loss: 10426.6318359375, Val Loss: 24883.462890625\n",
      "Epoch 1769/3000, Loss: 10431.373046875, Val Loss: 24883.462890625\n",
      "Epoch 1770/3000, Loss: 10349.2275390625, Val Loss: 24883.462890625\n",
      "Epoch 1771/3000, Loss: 10447.462890625, Val Loss: 24883.462890625\n",
      "Epoch 1772/3000, Loss: 10153.4306640625, Val Loss: 24883.462890625\n",
      "Epoch 1773/3000, Loss: 10410.19921875, Val Loss: 24883.458984375\n",
      "Epoch 1774/3000, Loss: 10657.494140625, Val Loss: 24883.462890625\n",
      "Epoch 1775/3000, Loss: 10500.1435546875, Val Loss: 24883.462890625\n",
      "Epoch 1776/3000, Loss: 10695.009765625, Val Loss: 24883.458984375\n",
      "Epoch 1777/3000, Loss: 10232.169921875, Val Loss: 24883.462890625\n",
      "Epoch 1778/3000, Loss: 10188.96484375, Val Loss: 24883.462890625\n",
      "Epoch 1779/3000, Loss: 10337.5859375, Val Loss: 24883.458984375\n",
      "Epoch 1780/3000, Loss: 10445.09375, Val Loss: 24883.458984375\n",
      "Epoch 1781/3000, Loss: 10424.7353515625, Val Loss: 24883.458984375\n",
      "Epoch 1782/3000, Loss: 10097.8369140625, Val Loss: 24883.458984375\n",
      "Epoch 1783/3000, Loss: 10141.6201171875, Val Loss: 24883.458984375\n",
      "Epoch 1784/3000, Loss: 10387.16796875, Val Loss: 24883.453125\n",
      "Epoch 1785/3000, Loss: 10115.5546875, Val Loss: 24883.45703125\n",
      "Epoch 1786/3000, Loss: 11288.3359375, Val Loss: 24883.455078125\n",
      "Epoch 1787/3000, Loss: 10136.3515625, Val Loss: 24883.45703125\n",
      "Epoch 1788/3000, Loss: 10650.486328125, Val Loss: 24883.455078125\n",
      "Epoch 1789/3000, Loss: 10129.83984375, Val Loss: 24883.455078125\n",
      "Epoch 1790/3000, Loss: 10154.4794921875, Val Loss: 24883.455078125\n",
      "Epoch 1791/3000, Loss: 10170.3642578125, Val Loss: 24883.455078125\n",
      "Epoch 1792/3000, Loss: 10406.5986328125, Val Loss: 24883.455078125\n",
      "Epoch 1793/3000, Loss: 10018.900390625, Val Loss: 24883.455078125\n",
      "Epoch 1794/3000, Loss: 10546.6875, Val Loss: 24883.455078125\n",
      "Epoch 1795/3000, Loss: 10443.93359375, Val Loss: 24883.455078125\n",
      "Epoch 1796/3000, Loss: 10280.974609375, Val Loss: 24883.455078125\n",
      "Epoch 1797/3000, Loss: 10631.6240234375, Val Loss: 24883.455078125\n",
      "Epoch 1798/3000, Loss: 10507.646484375, Val Loss: 24883.455078125\n",
      "Epoch 1799/3000, Loss: 10493.474609375, Val Loss: 24883.455078125\n",
      "Epoch 1800/3000, Loss: 9930.423828125, Val Loss: 24883.455078125\n",
      "Epoch 1801/3000, Loss: 10134.2919921875, Val Loss: 24883.451171875\n",
      "Epoch 1802/3000, Loss: 10284.5400390625, Val Loss: 24883.455078125\n",
      "Epoch 1803/3000, Loss: 10419.720703125, Val Loss: 24883.451171875\n",
      "Epoch 1804/3000, Loss: 10280.7890625, Val Loss: 24883.451171875\n",
      "Epoch 1805/3000, Loss: 10247.970703125, Val Loss: 24883.451171875\n",
      "Epoch 1806/3000, Loss: 10559.4150390625, Val Loss: 24883.451171875\n",
      "Epoch 1807/3000, Loss: 10705.591796875, Val Loss: 24883.451171875\n",
      "Epoch 1808/3000, Loss: 10373.72265625, Val Loss: 24883.451171875\n",
      "Epoch 1809/3000, Loss: 10063.1201171875, Val Loss: 24883.451171875\n",
      "Epoch 1810/3000, Loss: 10705.05078125, Val Loss: 24883.451171875\n",
      "Epoch 1811/3000, Loss: 10967.77734375, Val Loss: 24883.451171875\n",
      "Epoch 1812/3000, Loss: 10489.1279296875, Val Loss: 24883.451171875\n",
      "Epoch 1813/3000, Loss: 9647.224609375, Val Loss: 24883.451171875\n",
      "Epoch 1814/3000, Loss: 10275.58203125, Val Loss: 24883.451171875\n",
      "Epoch 1815/3000, Loss: 10364.5087890625, Val Loss: 24883.451171875\n",
      "Epoch 1816/3000, Loss: 10673.03515625, Val Loss: 24883.451171875\n",
      "Epoch 1817/3000, Loss: 10110.720703125, Val Loss: 24883.451171875\n",
      "Epoch 1818/3000, Loss: 10439.3173828125, Val Loss: 24883.44921875\n",
      "Epoch 1819/3000, Loss: 11011.1982421875, Val Loss: 24883.451171875\n",
      "Epoch 1820/3000, Loss: 10688.3427734375, Val Loss: 24883.44921875\n",
      "Epoch 1821/3000, Loss: 10261.66796875, Val Loss: 24883.451171875\n",
      "Epoch 1822/3000, Loss: 10677.1083984375, Val Loss: 24883.44921875\n",
      "Epoch 1823/3000, Loss: 9991.3330078125, Val Loss: 24883.44921875\n",
      "Epoch 1824/3000, Loss: 10526.146484375, Val Loss: 24883.44921875\n",
      "Epoch 1825/3000, Loss: 10754.9130859375, Val Loss: 24883.44921875\n",
      "Epoch 1826/3000, Loss: 10244.712890625, Val Loss: 24883.447265625\n",
      "Epoch 1827/3000, Loss: 10379.0048828125, Val Loss: 24883.447265625\n",
      "Epoch 1828/3000, Loss: 10563.2314453125, Val Loss: 24883.44921875\n",
      "Epoch 1829/3000, Loss: 10034.6826171875, Val Loss: 24883.44921875\n",
      "Epoch 1830/3000, Loss: 10306.6396484375, Val Loss: 24883.44921875\n",
      "Epoch 1831/3000, Loss: 10758.4462890625, Val Loss: 24883.447265625\n",
      "Epoch 1832/3000, Loss: 11103.857421875, Val Loss: 24883.44921875\n",
      "Epoch 1833/3000, Loss: 9802.1787109375, Val Loss: 24883.4453125\n",
      "Epoch 1834/3000, Loss: 10492.5625, Val Loss: 24883.44921875\n",
      "Epoch 1835/3000, Loss: 10769.0498046875, Val Loss: 24883.443359375\n",
      "Epoch 1836/3000, Loss: 10240.123046875, Val Loss: 24883.443359375\n",
      "Epoch 1837/3000, Loss: 9990.259765625, Val Loss: 24883.443359375\n",
      "Epoch 1838/3000, Loss: 10806.2529296875, Val Loss: 24883.443359375\n",
      "Epoch 1839/3000, Loss: 10255.7548828125, Val Loss: 24883.443359375\n",
      "Epoch 1840/3000, Loss: 10290.2578125, Val Loss: 24883.443359375\n",
      "Epoch 1841/3000, Loss: 10324.23046875, Val Loss: 24883.443359375\n",
      "Epoch 1842/3000, Loss: 9905.537109375, Val Loss: 24883.44140625\n",
      "Epoch 1843/3000, Loss: 10422.4248046875, Val Loss: 24883.44140625\n",
      "Epoch 1844/3000, Loss: 10579.9072265625, Val Loss: 24883.44140625\n",
      "Epoch 1845/3000, Loss: 11072.955078125, Val Loss: 24883.44140625\n",
      "Epoch 1846/3000, Loss: 10409.8251953125, Val Loss: 24883.44140625\n",
      "Epoch 1847/3000, Loss: 10045.83203125, Val Loss: 24883.44140625\n",
      "Epoch 1848/3000, Loss: 10576.8330078125, Val Loss: 24883.443359375\n",
      "Epoch 1849/3000, Loss: 10936.099609375, Val Loss: 24883.439453125\n",
      "Epoch 1850/3000, Loss: 10225.423828125, Val Loss: 24883.44140625\n",
      "Epoch 1851/3000, Loss: 10454.0869140625, Val Loss: 24883.44140625\n",
      "Epoch 1852/3000, Loss: 9989.1474609375, Val Loss: 24883.44140625\n",
      "Epoch 1853/3000, Loss: 10710.1865234375, Val Loss: 24883.4375\n",
      "Epoch 1854/3000, Loss: 10178.2421875, Val Loss: 24883.4375\n",
      "Epoch 1855/3000, Loss: 10682.4951171875, Val Loss: 24883.4375\n",
      "Epoch 1856/3000, Loss: 10559.728515625, Val Loss: 24883.4375\n",
      "Epoch 1857/3000, Loss: 10204.5341796875, Val Loss: 24883.4375\n",
      "Epoch 1858/3000, Loss: 10662.490234375, Val Loss: 24883.4375\n",
      "Epoch 1859/3000, Loss: 10642.21875, Val Loss: 24883.4375\n",
      "Epoch 1860/3000, Loss: 10166.9970703125, Val Loss: 24883.4375\n",
      "Epoch 1861/3000, Loss: 10312.2705078125, Val Loss: 24883.4375\n",
      "Epoch 1862/3000, Loss: 10900.44921875, Val Loss: 24883.4375\n",
      "Epoch 1863/3000, Loss: 10270.451171875, Val Loss: 24883.4375\n",
      "Epoch 1864/3000, Loss: 10322.978515625, Val Loss: 24883.4375\n",
      "Epoch 1865/3000, Loss: 10732.98828125, Val Loss: 24883.4375\n",
      "Epoch 1866/3000, Loss: 10609.736328125, Val Loss: 24883.4375\n",
      "Epoch 1867/3000, Loss: 10109.828125, Val Loss: 24883.4375\n",
      "Epoch 1868/3000, Loss: 10720.552734375, Val Loss: 24883.4375\n",
      "Epoch 1869/3000, Loss: 10169.607421875, Val Loss: 24883.4375\n",
      "Epoch 1870/3000, Loss: 10911.232421875, Val Loss: 24883.4375\n",
      "Epoch 1871/3000, Loss: 10231.681640625, Val Loss: 24883.4375\n",
      "Epoch 1872/3000, Loss: 10924.150390625, Val Loss: 24883.4375\n",
      "Epoch 1873/3000, Loss: 10411.5029296875, Val Loss: 24883.439453125\n",
      "Epoch 1874/3000, Loss: 10451.5576171875, Val Loss: 24883.4375\n",
      "Epoch 1875/3000, Loss: 10281.01953125, Val Loss: 24883.439453125\n",
      "Epoch 1876/3000, Loss: 10769.4736328125, Val Loss: 24883.4375\n",
      "Epoch 1877/3000, Loss: 10296.3603515625, Val Loss: 24883.439453125\n",
      "Epoch 1878/3000, Loss: 10679.5078125, Val Loss: 24883.439453125\n",
      "Epoch 1879/3000, Loss: 10174.05078125, Val Loss: 24883.4375\n",
      "Epoch 1880/3000, Loss: 10704.265625, Val Loss: 24883.4375\n",
      "Epoch 1881/3000, Loss: 10327.2587890625, Val Loss: 24883.4375\n",
      "Epoch 1882/3000, Loss: 10299.541015625, Val Loss: 24883.4375\n",
      "Epoch 1883/3000, Loss: 10234.4931640625, Val Loss: 24883.4375\n",
      "Epoch 1884/3000, Loss: 10246.265625, Val Loss: 24883.4375\n",
      "Epoch 1885/3000, Loss: 10438.1708984375, Val Loss: 24883.4375\n",
      "Epoch 1886/3000, Loss: 10510.53125, Val Loss: 24883.4375\n",
      "Epoch 1887/3000, Loss: 10265.19921875, Val Loss: 24883.4375\n",
      "Epoch 1888/3000, Loss: 10561.587890625, Val Loss: 24883.4375\n",
      "Epoch 1889/3000, Loss: 10977.6826171875, Val Loss: 24883.4375\n",
      "Epoch 1890/3000, Loss: 10507.5341796875, Val Loss: 24883.439453125\n",
      "Epoch 1891/3000, Loss: 10790.84375, Val Loss: 24883.4375\n",
      "Epoch 1892/3000, Loss: 10956.9033203125, Val Loss: 24883.4375\n",
      "Epoch 1893/3000, Loss: 10553.70703125, Val Loss: 24883.4375\n",
      "Epoch 1894/3000, Loss: 10159.5166015625, Val Loss: 24883.4375\n",
      "Epoch 1895/3000, Loss: 10136.2119140625, Val Loss: 24883.4375\n",
      "Epoch 1896/3000, Loss: 10700.73828125, Val Loss: 24883.4375\n",
      "Epoch 1897/3000, Loss: 10392.9140625, Val Loss: 24883.4375\n",
      "Epoch 1898/3000, Loss: 10678.4775390625, Val Loss: 24883.4375\n",
      "Epoch 1899/3000, Loss: 10778.421875, Val Loss: 24883.4375\n",
      "Epoch 1900/3000, Loss: 10364.88671875, Val Loss: 24883.4375\n",
      "Epoch 1901/3000, Loss: 10625.94921875, Val Loss: 24883.4375\n",
      "Epoch 1902/3000, Loss: 10370.5, Val Loss: 24883.4375\n",
      "Epoch 1903/3000, Loss: 10760.19140625, Val Loss: 24883.4375\n",
      "Early stopping triggered!\n",
      "Validation RMSE: 157.74486226010347\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# One-hot encoding\n",
    "train_encoded = pd.get_dummies(train_data, columns=[\" \", \" \", \" \", \" \", \" \"])\n",
    "X = train_encoded.drop(columns=[\"ID\", \"\"]).values\n",
    "y = train_encoded[\"\"].values\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_val_standard = scaler.transform(X_val)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_standard)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val_standard)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# Initial Neural Network architecture with optional dropout\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)  # Optional dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)  # Optional dropout\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet(X_train_tensor.shape[1])\n",
    "\n",
    "# Loss, optimizer with weight decay, and learning rate scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Training with early stopping\n",
    "num_epochs = 3000\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor).squeeze()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Early stopping and learning rate reduction on plateau\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# ...\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# RMSE on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(X_val_tensor).squeeze().detach().numpy()\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "print(f\"Validation RMSE: {rmse}\")\n",
    "\n",
    "# Preprocessing test data\n",
    "test_encoded = pd.get_dummies(test_data, columns=[\" \", \" \", \" \", \" \", \" \"])\n",
    "X_test = test_encoded.drop(columns=[\"ID\"]).values\n",
    "X_test_standard = scaler.transform(X_test)\n",
    "X_test_tensor = torch.FloatTensor(X_test_standard)\n",
    "\n",
    "# Predict on test data\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "# Generate a submission file\n",
    "submission_dl = pd.DataFrame({'ID': test_data[\"ID\"], '': test_predictions})\n",
    "submission_dl.to_csv(\"./data/submission_dl_simple.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9d460-a6d5-438d-ae62-8ef944c53dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
