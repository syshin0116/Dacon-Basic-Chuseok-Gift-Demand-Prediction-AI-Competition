{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb170d5-743a-4e23-957a-3e3f9cb5222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664a1d2f-fa21-4f14-a34d-1d5e837ec33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# One-hot encode the categorical variables\n",
    "train_encoded = pd.get_dummies(train_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "\n",
    "# Separate the target variable and the features\n",
    "X = train_encoded.drop(columns=[\"ID\", \"수요량\"])\n",
    "y = train_encoded[\"수요량\"]\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_standard = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_standard, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd87c40e-4ebd-4736-9319-ae5f79a9c326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Loss: 183284.546875, Val Loss: 190231.96875\n",
      "Epoch 2/3000, Loss: 183264.546875, Val Loss: 190210.28125\n",
      "Epoch 3/3000, Loss: 183243.5625, Val Loss: 190187.21875\n",
      "Epoch 4/3000, Loss: 183221.265625, Val Loss: 190162.359375\n",
      "Epoch 5/3000, Loss: 183197.265625, Val Loss: 190135.265625\n",
      "Epoch 6/3000, Loss: 183171.21875, Val Loss: 190105.625\n",
      "Epoch 7/3000, Loss: 183142.625, Val Loss: 190072.890625\n",
      "Epoch 8/3000, Loss: 183111.109375, Val Loss: 190036.640625\n",
      "Epoch 9/3000, Loss: 183076.1875, Val Loss: 189996.1875\n",
      "Epoch 10/3000, Loss: 183037.28125, Val Loss: 189951.078125\n",
      "Epoch 11/3000, Loss: 182993.859375, Val Loss: 189900.765625\n",
      "Epoch 12/3000, Loss: 182945.375, Val Loss: 189844.71875\n",
      "Epoch 13/3000, Loss: 182891.28125, Val Loss: 189782.40625\n",
      "Epoch 14/3000, Loss: 182831.03125, Val Loss: 189713.296875\n",
      "Epoch 15/3000, Loss: 182763.984375, Val Loss: 189636.609375\n",
      "Epoch 16/3000, Loss: 182689.609375, Val Loss: 189551.578125\n",
      "Epoch 17/3000, Loss: 182607.140625, Val Loss: 189457.4375\n",
      "Epoch 18/3000, Loss: 182515.609375, Val Loss: 189353.03125\n",
      "Epoch 19/3000, Loss: 182414.109375, Val Loss: 189237.296875\n",
      "Epoch 20/3000, Loss: 182301.5625, Val Loss: 189109.109375\n",
      "Epoch 21/3000, Loss: 182176.859375, Val Loss: 188967.3125\n",
      "Epoch 22/3000, Loss: 182038.796875, Val Loss: 188810.375\n",
      "Epoch 23/3000, Loss: 181886.03125, Val Loss: 188637.078125\n",
      "Epoch 24/3000, Loss: 181717.25, Val Loss: 188446.0\n",
      "Epoch 25/3000, Loss: 181530.984375, Val Loss: 188235.546875\n",
      "Epoch 26/3000, Loss: 181325.640625, Val Loss: 188004.046875\n",
      "Epoch 27/3000, Loss: 181099.609375, Val Loss: 187749.5625\n",
      "Epoch 28/3000, Loss: 180851.078125, Val Loss: 187470.140625\n",
      "Epoch 29/3000, Loss: 180578.0, Val Loss: 187163.28125\n",
      "Epoch 30/3000, Loss: 180278.0, Val Loss: 186826.359375\n",
      "Epoch 31/3000, Loss: 179948.484375, Val Loss: 186456.671875\n",
      "Epoch 32/3000, Loss: 179586.78125, Val Loss: 186051.359375\n",
      "Epoch 33/3000, Loss: 179190.359375, Val Loss: 185608.125\n",
      "Epoch 34/3000, Loss: 178756.515625, Val Loss: 185124.65625\n",
      "Epoch 35/3000, Loss: 178282.9375, Val Loss: 184598.296875\n",
      "Epoch 36/3000, Loss: 177767.0, Val Loss: 184026.03125\n",
      "Epoch 37/3000, Loss: 177205.765625, Val Loss: 183404.640625\n",
      "Epoch 38/3000, Loss: 176596.125, Val Loss: 182730.90625\n",
      "Epoch 39/3000, Loss: 175935.0, Val Loss: 182001.546875\n",
      "Epoch 40/3000, Loss: 175219.046875, Val Loss: 181213.15625\n",
      "Epoch 41/3000, Loss: 174444.84375, Val Loss: 180362.25\n",
      "Epoch 42/3000, Loss: 173608.9375, Val Loss: 179445.3125\n",
      "Epoch 43/3000, Loss: 172707.8125, Val Loss: 178458.6875\n",
      "Epoch 44/3000, Loss: 171737.78125, Val Loss: 177398.671875\n",
      "Epoch 45/3000, Loss: 170695.21875, Val Loss: 176261.578125\n",
      "Epoch 46/3000, Loss: 169576.53125, Val Loss: 175043.75\n",
      "Epoch 47/3000, Loss: 168377.890625, Val Loss: 173741.484375\n",
      "Epoch 48/3000, Loss: 167095.609375, Val Loss: 172350.96875\n",
      "Epoch 49/3000, Loss: 165725.921875, Val Loss: 170868.390625\n",
      "Epoch 50/3000, Loss: 164265.265625, Val Loss: 169290.0\n",
      "Epoch 51/3000, Loss: 162709.8125, Val Loss: 167612.3125\n",
      "Epoch 52/3000, Loss: 161055.921875, Val Loss: 165831.9375\n",
      "Epoch 53/3000, Loss: 159300.34375, Val Loss: 163946.125\n",
      "Epoch 54/3000, Loss: 157439.84375, Val Loss: 161951.953125\n",
      "Epoch 55/3000, Loss: 155471.390625, Val Loss: 159847.65625\n",
      "Epoch 56/3000, Loss: 153392.9375, Val Loss: 157631.6875\n",
      "Epoch 57/3000, Loss: 151202.84375, Val Loss: 155302.484375\n",
      "Epoch 58/3000, Loss: 148899.65625, Val Loss: 152859.828125\n",
      "Epoch 59/3000, Loss: 146482.578125, Val Loss: 150303.625\n",
      "Epoch 60/3000, Loss: 143951.65625, Val Loss: 147634.453125\n",
      "Epoch 61/3000, Loss: 141307.453125, Val Loss: 144853.84375\n",
      "Epoch 62/3000, Loss: 138551.0, Val Loss: 141963.921875\n",
      "Epoch 63/3000, Loss: 135684.421875, Val Loss: 138967.890625\n",
      "Epoch 64/3000, Loss: 132711.015625, Val Loss: 135871.078125\n",
      "Epoch 65/3000, Loss: 129635.2734375, Val Loss: 132679.796875\n",
      "Epoch 66/3000, Loss: 126462.9921875, Val Loss: 129401.4609375\n",
      "Epoch 67/3000, Loss: 123201.2109375, Val Loss: 126045.453125\n",
      "Epoch 68/3000, Loss: 119858.4453125, Val Loss: 122622.2265625\n",
      "Epoch 69/3000, Loss: 116444.4609375, Val Loss: 119142.9453125\n",
      "Epoch 70/3000, Loss: 112970.78125, Val Loss: 115621.703125\n",
      "Epoch 71/3000, Loss: 109450.078125, Val Loss: 112073.3359375\n",
      "Epoch 72/3000, Loss: 105896.6171875, Val Loss: 108514.3671875\n",
      "Epoch 73/3000, Loss: 102326.265625, Val Loss: 104962.3359375\n",
      "Epoch 74/3000, Loss: 98755.9921875, Val Loss: 101436.2265625\n",
      "Epoch 75/3000, Loss: 95204.2265625, Val Loss: 97955.796875\n",
      "Epoch 76/3000, Loss: 91690.484375, Val Loss: 94541.7265625\n",
      "Epoch 77/3000, Loss: 88235.3828125, Val Loss: 91216.09375\n",
      "Epoch 78/3000, Loss: 84860.2578125, Val Loss: 88000.203125\n",
      "Epoch 79/3000, Loss: 81587.3828125, Val Loss: 84915.7421875\n",
      "Epoch 80/3000, Loss: 78438.9140625, Val Loss: 81984.5\n",
      "Epoch 81/3000, Loss: 75436.8984375, Val Loss: 79226.9140625\n",
      "Epoch 82/3000, Loss: 72602.984375, Val Loss: 76662.359375\n",
      "Epoch 83/3000, Loss: 69957.125, Val Loss: 74306.8203125\n",
      "Epoch 84/3000, Loss: 67516.5703125, Val Loss: 72172.3359375\n",
      "Epoch 85/3000, Loss: 65295.0, Val Loss: 70267.8671875\n",
      "Epoch 86/3000, Loss: 63302.47265625, Val Loss: 68597.5390625\n",
      "Epoch 87/3000, Loss: 61543.0703125, Val Loss: 67159.234375\n",
      "Epoch 88/3000, Loss: 60015.28125, Val Loss: 65943.4921875\n",
      "Epoch 89/3000, Loss: 58711.4296875, Val Loss: 64933.84765625\n",
      "Epoch 90/3000, Loss: 57616.47265625, Val Loss: 64107.30078125\n",
      "Epoch 91/3000, Loss: 56707.9140625, Val Loss: 63437.42578125\n",
      "Epoch 92/3000, Loss: 55959.32421875, Val Loss: 62892.26953125\n",
      "Epoch 93/3000, Loss: 55340.58203125, Val Loss: 62437.48828125\n",
      "Epoch 94/3000, Loss: 54817.90234375, Val Loss: 62039.3046875\n",
      "Epoch 95/3000, Loss: 54357.6640625, Val Loss: 61665.92578125\n",
      "Epoch 96/3000, Loss: 53929.1015625, Val Loss: 61290.8125\n",
      "Epoch 97/3000, Loss: 53505.41796875, Val Loss: 60895.09375\n",
      "Epoch 98/3000, Loss: 53067.12109375, Val Loss: 60466.2890625\n",
      "Epoch 99/3000, Loss: 52602.1015625, Val Loss: 59998.78125\n",
      "Epoch 100/3000, Loss: 52105.23046875, Val Loss: 59494.69140625\n",
      "Epoch 101/3000, Loss: 51577.51953125, Val Loss: 58959.7890625\n",
      "Epoch 102/3000, Loss: 51024.83203125, Val Loss: 58404.79296875\n",
      "Epoch 103/3000, Loss: 50456.07421875, Val Loss: 57840.12109375\n",
      "Epoch 104/3000, Loss: 49881.65625, Val Loss: 57275.2890625\n",
      "Epoch 105/3000, Loss: 49311.62890625, Val Loss: 56721.0078125\n",
      "Epoch 106/3000, Loss: 48755.78515625, Val Loss: 56185.7109375\n",
      "Epoch 107/3000, Loss: 48221.59375, Val Loss: 55675.59375\n",
      "Epoch 108/3000, Loss: 47715.1171875, Val Loss: 55195.38671875\n",
      "Epoch 109/3000, Loss: 47239.96875, Val Loss: 54746.625\n",
      "Epoch 110/3000, Loss: 46797.625, Val Loss: 54328.80078125\n",
      "Epoch 111/3000, Loss: 46388.328125, Val Loss: 53940.3125\n",
      "Epoch 112/3000, Loss: 46010.36328125, Val Loss: 53578.94921875\n",
      "Epoch 113/3000, Loss: 45660.6875, Val Loss: 53242.5703125\n",
      "Epoch 114/3000, Loss: 45335.87109375, Val Loss: 52927.171875\n",
      "Epoch 115/3000, Loss: 45031.5234375, Val Loss: 52627.85546875\n",
      "Epoch 116/3000, Loss: 44743.8125, Val Loss: 52341.2734375\n",
      "Epoch 117/3000, Loss: 44469.54296875, Val Loss: 52064.89453125\n",
      "Epoch 118/3000, Loss: 44205.78515625, Val Loss: 51796.3984375\n",
      "Epoch 119/3000, Loss: 43949.609375, Val Loss: 51533.72265625\n",
      "Epoch 120/3000, Loss: 43699.07421875, Val Loss: 51275.90625\n",
      "Epoch 121/3000, Loss: 43452.63671875, Val Loss: 51022.9296875\n",
      "Epoch 122/3000, Loss: 43209.6484375, Val Loss: 50774.3046875\n",
      "Epoch 123/3000, Loss: 42969.6015625, Val Loss: 50530.42578125\n",
      "Epoch 124/3000, Loss: 42732.70703125, Val Loss: 50291.2421875\n",
      "Epoch 125/3000, Loss: 42499.23828125, Val Loss: 50058.12109375\n",
      "Epoch 126/3000, Loss: 42270.14453125, Val Loss: 49831.41015625\n",
      "Epoch 127/3000, Loss: 42045.8671875, Val Loss: 49612.0078125\n",
      "Epoch 128/3000, Loss: 41827.1640625, Val Loss: 49400.62890625\n",
      "Epoch 129/3000, Loss: 41614.6484375, Val Loss: 49197.76171875\n",
      "Epoch 130/3000, Loss: 41408.84375, Val Loss: 49003.48046875\n",
      "Epoch 131/3000, Loss: 41210.1796875, Val Loss: 48817.57421875\n",
      "Epoch 132/3000, Loss: 41018.70703125, Val Loss: 48640.05859375\n",
      "Epoch 133/3000, Loss: 40834.4296875, Val Loss: 48470.34375\n",
      "Epoch 134/3000, Loss: 40657.2734375, Val Loss: 48308.50390625\n",
      "Epoch 135/3000, Loss: 40486.7734375, Val Loss: 48153.57421875\n",
      "Epoch 136/3000, Loss: 40322.62109375, Val Loss: 48004.8359375\n",
      "Epoch 137/3000, Loss: 40164.171875, Val Loss: 47861.6484375\n",
      "Epoch 138/3000, Loss: 40010.8359375, Val Loss: 47723.18359375\n",
      "Epoch 139/3000, Loss: 39862.00390625, Val Loss: 47588.62109375\n",
      "Epoch 140/3000, Loss: 39717.25, Val Loss: 47457.51171875\n",
      "Epoch 141/3000, Loss: 39575.921875, Val Loss: 47330.015625\n",
      "Epoch 142/3000, Loss: 39437.72265625, Val Loss: 47205.41796875\n",
      "Epoch 143/3000, Loss: 39302.47265625, Val Loss: 47083.0703125\n",
      "Epoch 144/3000, Loss: 39169.80859375, Val Loss: 46963.20703125\n",
      "Epoch 145/3000, Loss: 39039.75, Val Loss: 46845.5234375\n",
      "Epoch 146/3000, Loss: 38912.2265625, Val Loss: 46729.9609375\n",
      "Epoch 147/3000, Loss: 38787.16015625, Val Loss: 46616.640625\n",
      "Epoch 148/3000, Loss: 38664.546875, Val Loss: 46505.65625\n",
      "Epoch 149/3000, Loss: 38544.51171875, Val Loss: 46397.5234375\n",
      "Epoch 150/3000, Loss: 38427.109375, Val Loss: 46292.26953125\n",
      "Epoch 151/3000, Loss: 38312.2734375, Val Loss: 46189.8671875\n",
      "Epoch 152/3000, Loss: 38199.90625, Val Loss: 46090.26171875\n",
      "Epoch 153/3000, Loss: 38090.21875, Val Loss: 45993.44140625\n",
      "Epoch 154/3000, Loss: 37983.05078125, Val Loss: 45899.34375\n",
      "Epoch 155/3000, Loss: 37878.28515625, Val Loss: 45807.6640625\n",
      "Epoch 156/3000, Loss: 37775.91015625, Val Loss: 45718.78515625\n",
      "Epoch 157/3000, Loss: 37675.79296875, Val Loss: 45632.44140625\n",
      "Epoch 158/3000, Loss: 37577.80859375, Val Loss: 45548.7890625\n",
      "Epoch 159/3000, Loss: 37481.87890625, Val Loss: 45467.95703125\n",
      "Epoch 160/3000, Loss: 37387.90234375, Val Loss: 45389.7265625\n",
      "Epoch 161/3000, Loss: 37295.70703125, Val Loss: 45313.90234375\n",
      "Epoch 162/3000, Loss: 37205.25390625, Val Loss: 45240.81640625\n",
      "Epoch 163/3000, Loss: 37116.2578125, Val Loss: 45170.16015625\n",
      "Epoch 164/3000, Loss: 37028.73046875, Val Loss: 45102.01171875\n",
      "Epoch 165/3000, Loss: 36942.62109375, Val Loss: 45036.125\n",
      "Epoch 166/3000, Loss: 36857.875, Val Loss: 44972.21484375\n",
      "Epoch 167/3000, Loss: 36774.54296875, Val Loss: 44910.15625\n",
      "Epoch 168/3000, Loss: 36692.5546875, Val Loss: 44849.8984375\n",
      "Epoch 169/3000, Loss: 36611.87109375, Val Loss: 44791.46875\n",
      "Epoch 170/3000, Loss: 36532.32421875, Val Loss: 44734.76953125\n",
      "Epoch 171/3000, Loss: 36453.89453125, Val Loss: 44679.7265625\n",
      "Epoch 172/3000, Loss: 36376.5859375, Val Loss: 44626.203125\n",
      "Epoch 173/3000, Loss: 36300.39453125, Val Loss: 44573.9453125\n",
      "Epoch 174/3000, Loss: 36225.30859375, Val Loss: 44522.6875\n",
      "Epoch 175/3000, Loss: 36151.34765625, Val Loss: 44472.046875\n",
      "Epoch 176/3000, Loss: 36078.40625, Val Loss: 44422.2109375\n",
      "Epoch 177/3000, Loss: 36006.42578125, Val Loss: 44373.0703125\n",
      "Epoch 178/3000, Loss: 35935.49609375, Val Loss: 44324.62109375\n",
      "Epoch 179/3000, Loss: 35865.49609375, Val Loss: 44276.51953125\n",
      "Epoch 180/3000, Loss: 35796.39453125, Val Loss: 44229.046875\n",
      "Epoch 181/3000, Loss: 35728.21875, Val Loss: 44182.28515625\n",
      "Epoch 182/3000, Loss: 35660.79296875, Val Loss: 44135.8046875\n",
      "Epoch 183/3000, Loss: 35594.0546875, Val Loss: 44089.57421875\n",
      "Epoch 184/3000, Loss: 35528.09765625, Val Loss: 44043.69921875\n",
      "Epoch 185/3000, Loss: 35462.84765625, Val Loss: 43997.80859375\n",
      "Epoch 186/3000, Loss: 35398.29296875, Val Loss: 43952.2109375\n",
      "Epoch 187/3000, Loss: 35334.53125, Val Loss: 43906.83984375\n",
      "Epoch 188/3000, Loss: 35271.39453125, Val Loss: 43861.55078125\n",
      "Epoch 189/3000, Loss: 35208.87890625, Val Loss: 43816.72265625\n",
      "Epoch 190/3000, Loss: 35147.0703125, Val Loss: 43772.19140625\n",
      "Epoch 191/3000, Loss: 35085.9296875, Val Loss: 43728.2421875\n",
      "Epoch 192/3000, Loss: 35025.37890625, Val Loss: 43684.78125\n",
      "Epoch 193/3000, Loss: 34965.38671875, Val Loss: 43641.60546875\n",
      "Epoch 194/3000, Loss: 34905.8828125, Val Loss: 43598.63671875\n",
      "Epoch 195/3000, Loss: 34846.8828125, Val Loss: 43555.90234375\n",
      "Epoch 196/3000, Loss: 34788.375, Val Loss: 43513.5546875\n",
      "Epoch 197/3000, Loss: 34730.22265625, Val Loss: 43471.625\n",
      "Epoch 198/3000, Loss: 34672.61328125, Val Loss: 43430.2421875\n",
      "Epoch 199/3000, Loss: 34615.4453125, Val Loss: 43389.58984375\n",
      "Epoch 200/3000, Loss: 34558.67578125, Val Loss: 43349.35546875\n",
      "Epoch 201/3000, Loss: 34502.2890625, Val Loss: 43309.48046875\n",
      "Epoch 202/3000, Loss: 34446.28515625, Val Loss: 43269.96875\n",
      "Epoch 203/3000, Loss: 34390.70703125, Val Loss: 43230.6875\n",
      "Epoch 204/3000, Loss: 34335.56640625, Val Loss: 43191.66015625\n",
      "Epoch 205/3000, Loss: 34280.828125, Val Loss: 43152.9921875\n",
      "Epoch 206/3000, Loss: 34226.53125, Val Loss: 43115.0390625\n",
      "Epoch 207/3000, Loss: 34172.57421875, Val Loss: 43077.83203125\n",
      "Epoch 208/3000, Loss: 34118.8984375, Val Loss: 43041.22265625\n",
      "Epoch 209/3000, Loss: 34065.45703125, Val Loss: 43004.9375\n",
      "Epoch 210/3000, Loss: 34012.2421875, Val Loss: 42969.0\n",
      "Epoch 211/3000, Loss: 33959.39453125, Val Loss: 42933.28125\n",
      "Epoch 212/3000, Loss: 33906.83984375, Val Loss: 42897.7578125\n",
      "Epoch 213/3000, Loss: 33854.63671875, Val Loss: 42862.046875\n",
      "Epoch 214/3000, Loss: 33802.734375, Val Loss: 42826.20703125\n",
      "Epoch 215/3000, Loss: 33751.06640625, Val Loss: 42790.44140625\n",
      "Epoch 216/3000, Loss: 33699.640625, Val Loss: 42754.41796875\n",
      "Epoch 217/3000, Loss: 33648.421875, Val Loss: 42718.33203125\n",
      "Epoch 218/3000, Loss: 33597.41015625, Val Loss: 42682.453125\n",
      "Epoch 219/3000, Loss: 33546.69921875, Val Loss: 42646.58984375\n",
      "Epoch 220/3000, Loss: 33496.31640625, Val Loss: 42610.62109375\n",
      "Epoch 221/3000, Loss: 33446.2109375, Val Loss: 42574.63671875\n",
      "Epoch 222/3000, Loss: 33396.30078125, Val Loss: 42538.8125\n",
      "Epoch 223/3000, Loss: 33346.59375, Val Loss: 42503.08203125\n",
      "Epoch 224/3000, Loss: 33297.140625, Val Loss: 42467.65234375\n",
      "Epoch 225/3000, Loss: 33248.078125, Val Loss: 42432.3046875\n",
      "Epoch 226/3000, Loss: 33199.26953125, Val Loss: 42397.08984375\n",
      "Epoch 227/3000, Loss: 33150.625, Val Loss: 42361.85546875\n",
      "Epoch 228/3000, Loss: 33102.09765625, Val Loss: 42326.89453125\n",
      "Epoch 229/3000, Loss: 33053.7890625, Val Loss: 42292.0703125\n",
      "Epoch 230/3000, Loss: 33005.6953125, Val Loss: 42257.48046875\n",
      "Epoch 231/3000, Loss: 32957.83203125, Val Loss: 42222.84765625\n",
      "Epoch 232/3000, Loss: 32910.140625, Val Loss: 42188.46484375\n",
      "Epoch 233/3000, Loss: 32862.5859375, Val Loss: 42154.41015625\n",
      "Epoch 234/3000, Loss: 32815.21484375, Val Loss: 42120.5546875\n",
      "Epoch 235/3000, Loss: 32767.927734375, Val Loss: 42086.91015625\n",
      "Epoch 236/3000, Loss: 32720.837890625, Val Loss: 42053.46875\n",
      "Epoch 237/3000, Loss: 32673.990234375, Val Loss: 42020.421875\n",
      "Epoch 238/3000, Loss: 32627.27734375, Val Loss: 41987.5625\n",
      "Epoch 239/3000, Loss: 32580.783203125, Val Loss: 41954.734375\n",
      "Epoch 240/3000, Loss: 32534.4375, Val Loss: 41922.01171875\n",
      "Epoch 241/3000, Loss: 32488.263671875, Val Loss: 41889.4296875\n",
      "Epoch 242/3000, Loss: 32442.322265625, Val Loss: 41856.9296875\n",
      "Epoch 243/3000, Loss: 32396.5703125, Val Loss: 41824.46484375\n",
      "Epoch 244/3000, Loss: 32351.03515625, Val Loss: 41791.7734375\n",
      "Epoch 245/3000, Loss: 32305.662109375, Val Loss: 41759.17578125\n",
      "Epoch 246/3000, Loss: 32260.462890625, Val Loss: 41726.6640625\n",
      "Epoch 247/3000, Loss: 32215.408203125, Val Loss: 41694.2578125\n",
      "Epoch 248/3000, Loss: 32170.478515625, Val Loss: 41661.68359375\n",
      "Epoch 249/3000, Loss: 32125.615234375, Val Loss: 41628.95703125\n",
      "Epoch 250/3000, Loss: 32080.91015625, Val Loss: 41596.1015625\n",
      "Epoch 251/3000, Loss: 32036.322265625, Val Loss: 41563.29296875\n",
      "Epoch 252/3000, Loss: 31991.87890625, Val Loss: 41530.5234375\n",
      "Epoch 253/3000, Loss: 31947.572265625, Val Loss: 41497.92578125\n",
      "Epoch 254/3000, Loss: 31903.501953125, Val Loss: 41465.57421875\n",
      "Epoch 255/3000, Loss: 31859.583984375, Val Loss: 41433.44140625\n",
      "Epoch 256/3000, Loss: 31815.8203125, Val Loss: 41401.4140625\n",
      "Epoch 257/3000, Loss: 31772.21875, Val Loss: 41369.34765625\n",
      "Epoch 258/3000, Loss: 31728.80078125, Val Loss: 41337.109375\n",
      "Epoch 259/3000, Loss: 31685.5390625, Val Loss: 41304.74609375\n",
      "Epoch 260/3000, Loss: 31642.392578125, Val Loss: 41272.27734375\n",
      "Epoch 261/3000, Loss: 31599.404296875, Val Loss: 41240.09375\n",
      "Epoch 262/3000, Loss: 31556.568359375, Val Loss: 41208.03125\n",
      "Epoch 263/3000, Loss: 31513.857421875, Val Loss: 41176.13671875\n",
      "Epoch 264/3000, Loss: 31471.30859375, Val Loss: 41144.4765625\n",
      "Epoch 265/3000, Loss: 31428.89453125, Val Loss: 41113.078125\n",
      "Epoch 266/3000, Loss: 31386.630859375, Val Loss: 41081.97265625\n",
      "Epoch 267/3000, Loss: 31344.466796875, Val Loss: 41050.82421875\n",
      "Epoch 268/3000, Loss: 31302.349609375, Val Loss: 41019.57421875\n",
      "Epoch 269/3000, Loss: 31260.34765625, Val Loss: 40987.51953125\n",
      "Epoch 270/3000, Loss: 31218.375, Val Loss: 40954.95703125\n",
      "Epoch 271/3000, Loss: 31176.525390625, Val Loss: 40922.14453125\n",
      "Epoch 272/3000, Loss: 31134.80859375, Val Loss: 40889.3046875\n",
      "Epoch 273/3000, Loss: 31093.1875, Val Loss: 40856.55078125\n",
      "Epoch 274/3000, Loss: 31051.70703125, Val Loss: 40823.86328125\n",
      "Epoch 275/3000, Loss: 31010.322265625, Val Loss: 40791.24609375\n",
      "Epoch 276/3000, Loss: 30969.041015625, Val Loss: 40759.078125\n",
      "Epoch 277/3000, Loss: 30927.908203125, Val Loss: 40727.1875\n",
      "Epoch 278/3000, Loss: 30886.884765625, Val Loss: 40695.08984375\n",
      "Epoch 279/3000, Loss: 30845.71484375, Val Loss: 40662.79296875\n",
      "Epoch 280/3000, Loss: 30804.595703125, Val Loss: 40629.765625\n",
      "Epoch 281/3000, Loss: 30763.466796875, Val Loss: 40596.5703125\n",
      "Epoch 282/3000, Loss: 30722.416015625, Val Loss: 40563.1640625\n",
      "Epoch 283/3000, Loss: 30681.376953125, Val Loss: 40529.4453125\n",
      "Epoch 284/3000, Loss: 30640.357421875, Val Loss: 40495.44921875\n",
      "Epoch 285/3000, Loss: 30599.357421875, Val Loss: 40461.5078125\n",
      "Epoch 286/3000, Loss: 30558.419921875, Val Loss: 40427.69140625\n",
      "Epoch 287/3000, Loss: 30517.48828125, Val Loss: 40393.68359375\n",
      "Epoch 288/3000, Loss: 30476.505859375, Val Loss: 40359.8125\n",
      "Epoch 289/3000, Loss: 30435.587890625, Val Loss: 40325.96484375\n",
      "Epoch 290/3000, Loss: 30394.767578125, Val Loss: 40292.109375\n",
      "Epoch 291/3000, Loss: 30354.064453125, Val Loss: 40258.42578125\n",
      "Epoch 292/3000, Loss: 30313.435546875, Val Loss: 40224.8046875\n",
      "Epoch 293/3000, Loss: 30272.677734375, Val Loss: 40190.984375\n",
      "Epoch 294/3000, Loss: 30231.8203125, Val Loss: 40157.4765625\n",
      "Epoch 295/3000, Loss: 30191.056640625, Val Loss: 40124.32421875\n",
      "Epoch 296/3000, Loss: 30150.369140625, Val Loss: 40091.46484375\n",
      "Epoch 297/3000, Loss: 30109.76171875, Val Loss: 40058.85546875\n",
      "Epoch 298/3000, Loss: 30069.208984375, Val Loss: 40026.546875\n",
      "Epoch 299/3000, Loss: 30028.7265625, Val Loss: 39994.171875\n",
      "Epoch 300/3000, Loss: 29988.224609375, Val Loss: 39961.56640625\n",
      "Epoch 301/3000, Loss: 29947.76953125, Val Loss: 39928.734375\n",
      "Epoch 302/3000, Loss: 29907.318359375, Val Loss: 39895.8046875\n",
      "Epoch 303/3000, Loss: 29866.9921875, Val Loss: 39862.609375\n",
      "Epoch 304/3000, Loss: 29826.796875, Val Loss: 39829.59765625\n",
      "Epoch 305/3000, Loss: 29786.740234375, Val Loss: 39796.91796875\n",
      "Epoch 306/3000, Loss: 29746.916015625, Val Loss: 39764.296875\n",
      "Epoch 307/3000, Loss: 29707.046875, Val Loss: 39731.78125\n",
      "Epoch 308/3000, Loss: 29667.283203125, Val Loss: 39699.6171875\n",
      "Epoch 309/3000, Loss: 29627.599609375, Val Loss: 39667.99609375\n",
      "Epoch 310/3000, Loss: 29587.98828125, Val Loss: 39636.5703125\n",
      "Epoch 311/3000, Loss: 29548.447265625, Val Loss: 39604.94140625\n",
      "Epoch 312/3000, Loss: 29508.98046875, Val Loss: 39573.140625\n",
      "Epoch 313/3000, Loss: 29469.58203125, Val Loss: 39541.51953125\n",
      "Epoch 314/3000, Loss: 29430.3046875, Val Loss: 39510.07421875\n",
      "Epoch 315/3000, Loss: 29391.162109375, Val Loss: 39478.64453125\n",
      "Epoch 316/3000, Loss: 29352.103515625, Val Loss: 39447.26953125\n",
      "Epoch 317/3000, Loss: 29313.138671875, Val Loss: 39415.65625\n",
      "Epoch 318/3000, Loss: 29274.04296875, Val Loss: 39383.203125\n",
      "Epoch 319/3000, Loss: 29234.66796875, Val Loss: 39349.80078125\n",
      "Epoch 320/3000, Loss: 29195.04296875, Val Loss: 39316.125\n",
      "Epoch 321/3000, Loss: 29155.603515625, Val Loss: 39282.61328125\n",
      "Epoch 322/3000, Loss: 29116.29296875, Val Loss: 39249.73046875\n",
      "Epoch 323/3000, Loss: 29077.076171875, Val Loss: 39216.953125\n",
      "Epoch 324/3000, Loss: 29037.7578125, Val Loss: 39184.34765625\n",
      "Epoch 325/3000, Loss: 28998.396484375, Val Loss: 39151.71875\n",
      "Epoch 326/3000, Loss: 28959.046875, Val Loss: 39119.36328125\n",
      "Epoch 327/3000, Loss: 28919.701171875, Val Loss: 39086.94921875\n",
      "Epoch 328/3000, Loss: 28880.384765625, Val Loss: 39054.63671875\n",
      "Epoch 329/3000, Loss: 28841.1640625, Val Loss: 39022.40234375\n",
      "Epoch 330/3000, Loss: 28802.12890625, Val Loss: 38990.74609375\n",
      "Epoch 331/3000, Loss: 28763.271484375, Val Loss: 38959.80859375\n",
      "Epoch 332/3000, Loss: 28724.408203125, Val Loss: 38929.08984375\n",
      "Epoch 333/3000, Loss: 28685.69140625, Val Loss: 38898.20703125\n",
      "Epoch 334/3000, Loss: 28646.888671875, Val Loss: 38866.7109375\n",
      "Epoch 335/3000, Loss: 28607.765625, Val Loss: 38835.30859375\n",
      "Epoch 336/3000, Loss: 28568.587890625, Val Loss: 38803.46484375\n",
      "Epoch 337/3000, Loss: 28529.396484375, Val Loss: 38771.6171875\n",
      "Epoch 338/3000, Loss: 28490.33203125, Val Loss: 38739.45703125\n",
      "Epoch 339/3000, Loss: 28451.404296875, Val Loss: 38707.51171875\n",
      "Epoch 340/3000, Loss: 28412.525390625, Val Loss: 38675.890625\n",
      "Epoch 341/3000, Loss: 28373.650390625, Val Loss: 38644.27734375\n",
      "Epoch 342/3000, Loss: 28334.69140625, Val Loss: 38612.75390625\n",
      "Epoch 343/3000, Loss: 28295.85546875, Val Loss: 38581.6015625\n",
      "Epoch 344/3000, Loss: 28257.02734375, Val Loss: 38550.48828125\n",
      "Epoch 345/3000, Loss: 28218.189453125, Val Loss: 38519.4296875\n",
      "Epoch 346/3000, Loss: 28179.341796875, Val Loss: 38488.91796875\n",
      "Epoch 347/3000, Loss: 28140.5703125, Val Loss: 38458.59375\n",
      "Epoch 348/3000, Loss: 28101.830078125, Val Loss: 38428.4921875\n",
      "Epoch 349/3000, Loss: 28063.22265625, Val Loss: 38398.90625\n",
      "Epoch 350/3000, Loss: 28024.712890625, Val Loss: 38369.72265625\n",
      "Epoch 351/3000, Loss: 27986.431640625, Val Loss: 38340.5234375\n",
      "Epoch 352/3000, Loss: 27948.279296875, Val Loss: 38311.0859375\n",
      "Epoch 353/3000, Loss: 27910.21875, Val Loss: 38281.73046875\n",
      "Epoch 354/3000, Loss: 27872.26953125, Val Loss: 38252.00390625\n",
      "Epoch 355/3000, Loss: 27834.361328125, Val Loss: 38222.1875\n",
      "Epoch 356/3000, Loss: 27796.4453125, Val Loss: 38192.578125\n",
      "Epoch 357/3000, Loss: 27758.560546875, Val Loss: 38163.09765625\n",
      "Epoch 358/3000, Loss: 27720.693359375, Val Loss: 38133.4921875\n",
      "Epoch 359/3000, Loss: 27682.83203125, Val Loss: 38104.38671875\n",
      "Epoch 360/3000, Loss: 27644.931640625, Val Loss: 38075.6953125\n",
      "Epoch 361/3000, Loss: 27607.51953125, Val Loss: 38047.73046875\n",
      "Epoch 362/3000, Loss: 27570.0546875, Val Loss: 38019.9140625\n",
      "Epoch 363/3000, Loss: 27532.212890625, Val Loss: 37992.46484375\n",
      "Epoch 364/3000, Loss: 27494.4375, Val Loss: 37965.203125\n",
      "Epoch 365/3000, Loss: 27457.021484375, Val Loss: 37937.9921875\n",
      "Epoch 366/3000, Loss: 27419.435546875, Val Loss: 37910.6796875\n",
      "Epoch 367/3000, Loss: 27381.64453125, Val Loss: 37883.14453125\n",
      "Epoch 368/3000, Loss: 27343.63671875, Val Loss: 37854.84765625\n",
      "Epoch 369/3000, Loss: 27305.8984375, Val Loss: 37827.4609375\n",
      "Epoch 370/3000, Loss: 27268.37109375, Val Loss: 37799.8359375\n",
      "Epoch 371/3000, Loss: 27230.44921875, Val Loss: 37772.95703125\n",
      "Epoch 372/3000, Loss: 27192.912109375, Val Loss: 37746.0859375\n",
      "Epoch 373/3000, Loss: 27155.44140625, Val Loss: 37719.02734375\n",
      "Epoch 374/3000, Loss: 27117.841796875, Val Loss: 37691.40234375\n",
      "Epoch 375/3000, Loss: 27080.1796875, Val Loss: 37663.81640625\n",
      "Epoch 376/3000, Loss: 27042.74609375, Val Loss: 37636.8984375\n",
      "Epoch 377/3000, Loss: 27005.11328125, Val Loss: 37610.62109375\n",
      "Epoch 378/3000, Loss: 26967.3671875, Val Loss: 37583.34375\n",
      "Epoch 379/3000, Loss: 26929.615234375, Val Loss: 37555.33203125\n",
      "Epoch 380/3000, Loss: 26891.802734375, Val Loss: 37527.73828125\n",
      "Epoch 381/3000, Loss: 26853.66796875, Val Loss: 37500.2421875\n",
      "Epoch 382/3000, Loss: 26815.55078125, Val Loss: 37472.6875\n",
      "Epoch 383/3000, Loss: 26777.318359375, Val Loss: 37445.01171875\n",
      "Epoch 384/3000, Loss: 26739.109375, Val Loss: 37416.8203125\n",
      "Epoch 385/3000, Loss: 26700.578125, Val Loss: 37388.87890625\n",
      "Epoch 386/3000, Loss: 26661.9296875, Val Loss: 37361.6015625\n",
      "Epoch 387/3000, Loss: 26623.123046875, Val Loss: 37333.3515625\n",
      "Epoch 388/3000, Loss: 26583.966796875, Val Loss: 37303.6953125\n",
      "Epoch 389/3000, Loss: 26544.376953125, Val Loss: 37273.2890625\n",
      "Epoch 390/3000, Loss: 26504.68359375, Val Loss: 37242.7421875\n",
      "Epoch 391/3000, Loss: 26464.98046875, Val Loss: 37212.328125\n",
      "Epoch 392/3000, Loss: 26425.080078125, Val Loss: 37180.7265625\n",
      "Epoch 393/3000, Loss: 26385.013671875, Val Loss: 37149.4765625\n",
      "Epoch 394/3000, Loss: 26345.017578125, Val Loss: 37118.6640625\n",
      "Epoch 395/3000, Loss: 26304.916015625, Val Loss: 37088.1953125\n",
      "Epoch 396/3000, Loss: 26264.62109375, Val Loss: 37057.70703125\n",
      "Epoch 397/3000, Loss: 26224.41015625, Val Loss: 37028.17578125\n",
      "Epoch 398/3000, Loss: 26184.251953125, Val Loss: 36999.8515625\n",
      "Epoch 399/3000, Loss: 26144.0078125, Val Loss: 36971.9609375\n",
      "Epoch 400/3000, Loss: 26103.63671875, Val Loss: 36945.35546875\n",
      "Epoch 401/3000, Loss: 26063.30078125, Val Loss: 36919.5078125\n",
      "Epoch 402/3000, Loss: 26023.310546875, Val Loss: 36893.42578125\n",
      "Epoch 403/3000, Loss: 25983.236328125, Val Loss: 36867.1328125\n",
      "Epoch 404/3000, Loss: 25943.220703125, Val Loss: 36841.28515625\n",
      "Epoch 405/3000, Loss: 25903.130859375, Val Loss: 36815.5625\n",
      "Epoch 406/3000, Loss: 25862.9609375, Val Loss: 36789.48828125\n",
      "Epoch 407/3000, Loss: 25822.64453125, Val Loss: 36762.18359375\n",
      "Epoch 408/3000, Loss: 25782.208984375, Val Loss: 36734.86328125\n",
      "Epoch 409/3000, Loss: 25741.814453125, Val Loss: 36707.98828125\n",
      "Epoch 410/3000, Loss: 25701.076171875, Val Loss: 36681.06640625\n",
      "Epoch 411/3000, Loss: 25660.099609375, Val Loss: 36653.0625\n",
      "Epoch 412/3000, Loss: 25618.806640625, Val Loss: 36623.4765625\n",
      "Epoch 413/3000, Loss: 25577.25390625, Val Loss: 36593.3203125\n",
      "Epoch 414/3000, Loss: 25535.5546875, Val Loss: 36563.51171875\n",
      "Epoch 415/3000, Loss: 25493.10546875, Val Loss: 36533.10546875\n",
      "Epoch 416/3000, Loss: 25450.349609375, Val Loss: 36500.83984375\n",
      "Epoch 417/3000, Loss: 25407.505859375, Val Loss: 36468.2265625\n",
      "Epoch 418/3000, Loss: 25364.8125, Val Loss: 36435.5859375\n",
      "Epoch 419/3000, Loss: 25322.205078125, Val Loss: 36404.359375\n",
      "Epoch 420/3000, Loss: 25279.681640625, Val Loss: 36371.828125\n",
      "Epoch 421/3000, Loss: 25236.68359375, Val Loss: 36338.1875\n",
      "Epoch 422/3000, Loss: 25193.337890625, Val Loss: 36303.82421875\n",
      "Epoch 423/3000, Loss: 25149.9453125, Val Loss: 36271.44140625\n",
      "Epoch 424/3000, Loss: 25106.25, Val Loss: 36240.08203125\n",
      "Epoch 425/3000, Loss: 25062.44140625, Val Loss: 36207.35546875\n",
      "Epoch 426/3000, Loss: 25018.36328125, Val Loss: 36173.84375\n",
      "Epoch 427/3000, Loss: 24974.361328125, Val Loss: 36139.53125\n",
      "Epoch 428/3000, Loss: 24930.515625, Val Loss: 36106.31640625\n",
      "Epoch 429/3000, Loss: 24885.66796875, Val Loss: 36072.52734375\n",
      "Epoch 430/3000, Loss: 24840.09765625, Val Loss: 36036.72265625\n",
      "Epoch 431/3000, Loss: 24793.0859375, Val Loss: 36000.6796875\n",
      "Epoch 432/3000, Loss: 24744.841796875, Val Loss: 35966.65625\n",
      "Epoch 433/3000, Loss: 24695.6640625, Val Loss: 35935.12890625\n",
      "Epoch 434/3000, Loss: 24645.73046875, Val Loss: 35902.1171875\n",
      "Epoch 435/3000, Loss: 24594.935546875, Val Loss: 35869.6953125\n",
      "Epoch 436/3000, Loss: 24543.181640625, Val Loss: 35839.32421875\n",
      "Epoch 437/3000, Loss: 24491.24609375, Val Loss: 35811.2890625\n",
      "Epoch 438/3000, Loss: 24438.888671875, Val Loss: 35784.6015625\n",
      "Epoch 439/3000, Loss: 24386.09375, Val Loss: 35758.02734375\n",
      "Epoch 440/3000, Loss: 24334.490234375, Val Loss: 35731.9296875\n",
      "Epoch 441/3000, Loss: 24283.388671875, Val Loss: 35705.65625\n",
      "Epoch 442/3000, Loss: 24232.873046875, Val Loss: 35680.5703125\n",
      "Epoch 443/3000, Loss: 24182.857421875, Val Loss: 35655.87890625\n",
      "Epoch 444/3000, Loss: 24133.337890625, Val Loss: 35631.81640625\n",
      "Epoch 445/3000, Loss: 24084.345703125, Val Loss: 35608.3046875\n",
      "Epoch 446/3000, Loss: 24035.40625, Val Loss: 35585.4296875\n",
      "Epoch 447/3000, Loss: 23986.333984375, Val Loss: 35563.05859375\n",
      "Epoch 448/3000, Loss: 23936.947265625, Val Loss: 35541.390625\n",
      "Epoch 449/3000, Loss: 23887.833984375, Val Loss: 35520.796875\n",
      "Epoch 450/3000, Loss: 23838.515625, Val Loss: 35499.89453125\n",
      "Epoch 451/3000, Loss: 23789.15234375, Val Loss: 35478.0\n",
      "Epoch 452/3000, Loss: 23739.73046875, Val Loss: 35455.55078125\n",
      "Epoch 453/3000, Loss: 23690.17578125, Val Loss: 35433.53515625\n",
      "Epoch 454/3000, Loss: 23640.88671875, Val Loss: 35409.30859375\n",
      "Epoch 455/3000, Loss: 23591.84765625, Val Loss: 35384.1875\n",
      "Epoch 456/3000, Loss: 23543.083984375, Val Loss: 35359.61328125\n",
      "Epoch 457/3000, Loss: 23494.013671875, Val Loss: 35335.45703125\n",
      "Epoch 458/3000, Loss: 23444.470703125, Val Loss: 35311.73046875\n",
      "Epoch 459/3000, Loss: 23394.916015625, Val Loss: 35284.09375\n",
      "Epoch 460/3000, Loss: 23345.2578125, Val Loss: 35254.6953125\n",
      "Epoch 461/3000, Loss: 23295.80859375, Val Loss: 35225.92578125\n",
      "Epoch 462/3000, Loss: 23246.66015625, Val Loss: 35198.80078125\n",
      "Epoch 463/3000, Loss: 23197.5703125, Val Loss: 35172.09765625\n",
      "Epoch 464/3000, Loss: 23148.568359375, Val Loss: 35145.9453125\n",
      "Epoch 465/3000, Loss: 23099.5546875, Val Loss: 35121.46875\n",
      "Epoch 466/3000, Loss: 23050.65625, Val Loss: 35098.02734375\n",
      "Epoch 467/3000, Loss: 23001.61328125, Val Loss: 35074.7109375\n",
      "Epoch 468/3000, Loss: 22952.15625, Val Loss: 35053.125\n",
      "Epoch 469/3000, Loss: 22902.7890625, Val Loss: 35031.72265625\n",
      "Epoch 470/3000, Loss: 22853.482421875, Val Loss: 35011.04296875\n",
      "Epoch 471/3000, Loss: 22804.359375, Val Loss: 34989.65625\n",
      "Epoch 472/3000, Loss: 22755.41015625, Val Loss: 34967.7578125\n",
      "Epoch 473/3000, Loss: 22706.501953125, Val Loss: 34946.9375\n",
      "Epoch 474/3000, Loss: 22657.5546875, Val Loss: 34926.5390625\n",
      "Epoch 475/3000, Loss: 22608.677734375, Val Loss: 34906.5859375\n",
      "Epoch 476/3000, Loss: 22559.732421875, Val Loss: 34886.40234375\n",
      "Epoch 477/3000, Loss: 22511.01171875, Val Loss: 34867.89453125\n",
      "Epoch 478/3000, Loss: 22462.556640625, Val Loss: 34850.6015625\n",
      "Epoch 479/3000, Loss: 22414.091796875, Val Loss: 34835.09375\n",
      "Epoch 480/3000, Loss: 22365.599609375, Val Loss: 34819.546875\n",
      "Epoch 481/3000, Loss: 22317.15234375, Val Loss: 34804.4453125\n",
      "Epoch 482/3000, Loss: 22268.9296875, Val Loss: 34788.90625\n",
      "Epoch 483/3000, Loss: 22220.6640625, Val Loss: 34771.66796875\n",
      "Epoch 484/3000, Loss: 22172.4453125, Val Loss: 34753.48828125\n",
      "Epoch 485/3000, Loss: 22124.30859375, Val Loss: 34733.234375\n",
      "Epoch 486/3000, Loss: 22076.068359375, Val Loss: 34712.64453125\n",
      "Epoch 487/3000, Loss: 22027.921875, Val Loss: 34691.04296875\n",
      "Epoch 488/3000, Loss: 21979.86328125, Val Loss: 34668.921875\n",
      "Epoch 489/3000, Loss: 21932.095703125, Val Loss: 34646.38671875\n",
      "Epoch 490/3000, Loss: 21883.958984375, Val Loss: 34625.07421875\n",
      "Epoch 491/3000, Loss: 21836.09375, Val Loss: 34605.09765625\n",
      "Epoch 492/3000, Loss: 21788.296875, Val Loss: 34584.9921875\n",
      "Epoch 493/3000, Loss: 21740.939453125, Val Loss: 34564.5703125\n",
      "Epoch 494/3000, Loss: 21693.873046875, Val Loss: 34544.02734375\n",
      "Epoch 495/3000, Loss: 21646.822265625, Val Loss: 34525.06640625\n",
      "Epoch 496/3000, Loss: 21599.771484375, Val Loss: 34506.14453125\n",
      "Epoch 497/3000, Loss: 21552.884765625, Val Loss: 34486.0\n",
      "Epoch 498/3000, Loss: 21505.87890625, Val Loss: 34466.46484375\n",
      "Epoch 499/3000, Loss: 21458.728515625, Val Loss: 34447.97265625\n",
      "Epoch 500/3000, Loss: 21411.787109375, Val Loss: 34429.71484375\n",
      "Epoch 501/3000, Loss: 21364.8828125, Val Loss: 34409.0078125\n",
      "Epoch 502/3000, Loss: 21318.283203125, Val Loss: 34384.734375\n",
      "Epoch 503/3000, Loss: 21271.703125, Val Loss: 34360.9140625\n",
      "Epoch 504/3000, Loss: 21225.35546875, Val Loss: 34339.41015625\n",
      "Epoch 505/3000, Loss: 21178.921875, Val Loss: 34317.86328125\n",
      "Epoch 506/3000, Loss: 21132.62890625, Val Loss: 34293.9921875\n",
      "Epoch 507/3000, Loss: 21086.337890625, Val Loss: 34272.97265625\n",
      "Epoch 508/3000, Loss: 21040.330078125, Val Loss: 34254.33203125\n",
      "Epoch 509/3000, Loss: 20994.30859375, Val Loss: 34236.3125\n",
      "Epoch 510/3000, Loss: 20948.412109375, Val Loss: 34217.57421875\n",
      "Epoch 511/3000, Loss: 20902.69140625, Val Loss: 34197.12890625\n",
      "Epoch 512/3000, Loss: 20857.111328125, Val Loss: 34176.546875\n",
      "Epoch 513/3000, Loss: 20811.73828125, Val Loss: 34154.9765625\n",
      "Epoch 514/3000, Loss: 20766.361328125, Val Loss: 34132.07421875\n",
      "Epoch 515/3000, Loss: 20721.16796875, Val Loss: 34109.37890625\n",
      "Epoch 516/3000, Loss: 20676.193359375, Val Loss: 34085.53515625\n",
      "Epoch 517/3000, Loss: 20631.3203125, Val Loss: 34061.93359375\n",
      "Epoch 518/3000, Loss: 20586.41015625, Val Loss: 34041.80078125\n",
      "Epoch 519/3000, Loss: 20541.6640625, Val Loss: 34023.37890625\n",
      "Epoch 520/3000, Loss: 20496.5546875, Val Loss: 34004.41796875\n",
      "Epoch 521/3000, Loss: 20451.90625, Val Loss: 33986.62890625\n",
      "Epoch 522/3000, Loss: 20407.630859375, Val Loss: 33969.3203125\n",
      "Epoch 523/3000, Loss: 20363.728515625, Val Loss: 33952.32421875\n",
      "Epoch 524/3000, Loss: 20319.486328125, Val Loss: 33936.61328125\n",
      "Epoch 525/3000, Loss: 20275.537109375, Val Loss: 33922.0546875\n",
      "Epoch 526/3000, Loss: 20231.21484375, Val Loss: 33908.703125\n",
      "Epoch 527/3000, Loss: 20187.158203125, Val Loss: 33893.515625\n",
      "Epoch 528/3000, Loss: 20142.97265625, Val Loss: 33875.8515625\n",
      "Epoch 529/3000, Loss: 20099.1171875, Val Loss: 33858.8203125\n",
      "Epoch 530/3000, Loss: 20055.076171875, Val Loss: 33841.19921875\n",
      "Epoch 531/3000, Loss: 20011.35546875, Val Loss: 33821.82421875\n",
      "Epoch 532/3000, Loss: 19967.353515625, Val Loss: 33802.09375\n",
      "Epoch 533/3000, Loss: 19923.8671875, Val Loss: 33783.6796875\n",
      "Epoch 534/3000, Loss: 19880.265625, Val Loss: 33766.30859375\n",
      "Epoch 535/3000, Loss: 19837.328125, Val Loss: 33746.046875\n",
      "Epoch 536/3000, Loss: 19793.556640625, Val Loss: 33726.953125\n",
      "Epoch 537/3000, Loss: 19750.611328125, Val Loss: 33708.796875\n",
      "Epoch 538/3000, Loss: 19707.73046875, Val Loss: 33691.9453125\n",
      "Epoch 539/3000, Loss: 19664.53515625, Val Loss: 33674.671875\n",
      "Epoch 540/3000, Loss: 19621.599609375, Val Loss: 33656.79296875\n",
      "Epoch 541/3000, Loss: 19578.853515625, Val Loss: 33637.41796875\n",
      "Epoch 542/3000, Loss: 19536.0859375, Val Loss: 33617.51171875\n",
      "Epoch 543/3000, Loss: 19493.478515625, Val Loss: 33599.453125\n",
      "Epoch 544/3000, Loss: 19450.822265625, Val Loss: 33582.21484375\n",
      "Epoch 545/3000, Loss: 19408.5078125, Val Loss: 33563.5625\n",
      "Epoch 546/3000, Loss: 19366.447265625, Val Loss: 33542.5390625\n",
      "Epoch 547/3000, Loss: 19324.0625, Val Loss: 33524.3046875\n",
      "Epoch 548/3000, Loss: 19282.09765625, Val Loss: 33506.25390625\n",
      "Epoch 549/3000, Loss: 19239.998046875, Val Loss: 33487.21875\n",
      "Epoch 550/3000, Loss: 19197.939453125, Val Loss: 33468.87109375\n",
      "Epoch 551/3000, Loss: 19156.515625, Val Loss: 33450.0546875\n",
      "Epoch 552/3000, Loss: 19114.5546875, Val Loss: 33430.921875\n",
      "Epoch 553/3000, Loss: 19072.994140625, Val Loss: 33412.3984375\n",
      "Epoch 554/3000, Loss: 19031.5234375, Val Loss: 33396.2265625\n",
      "Epoch 555/3000, Loss: 18989.6640625, Val Loss: 33382.9375\n",
      "Epoch 556/3000, Loss: 18948.56640625, Val Loss: 33363.80859375\n",
      "Epoch 557/3000, Loss: 18906.806640625, Val Loss: 33341.75390625\n",
      "Epoch 558/3000, Loss: 18865.15234375, Val Loss: 33321.52734375\n",
      "Epoch 559/3000, Loss: 18823.896484375, Val Loss: 33303.9609375\n",
      "Epoch 560/3000, Loss: 18782.34375, Val Loss: 33286.33203125\n",
      "Epoch 561/3000, Loss: 18741.517578125, Val Loss: 33268.421875\n",
      "Epoch 562/3000, Loss: 18700.607421875, Val Loss: 33248.7578125\n",
      "Epoch 563/3000, Loss: 18659.107421875, Val Loss: 33228.00390625\n",
      "Epoch 564/3000, Loss: 18618.400390625, Val Loss: 33208.171875\n",
      "Epoch 565/3000, Loss: 18577.712890625, Val Loss: 33190.14453125\n",
      "Epoch 566/3000, Loss: 18536.509765625, Val Loss: 33174.04296875\n",
      "Epoch 567/3000, Loss: 18496.40625, Val Loss: 33157.9296875\n",
      "Epoch 568/3000, Loss: 18455.8125, Val Loss: 33140.0\n",
      "Epoch 569/3000, Loss: 18414.79296875, Val Loss: 33121.1640625\n",
      "Epoch 570/3000, Loss: 18374.626953125, Val Loss: 33101.984375\n",
      "Epoch 571/3000, Loss: 18333.775390625, Val Loss: 33085.98828125\n",
      "Epoch 572/3000, Loss: 18292.734375, Val Loss: 33072.859375\n",
      "Epoch 573/3000, Loss: 18252.283203125, Val Loss: 33060.52734375\n",
      "Epoch 574/3000, Loss: 18211.423828125, Val Loss: 33046.12109375\n",
      "Epoch 575/3000, Loss: 18170.17578125, Val Loss: 33030.05078125\n",
      "Epoch 576/3000, Loss: 18129.4765625, Val Loss: 33014.13671875\n",
      "Epoch 577/3000, Loss: 18088.34375, Val Loss: 32998.69921875\n",
      "Epoch 578/3000, Loss: 18047.1953125, Val Loss: 32984.453125\n",
      "Epoch 579/3000, Loss: 18005.845703125, Val Loss: 32969.6953125\n",
      "Epoch 580/3000, Loss: 17964.65234375, Val Loss: 32955.76953125\n",
      "Epoch 581/3000, Loss: 17923.240234375, Val Loss: 32939.1953125\n",
      "Epoch 582/3000, Loss: 17881.94921875, Val Loss: 32923.90625\n",
      "Epoch 583/3000, Loss: 17840.53515625, Val Loss: 32911.01171875\n",
      "Epoch 584/3000, Loss: 17799.0859375, Val Loss: 32898.89453125\n",
      "Epoch 585/3000, Loss: 17757.271484375, Val Loss: 32886.5078125\n",
      "Epoch 586/3000, Loss: 17715.763671875, Val Loss: 32877.015625\n",
      "Epoch 587/3000, Loss: 17673.89453125, Val Loss: 32868.0\n",
      "Epoch 588/3000, Loss: 17632.646484375, Val Loss: 32854.08203125\n",
      "Epoch 589/3000, Loss: 17590.927734375, Val Loss: 32835.91796875\n",
      "Epoch 590/3000, Loss: 17549.662109375, Val Loss: 32822.390625\n",
      "Epoch 591/3000, Loss: 17507.978515625, Val Loss: 32811.640625\n",
      "Epoch 592/3000, Loss: 17466.349609375, Val Loss: 32800.09765625\n",
      "Epoch 593/3000, Loss: 17424.779296875, Val Loss: 32787.72265625\n",
      "Epoch 594/3000, Loss: 17382.734375, Val Loss: 32774.3828125\n",
      "Epoch 595/3000, Loss: 17341.462890625, Val Loss: 32762.978515625\n",
      "Epoch 596/3000, Loss: 17299.671875, Val Loss: 32747.583984375\n",
      "Epoch 597/3000, Loss: 17258.234375, Val Loss: 32727.578125\n",
      "Epoch 598/3000, Loss: 17216.583984375, Val Loss: 32706.205078125\n",
      "Epoch 599/3000, Loss: 17175.21484375, Val Loss: 32689.3984375\n",
      "Epoch 600/3000, Loss: 17133.806640625, Val Loss: 32676.748046875\n",
      "Epoch 601/3000, Loss: 17092.205078125, Val Loss: 32662.78125\n",
      "Epoch 602/3000, Loss: 17051.01171875, Val Loss: 32645.701171875\n",
      "Epoch 603/3000, Loss: 17009.970703125, Val Loss: 32626.9921875\n",
      "Epoch 604/3000, Loss: 16969.07421875, Val Loss: 32608.416015625\n",
      "Epoch 605/3000, Loss: 16928.23046875, Val Loss: 32593.9609375\n",
      "Epoch 606/3000, Loss: 16886.970703125, Val Loss: 32577.9921875\n",
      "Epoch 607/3000, Loss: 16845.556640625, Val Loss: 32563.5703125\n",
      "Epoch 608/3000, Loss: 16804.11328125, Val Loss: 32548.576171875\n",
      "Epoch 609/3000, Loss: 16762.890625, Val Loss: 32530.904296875\n",
      "Epoch 610/3000, Loss: 16721.65625, Val Loss: 32512.7890625\n",
      "Epoch 611/3000, Loss: 16680.48828125, Val Loss: 32491.91796875\n",
      "Epoch 612/3000, Loss: 16639.75390625, Val Loss: 32471.0234375\n",
      "Epoch 613/3000, Loss: 16598.931640625, Val Loss: 32453.732421875\n",
      "Epoch 614/3000, Loss: 16558.107421875, Val Loss: 32436.669921875\n",
      "Epoch 615/3000, Loss: 16517.384765625, Val Loss: 32414.9140625\n",
      "Epoch 616/3000, Loss: 16476.115234375, Val Loss: 32394.451171875\n",
      "Epoch 617/3000, Loss: 16434.908203125, Val Loss: 32378.005859375\n",
      "Epoch 618/3000, Loss: 16393.689453125, Val Loss: 32359.134765625\n",
      "Epoch 619/3000, Loss: 16352.4443359375, Val Loss: 32340.23828125\n",
      "Epoch 620/3000, Loss: 16310.962890625, Val Loss: 32324.599609375\n",
      "Epoch 621/3000, Loss: 16269.6533203125, Val Loss: 32308.6953125\n",
      "Epoch 622/3000, Loss: 16228.1064453125, Val Loss: 32289.705078125\n",
      "Epoch 623/3000, Loss: 16187.072265625, Val Loss: 32266.185546875\n",
      "Epoch 624/3000, Loss: 16145.5537109375, Val Loss: 32245.333984375\n",
      "Epoch 625/3000, Loss: 16104.029296875, Val Loss: 32229.845703125\n",
      "Epoch 626/3000, Loss: 16062.68359375, Val Loss: 32215.884765625\n",
      "Epoch 627/3000, Loss: 16021.2744140625, Val Loss: 32206.072265625\n",
      "Epoch 628/3000, Loss: 15979.884765625, Val Loss: 32190.91015625\n",
      "Epoch 629/3000, Loss: 15938.7744140625, Val Loss: 32167.806640625\n",
      "Epoch 630/3000, Loss: 15897.318359375, Val Loss: 32146.0859375\n",
      "Epoch 631/3000, Loss: 15856.2958984375, Val Loss: 32128.79296875\n",
      "Epoch 632/3000, Loss: 15815.5380859375, Val Loss: 32108.755859375\n",
      "Epoch 633/3000, Loss: 15774.40234375, Val Loss: 32086.958984375\n",
      "Epoch 634/3000, Loss: 15733.44140625, Val Loss: 32067.927734375\n",
      "Epoch 635/3000, Loss: 15692.109375, Val Loss: 32052.005859375\n",
      "Epoch 636/3000, Loss: 15651.2919921875, Val Loss: 32032.376953125\n",
      "Epoch 637/3000, Loss: 15610.0576171875, Val Loss: 32012.361328125\n",
      "Epoch 638/3000, Loss: 15568.9365234375, Val Loss: 31995.884765625\n",
      "Epoch 639/3000, Loss: 15527.9697265625, Val Loss: 31977.017578125\n",
      "Epoch 640/3000, Loss: 15486.998046875, Val Loss: 31954.3515625\n",
      "Epoch 641/3000, Loss: 15446.0703125, Val Loss: 31930.873046875\n",
      "Epoch 642/3000, Loss: 15405.162109375, Val Loss: 31907.490234375\n",
      "Epoch 643/3000, Loss: 15364.3212890625, Val Loss: 31885.369140625\n",
      "Epoch 644/3000, Loss: 15323.685546875, Val Loss: 31863.51953125\n",
      "Epoch 645/3000, Loss: 15283.21875, Val Loss: 31849.3046875\n",
      "Epoch 646/3000, Loss: 15242.5751953125, Val Loss: 31832.361328125\n",
      "Epoch 647/3000, Loss: 15202.10546875, Val Loss: 31807.6796875\n",
      "Epoch 648/3000, Loss: 15161.33203125, Val Loss: 31781.474609375\n",
      "Epoch 649/3000, Loss: 15120.826171875, Val Loss: 31760.2578125\n",
      "Epoch 650/3000, Loss: 15080.0224609375, Val Loss: 31738.2734375\n",
      "Epoch 651/3000, Loss: 15039.4755859375, Val Loss: 31716.650390625\n",
      "Epoch 652/3000, Loss: 14998.896484375, Val Loss: 31693.607421875\n",
      "Epoch 653/3000, Loss: 14958.3173828125, Val Loss: 31674.2734375\n",
      "Epoch 654/3000, Loss: 14918.046875, Val Loss: 31650.556640625\n",
      "Epoch 655/3000, Loss: 14877.869140625, Val Loss: 31628.7734375\n",
      "Epoch 656/3000, Loss: 14837.7109375, Val Loss: 31606.662109375\n",
      "Epoch 657/3000, Loss: 14797.0927734375, Val Loss: 31586.166015625\n",
      "Epoch 658/3000, Loss: 14756.939453125, Val Loss: 31564.388671875\n",
      "Epoch 659/3000, Loss: 14716.8251953125, Val Loss: 31544.099609375\n",
      "Epoch 660/3000, Loss: 14676.349609375, Val Loss: 31525.041015625\n",
      "Epoch 661/3000, Loss: 14636.0537109375, Val Loss: 31500.05078125\n",
      "Epoch 662/3000, Loss: 14596.072265625, Val Loss: 31472.990234375\n",
      "Epoch 663/3000, Loss: 14555.591796875, Val Loss: 31447.458984375\n",
      "Epoch 664/3000, Loss: 14514.99609375, Val Loss: 31427.380859375\n",
      "Epoch 665/3000, Loss: 14474.7060546875, Val Loss: 31405.119140625\n",
      "Epoch 666/3000, Loss: 14434.43359375, Val Loss: 31387.05078125\n",
      "Epoch 667/3000, Loss: 14393.703125, Val Loss: 31368.201171875\n",
      "Epoch 668/3000, Loss: 14353.1953125, Val Loss: 31348.40234375\n",
      "Epoch 669/3000, Loss: 14313.2412109375, Val Loss: 31327.8203125\n",
      "Epoch 670/3000, Loss: 14272.76953125, Val Loss: 31307.89453125\n",
      "Epoch 671/3000, Loss: 14232.5185546875, Val Loss: 31286.4296875\n",
      "Epoch 672/3000, Loss: 14192.0166015625, Val Loss: 31261.837890625\n",
      "Epoch 673/3000, Loss: 14152.0224609375, Val Loss: 31245.66796875\n",
      "Epoch 674/3000, Loss: 14111.5595703125, Val Loss: 31229.9140625\n",
      "Epoch 675/3000, Loss: 14070.982421875, Val Loss: 31210.138671875\n",
      "Epoch 676/3000, Loss: 14030.4287109375, Val Loss: 31187.037109375\n",
      "Epoch 677/3000, Loss: 13989.900390625, Val Loss: 31166.076171875\n",
      "Epoch 678/3000, Loss: 13949.5439453125, Val Loss: 31139.6015625\n",
      "Epoch 679/3000, Loss: 13909.0927734375, Val Loss: 31111.94140625\n",
      "Epoch 680/3000, Loss: 13868.5390625, Val Loss: 31088.990234375\n",
      "Epoch 681/3000, Loss: 13827.990234375, Val Loss: 31068.177734375\n",
      "Epoch 682/3000, Loss: 13787.7509765625, Val Loss: 31046.8359375\n",
      "Epoch 683/3000, Loss: 13747.8134765625, Val Loss: 31024.177734375\n",
      "Epoch 684/3000, Loss: 13707.8564453125, Val Loss: 31005.3515625\n",
      "Epoch 685/3000, Loss: 13667.541015625, Val Loss: 30982.798828125\n",
      "Epoch 686/3000, Loss: 13628.009765625, Val Loss: 30957.6484375\n",
      "Epoch 687/3000, Loss: 13588.0087890625, Val Loss: 30941.556640625\n",
      "Epoch 688/3000, Loss: 13547.96875, Val Loss: 30923.513671875\n",
      "Epoch 689/3000, Loss: 13508.169921875, Val Loss: 30904.2890625\n",
      "Epoch 690/3000, Loss: 13468.09375, Val Loss: 30888.7421875\n",
      "Epoch 691/3000, Loss: 13427.9462890625, Val Loss: 30871.251953125\n",
      "Epoch 692/3000, Loss: 13388.2724609375, Val Loss: 30856.74609375\n",
      "Epoch 693/3000, Loss: 13348.7822265625, Val Loss: 30844.650390625\n",
      "Epoch 694/3000, Loss: 13308.9619140625, Val Loss: 30828.01953125\n",
      "Epoch 695/3000, Loss: 13268.8193359375, Val Loss: 30808.390625\n",
      "Epoch 696/3000, Loss: 13228.7587890625, Val Loss: 30790.41796875\n",
      "Epoch 697/3000, Loss: 13188.7275390625, Val Loss: 30779.197265625\n",
      "Epoch 698/3000, Loss: 13148.326171875, Val Loss: 30769.6953125\n",
      "Epoch 699/3000, Loss: 13108.4970703125, Val Loss: 30753.861328125\n",
      "Epoch 700/3000, Loss: 13067.982421875, Val Loss: 30735.0234375\n",
      "Epoch 701/3000, Loss: 13027.625, Val Loss: 30726.470703125\n",
      "Epoch 702/3000, Loss: 12985.701171875, Val Loss: 30720.439453125\n",
      "Epoch 703/3000, Loss: 12943.6484375, Val Loss: 30704.701171875\n",
      "Epoch 704/3000, Loss: 12902.3046875, Val Loss: 30686.38671875\n",
      "Epoch 705/3000, Loss: 12861.5068359375, Val Loss: 30670.828125\n",
      "Epoch 706/3000, Loss: 12819.4404296875, Val Loss: 30653.5390625\n",
      "Epoch 707/3000, Loss: 12777.6630859375, Val Loss: 30631.46484375\n",
      "Epoch 708/3000, Loss: 12735.453125, Val Loss: 30607.265625\n",
      "Epoch 709/3000, Loss: 12693.1328125, Val Loss: 30588.29296875\n",
      "Epoch 710/3000, Loss: 12651.9326171875, Val Loss: 30585.08984375\n",
      "Epoch 711/3000, Loss: 12610.041015625, Val Loss: 30582.548828125\n",
      "Epoch 712/3000, Loss: 12568.0380859375, Val Loss: 30564.47265625\n",
      "Epoch 713/3000, Loss: 12526.7421875, Val Loss: 30536.37109375\n",
      "Epoch 714/3000, Loss: 12485.76171875, Val Loss: 30515.28515625\n",
      "Epoch 715/3000, Loss: 12444.9013671875, Val Loss: 30507.2578125\n",
      "Epoch 716/3000, Loss: 12402.8515625, Val Loss: 30505.787109375\n",
      "Epoch 717/3000, Loss: 12361.5869140625, Val Loss: 30493.681640625\n",
      "Epoch 718/3000, Loss: 12320.65234375, Val Loss: 30475.080078125\n",
      "Epoch 719/3000, Loss: 12279.1552734375, Val Loss: 30454.529296875\n",
      "Epoch 720/3000, Loss: 12237.2802734375, Val Loss: 30434.955078125\n",
      "Epoch 721/3000, Loss: 12196.07421875, Val Loss: 30418.94140625\n",
      "Epoch 722/3000, Loss: 12154.6318359375, Val Loss: 30403.84375\n",
      "Epoch 723/3000, Loss: 12113.697265625, Val Loss: 30381.123046875\n",
      "Epoch 724/3000, Loss: 12072.6865234375, Val Loss: 30354.345703125\n",
      "Epoch 725/3000, Loss: 12032.1396484375, Val Loss: 30339.146484375\n",
      "Epoch 726/3000, Loss: 11992.0498046875, Val Loss: 30329.0\n",
      "Epoch 727/3000, Loss: 11951.46875, Val Loss: 30315.345703125\n",
      "Epoch 728/3000, Loss: 11911.23828125, Val Loss: 30288.650390625\n",
      "Epoch 729/3000, Loss: 11870.8916015625, Val Loss: 30255.642578125\n",
      "Epoch 730/3000, Loss: 11830.681640625, Val Loss: 30241.146484375\n",
      "Epoch 731/3000, Loss: 11791.2841796875, Val Loss: 30243.193359375\n",
      "Epoch 732/3000, Loss: 11750.591796875, Val Loss: 30240.201171875\n",
      "Epoch 733/3000, Loss: 11711.048828125, Val Loss: 30217.53515625\n",
      "Epoch 734/3000, Loss: 11671.3037109375, Val Loss: 30189.26953125\n",
      "Epoch 735/3000, Loss: 11631.55859375, Val Loss: 30176.095703125\n",
      "Epoch 736/3000, Loss: 11592.4951171875, Val Loss: 30177.068359375\n",
      "Epoch 737/3000, Loss: 11552.2685546875, Val Loss: 30180.837890625\n",
      "Epoch 738/3000, Loss: 11513.96484375, Val Loss: 30165.72265625\n",
      "Epoch 739/3000, Loss: 11474.4609375, Val Loss: 30134.056640625\n",
      "Epoch 740/3000, Loss: 11434.8720703125, Val Loss: 30112.388671875\n",
      "Epoch 741/3000, Loss: 11396.4375, Val Loss: 30105.791015625\n",
      "Epoch 742/3000, Loss: 11356.9638671875, Val Loss: 30100.6640625\n",
      "Epoch 743/3000, Loss: 11318.3046875, Val Loss: 30086.19921875\n",
      "Epoch 744/3000, Loss: 11279.0732421875, Val Loss: 30073.943359375\n",
      "Epoch 745/3000, Loss: 11240.662109375, Val Loss: 30061.291015625\n",
      "Epoch 746/3000, Loss: 11202.00390625, Val Loss: 30039.291015625\n",
      "Epoch 747/3000, Loss: 11162.7568359375, Val Loss: 30010.65625\n",
      "Epoch 748/3000, Loss: 11124.5576171875, Val Loss: 29985.630859375\n",
      "Epoch 749/3000, Loss: 11086.8505859375, Val Loss: 29970.75\n",
      "Epoch 750/3000, Loss: 11048.0498046875, Val Loss: 29959.240234375\n",
      "Epoch 751/3000, Loss: 11008.986328125, Val Loss: 29946.3359375\n",
      "Epoch 752/3000, Loss: 10971.2724609375, Val Loss: 29926.666015625\n",
      "Epoch 753/3000, Loss: 10932.900390625, Val Loss: 29903.953125\n",
      "Epoch 754/3000, Loss: 10894.3701171875, Val Loss: 29887.349609375\n",
      "Epoch 755/3000, Loss: 10855.98828125, Val Loss: 29878.927734375\n",
      "Epoch 756/3000, Loss: 10817.37890625, Val Loss: 29861.37109375\n",
      "Epoch 757/3000, Loss: 10779.3876953125, Val Loss: 29827.3046875\n",
      "Epoch 758/3000, Loss: 10740.482421875, Val Loss: 29797.41015625\n",
      "Epoch 759/3000, Loss: 10701.7275390625, Val Loss: 29771.625\n",
      "Epoch 760/3000, Loss: 10663.291015625, Val Loss: 29759.349609375\n",
      "Epoch 761/3000, Loss: 10624.5673828125, Val Loss: 29744.115234375\n",
      "Epoch 762/3000, Loss: 10585.6083984375, Val Loss: 29725.98046875\n",
      "Epoch 763/3000, Loss: 10546.7724609375, Val Loss: 29713.783203125\n",
      "Epoch 764/3000, Loss: 10508.087890625, Val Loss: 29711.408203125\n",
      "Epoch 765/3000, Loss: 10469.283203125, Val Loss: 29698.0625\n",
      "Epoch 766/3000, Loss: 10430.8544921875, Val Loss: 29674.263671875\n",
      "Epoch 767/3000, Loss: 10392.19921875, Val Loss: 29651.822265625\n",
      "Epoch 768/3000, Loss: 10354.2080078125, Val Loss: 29640.490234375\n",
      "Epoch 769/3000, Loss: 10316.279296875, Val Loss: 29625.91015625\n",
      "Epoch 770/3000, Loss: 10278.22265625, Val Loss: 29615.431640625\n",
      "Epoch 771/3000, Loss: 10240.0537109375, Val Loss: 29602.056640625\n",
      "Epoch 772/3000, Loss: 10202.2275390625, Val Loss: 29583.44140625\n",
      "Epoch 773/3000, Loss: 10165.216796875, Val Loss: 29577.71875\n",
      "Epoch 774/3000, Loss: 10127.2724609375, Val Loss: 29574.21875\n",
      "Epoch 775/3000, Loss: 10089.7958984375, Val Loss: 29565.1875\n",
      "Epoch 776/3000, Loss: 10053.0556640625, Val Loss: 29542.509765625\n",
      "Epoch 777/3000, Loss: 10015.1513671875, Val Loss: 29513.78125\n",
      "Epoch 778/3000, Loss: 9977.9296875, Val Loss: 29488.087890625\n",
      "Epoch 779/3000, Loss: 9941.185546875, Val Loss: 29473.72265625\n",
      "Epoch 780/3000, Loss: 9903.6494140625, Val Loss: 29474.49609375\n",
      "Epoch 781/3000, Loss: 9867.0732421875, Val Loss: 29464.033203125\n",
      "Epoch 782/3000, Loss: 9829.6142578125, Val Loss: 29435.185546875\n",
      "Epoch 783/3000, Loss: 9793.4033203125, Val Loss: 29422.798828125\n",
      "Epoch 784/3000, Loss: 9756.8662109375, Val Loss: 29428.02734375\n",
      "Epoch 785/3000, Loss: 9720.083984375, Val Loss: 29423.990234375\n",
      "Epoch 786/3000, Loss: 9683.6435546875, Val Loss: 29402.33203125\n",
      "Epoch 787/3000, Loss: 9646.671875, Val Loss: 29377.939453125\n",
      "Epoch 788/3000, Loss: 9609.978515625, Val Loss: 29363.7109375\n",
      "Epoch 789/3000, Loss: 9573.93359375, Val Loss: 29361.80078125\n",
      "Epoch 790/3000, Loss: 9537.7734375, Val Loss: 29359.380859375\n",
      "Epoch 791/3000, Loss: 9502.7587890625, Val Loss: 29344.37109375\n",
      "Epoch 792/3000, Loss: 9466.22265625, Val Loss: 29314.904296875\n",
      "Epoch 793/3000, Loss: 9430.619140625, Val Loss: 29304.416015625\n",
      "Epoch 794/3000, Loss: 9394.849609375, Val Loss: 29306.947265625\n",
      "Epoch 795/3000, Loss: 9358.8115234375, Val Loss: 29310.294921875\n",
      "Epoch 796/3000, Loss: 9323.158203125, Val Loss: 29300.41796875\n",
      "Epoch 797/3000, Loss: 9287.03125, Val Loss: 29282.1484375\n",
      "Epoch 798/3000, Loss: 9250.5966796875, Val Loss: 29261.796875\n",
      "Epoch 799/3000, Loss: 9216.15625, Val Loss: 29252.232421875\n",
      "Epoch 800/3000, Loss: 9180.783203125, Val Loss: 29252.935546875\n",
      "Epoch 801/3000, Loss: 9144.7236328125, Val Loss: 29267.62890625\n",
      "Epoch 802/3000, Loss: 9109.23046875, Val Loss: 29284.5546875\n",
      "Epoch 803/3000, Loss: 9074.1845703125, Val Loss: 29283.111328125\n",
      "Epoch 804/3000, Loss: 9038.29296875, Val Loss: 29258.880859375\n",
      "Epoch 805/3000, Loss: 9002.111328125, Val Loss: 29232.5859375\n",
      "Epoch 806/3000, Loss: 8965.6689453125, Val Loss: 29198.75390625\n",
      "Epoch 807/3000, Loss: 8930.72265625, Val Loss: 29191.2109375\n",
      "Epoch 808/3000, Loss: 8895.1396484375, Val Loss: 29195.49609375\n",
      "Epoch 809/3000, Loss: 8859.6396484375, Val Loss: 29190.634765625\n",
      "Epoch 810/3000, Loss: 8824.2197265625, Val Loss: 29171.8984375\n",
      "Epoch 811/3000, Loss: 8788.1923828125, Val Loss: 29177.76953125\n",
      "Epoch 812/3000, Loss: 8752.716796875, Val Loss: 29194.099609375\n",
      "Epoch 813/3000, Loss: 8717.392578125, Val Loss: 29202.548828125\n",
      "Epoch 814/3000, Loss: 8682.3125, Val Loss: 29191.326171875\n",
      "Epoch 815/3000, Loss: 8646.99609375, Val Loss: 29179.83984375\n",
      "Epoch 816/3000, Loss: 8611.75, Val Loss: 29168.123046875\n",
      "Epoch 817/3000, Loss: 8577.1611328125, Val Loss: 29173.140625\n",
      "Epoch 818/3000, Loss: 8542.33984375, Val Loss: 29174.4609375\n",
      "Epoch 819/3000, Loss: 8506.6259765625, Val Loss: 29158.638671875\n",
      "Epoch 820/3000, Loss: 8472.783203125, Val Loss: 29147.166015625\n",
      "Epoch 821/3000, Loss: 8437.3828125, Val Loss: 29153.419921875\n",
      "Epoch 822/3000, Loss: 8402.1513671875, Val Loss: 29170.41796875\n",
      "Epoch 823/3000, Loss: 8366.5166015625, Val Loss: 29167.79296875\n",
      "Epoch 824/3000, Loss: 8330.451171875, Val Loss: 29161.865234375\n",
      "Epoch 825/3000, Loss: 8293.685546875, Val Loss: 29155.953125\n",
      "Epoch 826/3000, Loss: 8256.7822265625, Val Loss: 29163.17578125\n",
      "Epoch 827/3000, Loss: 8219.1591796875, Val Loss: 29170.01953125\n",
      "Epoch 828/3000, Loss: 8180.94189453125, Val Loss: 29177.103515625\n",
      "Epoch 829/3000, Loss: 8142.8603515625, Val Loss: 29167.12109375\n",
      "Epoch 830/3000, Loss: 8106.158203125, Val Loss: 29163.41015625\n",
      "Epoch 831/3000, Loss: 8069.0654296875, Val Loss: 29162.908203125\n",
      "Epoch 832/3000, Loss: 8031.37646484375, Val Loss: 29179.587890625\n",
      "Epoch 833/3000, Loss: 7993.04833984375, Val Loss: 29194.9140625\n",
      "Epoch 834/3000, Loss: 7954.28466796875, Val Loss: 29203.91796875\n",
      "Epoch 835/3000, Loss: 7916.39453125, Val Loss: 29195.125\n",
      "Epoch 836/3000, Loss: 7878.0283203125, Val Loss: 29180.109375\n",
      "Epoch 837/3000, Loss: 7840.14453125, Val Loss: 29180.73828125\n",
      "Epoch 838/3000, Loss: 7802.85546875, Val Loss: 29193.00390625\n",
      "Epoch 839/3000, Loss: 7766.02685546875, Val Loss: 29199.62890625\n",
      "Epoch 840/3000, Loss: 7728.02197265625, Val Loss: 29218.4296875\n",
      "Epoch 841/3000, Loss: 7690.2412109375, Val Loss: 29228.8515625\n",
      "Epoch 842/3000, Loss: 7652.94873046875, Val Loss: 29227.587890625\n",
      "Epoch 843/3000, Loss: 7614.9482421875, Val Loss: 29223.9765625\n",
      "Epoch 844/3000, Loss: 7577.42919921875, Val Loss: 29205.337890625\n",
      "Epoch 845/3000, Loss: 7540.9853515625, Val Loss: 29189.466796875\n",
      "Epoch 846/3000, Loss: 7504.80322265625, Val Loss: 29186.826171875\n",
      "Epoch 847/3000, Loss: 7467.681640625, Val Loss: 29193.22265625\n",
      "Epoch 848/3000, Loss: 7430.4677734375, Val Loss: 29202.294921875\n",
      "Epoch 849/3000, Loss: 7394.36474609375, Val Loss: 29199.861328125\n",
      "Epoch 850/3000, Loss: 7357.69482421875, Val Loss: 29206.78125\n",
      "Epoch 851/3000, Loss: 7322.78662109375, Val Loss: 29209.599609375\n",
      "Epoch 852/3000, Loss: 7287.72998046875, Val Loss: 29206.35546875\n",
      "Epoch 853/3000, Loss: 7252.7041015625, Val Loss: 29198.083984375\n",
      "Epoch 854/3000, Loss: 7217.37109375, Val Loss: 29193.10546875\n",
      "Epoch 855/3000, Loss: 7182.9560546875, Val Loss: 29212.224609375\n",
      "Epoch 856/3000, Loss: 7148.66015625, Val Loss: 29241.861328125\n",
      "Epoch 857/3000, Loss: 7114.947265625, Val Loss: 29246.4609375\n",
      "Epoch 858/3000, Loss: 7081.52587890625, Val Loss: 29251.8359375\n",
      "Epoch 859/3000, Loss: 7048.14208984375, Val Loss: 29266.267578125\n",
      "Epoch 860/3000, Loss: 7013.49853515625, Val Loss: 29301.01171875\n",
      "Epoch 861/3000, Loss: 6979.5625, Val Loss: 29332.5859375\n",
      "Epoch 862/3000, Loss: 6945.2841796875, Val Loss: 29337.306640625\n",
      "Epoch 863/3000, Loss: 6911.111328125, Val Loss: 29335.376953125\n",
      "Epoch 864/3000, Loss: 6878.234375, Val Loss: 29336.94921875\n",
      "Epoch 865/3000, Loss: 6844.93359375, Val Loss: 29345.53515625\n",
      "Epoch 866/3000, Loss: 6811.0107421875, Val Loss: 29366.482421875\n",
      "Epoch 867/3000, Loss: 6778.41162109375, Val Loss: 29384.23828125\n",
      "Epoch 868/3000, Loss: 6745.53369140625, Val Loss: 29388.89453125\n",
      "Epoch 869/3000, Loss: 6713.08251953125, Val Loss: 29383.10546875\n",
      "Epoch 870/3000, Loss: 6680.81298828125, Val Loss: 29381.3515625\n",
      "Early stopping triggered!\n",
      "Validation RMSE: 170.7254120816078\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# One-hot encoding\n",
    "train_encoded = pd.get_dummies(train_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "X = train_encoded.drop(columns=[\"ID\", \"수요량\"]).values\n",
    "y = train_encoded[\"수요량\"].values\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_val_standard = scaler.transform(X_val)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_standard)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val_standard)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# Neural Network architecture\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet(X_train_tensor.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training with early stopping\n",
    "num_epochs = 3000\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor).squeeze()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# RMSE on validation set\n",
    "val_predictions = model(X_val_tensor).squeeze().detach().numpy()\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "print(f\"Validation RMSE: {rmse}\")\n",
    "\n",
    "# Preprocessing test data\n",
    "test_encoded = pd.get_dummies(test_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "X_test = test_encoded.drop(columns=[\"ID\"]).values\n",
    "X_test_standard = scaler.transform(X_test)\n",
    "X_test_tensor = torch.FloatTensor(X_test_standard)\n",
    "\n",
    "# Predict on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "# Submission file\n",
    "submission_dl = pd.DataFrame({'ID': test_data[\"ID\"], '수요량': test_predictions})\n",
    "submission_dl.to_csv(\"./data/submission_dl.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee88126-cae9-4770-aa1d-7ed5ec809cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.7254120816078"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1974fa0-6f5f-465e-8498-dc0cc5e61a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Loss: 183189.140625, Val Loss: 190145.6875\n",
      "Epoch 2/3000, Loss: 183158.5625, Val Loss: 190147.96875\n",
      "Epoch 3/3000, Loss: 183148.1875, Val Loss: 190147.828125\n",
      "Epoch 4/3000, Loss: 183137.84375, Val Loss: 190145.859375\n",
      "Epoch 5/3000, Loss: 183106.546875, Val Loss: 190141.609375\n",
      "Epoch 6/3000, Loss: 183077.515625, Val Loss: 190135.0\n",
      "Epoch 7/3000, Loss: 183036.34375, Val Loss: 190126.046875\n",
      "Epoch 8/3000, Loss: 182988.015625, Val Loss: 190114.390625\n",
      "Epoch 9/3000, Loss: 182947.46875, Val Loss: 190099.84375\n",
      "Epoch 10/3000, Loss: 182899.359375, Val Loss: 190081.125\n",
      "Epoch 11/3000, Loss: 182847.03125, Val Loss: 190057.15625\n",
      "Epoch 12/3000, Loss: 182799.71875, Val Loss: 190026.1875\n",
      "Epoch 13/3000, Loss: 182748.5, Val Loss: 189987.484375\n",
      "Epoch 14/3000, Loss: 182705.9375, Val Loss: 189939.921875\n",
      "Epoch 15/3000, Loss: 182658.984375, Val Loss: 189882.640625\n",
      "Epoch 16/3000, Loss: 182626.15625, Val Loss: 189816.703125\n",
      "Epoch 17/3000, Loss: 182571.203125, Val Loss: 189742.421875\n",
      "Epoch 18/3000, Loss: 182529.65625, Val Loss: 189660.953125\n",
      "Epoch 19/3000, Loss: 182503.1875, Val Loss: 189570.96875\n",
      "Epoch 20/3000, Loss: 182459.84375, Val Loss: 189475.5\n",
      "Epoch 21/3000, Loss: 182420.96875, Val Loss: 189376.8125\n",
      "Epoch 22/3000, Loss: 182384.53125, Val Loss: 189275.046875\n",
      "Epoch 23/3000, Loss: 182350.4375, Val Loss: 189171.59375\n",
      "Epoch 24/3000, Loss: 182311.3125, Val Loss: 189066.65625\n",
      "Epoch 25/3000, Loss: 182298.359375, Val Loss: 188961.78125\n",
      "Epoch 26/3000, Loss: 182241.21875, Val Loss: 188858.265625\n",
      "Epoch 27/3000, Loss: 182216.1875, Val Loss: 188754.21875\n",
      "Epoch 28/3000, Loss: 182187.40625, Val Loss: 188655.75\n",
      "Epoch 29/3000, Loss: 182159.140625, Val Loss: 188559.515625\n",
      "Epoch 30/3000, Loss: 182124.078125, Val Loss: 188472.328125\n",
      "Epoch 31/3000, Loss: 182096.9375, Val Loss: 188390.125\n",
      "Epoch 32/3000, Loss: 182074.140625, Val Loss: 188312.171875\n",
      "Epoch 33/3000, Loss: 182044.75, Val Loss: 188241.75\n",
      "Epoch 34/3000, Loss: 182008.34375, Val Loss: 188183.875\n",
      "Epoch 35/3000, Loss: 181988.09375, Val Loss: 188130.5625\n",
      "Epoch 36/3000, Loss: 181958.640625, Val Loss: 188088.84375\n",
      "Epoch 37/3000, Loss: 181933.125, Val Loss: 188058.640625\n",
      "Epoch 38/3000, Loss: 181919.96875, Val Loss: 188028.0\n",
      "Epoch 39/3000, Loss: 181880.8125, Val Loss: 188009.28125\n",
      "Epoch 40/3000, Loss: 181858.875, Val Loss: 187998.984375\n",
      "Epoch 41/3000, Loss: 181840.921875, Val Loss: 187994.421875\n",
      "Epoch 42/3000, Loss: 181804.984375, Val Loss: 187990.875\n",
      "Epoch 43/3000, Loss: 181792.125, Val Loss: 187994.078125\n",
      "Epoch 44/3000, Loss: 181764.59375, Val Loss: 188000.078125\n",
      "Epoch 45/3000, Loss: 181736.75, Val Loss: 188012.078125\n",
      "Epoch 46/3000, Loss: 181719.15625, Val Loss: 188020.3125\n",
      "Epoch 47/3000, Loss: 181680.40625, Val Loss: 188032.234375\n",
      "Epoch 48/3000, Loss: 181682.125, Val Loss: 188033.9375\n",
      "Epoch 49/3000, Loss: 181643.984375, Val Loss: 188045.46875\n",
      "Epoch 50/3000, Loss: 181627.0, Val Loss: 188059.078125\n",
      "Epoch 51/3000, Loss: 181597.4375, Val Loss: 188078.984375\n",
      "Epoch 00051: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 52/3000, Loss: 181572.5, Val Loss: 188113.484375\n",
      "Epoch 53/3000, Loss: 181551.828125, Val Loss: 188146.140625\n",
      "Epoch 54/3000, Loss: 181552.671875, Val Loss: 188175.140625\n",
      "Epoch 55/3000, Loss: 181542.671875, Val Loss: 188201.34375\n",
      "Epoch 56/3000, Loss: 181525.78125, Val Loss: 188227.65625\n",
      "Epoch 57/3000, Loss: 181522.453125, Val Loss: 188252.546875\n",
      "Epoch 58/3000, Loss: 181498.703125, Val Loss: 188270.546875\n",
      "Epoch 59/3000, Loss: 181486.484375, Val Loss: 188285.71875\n",
      "Epoch 60/3000, Loss: 181469.84375, Val Loss: 188297.84375\n",
      "Epoch 61/3000, Loss: 181478.453125, Val Loss: 188306.96875\n",
      "Epoch 62/3000, Loss: 181457.75, Val Loss: 188321.484375\n",
      "Epoch 00062: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 63/3000, Loss: 181434.5625, Val Loss: 188329.59375\n",
      "Epoch 64/3000, Loss: 181434.59375, Val Loss: 188341.25\n",
      "Epoch 65/3000, Loss: 181429.25, Val Loss: 188349.25\n",
      "Epoch 66/3000, Loss: 181429.671875, Val Loss: 188352.359375\n",
      "Epoch 67/3000, Loss: 181432.703125, Val Loss: 188357.46875\n",
      "Epoch 68/3000, Loss: 181422.984375, Val Loss: 188362.90625\n",
      "Epoch 69/3000, Loss: 181416.6875, Val Loss: 188367.3125\n",
      "Epoch 70/3000, Loss: 181401.71875, Val Loss: 188371.03125\n",
      "Epoch 71/3000, Loss: 181396.84375, Val Loss: 188372.09375\n",
      "Epoch 72/3000, Loss: 181396.296875, Val Loss: 188379.4375\n",
      "Epoch 73/3000, Loss: 181397.484375, Val Loss: 188381.71875\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 74/3000, Loss: 181392.0, Val Loss: 188384.203125\n",
      "Epoch 75/3000, Loss: 181383.1875, Val Loss: 188385.125\n",
      "Epoch 76/3000, Loss: 181387.90625, Val Loss: 188382.890625\n",
      "Epoch 77/3000, Loss: 181377.796875, Val Loss: 188382.796875\n",
      "Epoch 78/3000, Loss: 181370.53125, Val Loss: 188385.296875\n",
      "Epoch 79/3000, Loss: 181361.84375, Val Loss: 188387.109375\n",
      "Epoch 80/3000, Loss: 181373.859375, Val Loss: 188383.421875\n",
      "Epoch 81/3000, Loss: 181355.765625, Val Loss: 188386.0625\n",
      "Epoch 82/3000, Loss: 181364.546875, Val Loss: 188385.71875\n",
      "Epoch 83/3000, Loss: 181352.46875, Val Loss: 188385.171875\n",
      "Epoch 84/3000, Loss: 181348.8125, Val Loss: 188383.234375\n",
      "Epoch 00084: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 85/3000, Loss: 181353.171875, Val Loss: 188382.84375\n",
      "Epoch 86/3000, Loss: 181357.078125, Val Loss: 188381.921875\n",
      "Epoch 87/3000, Loss: 181347.171875, Val Loss: 188380.625\n",
      "Epoch 88/3000, Loss: 181345.234375, Val Loss: 188379.921875\n",
      "Epoch 89/3000, Loss: 181332.421875, Val Loss: 188376.65625\n",
      "Epoch 90/3000, Loss: 181341.8125, Val Loss: 188376.609375\n",
      "Epoch 91/3000, Loss: 181344.953125, Val Loss: 188375.484375\n",
      "Epoch 92/3000, Loss: 181337.84375, Val Loss: 188373.1875\n",
      "Epoch 93/3000, Loss: 181328.546875, Val Loss: 188372.390625\n",
      "Epoch 94/3000, Loss: 181337.765625, Val Loss: 188371.34375\n",
      "Epoch 95/3000, Loss: 181334.859375, Val Loss: 188371.15625\n",
      "Epoch 00095: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 96/3000, Loss: 181337.921875, Val Loss: 188366.953125\n",
      "Epoch 97/3000, Loss: 181338.0625, Val Loss: 188364.703125\n",
      "Epoch 98/3000, Loss: 181331.71875, Val Loss: 188361.984375\n",
      "Epoch 99/3000, Loss: 181338.03125, Val Loss: 188364.1875\n",
      "Epoch 100/3000, Loss: 181335.0625, Val Loss: 188365.078125\n",
      "Epoch 101/3000, Loss: 181331.90625, Val Loss: 188365.453125\n",
      "Epoch 102/3000, Loss: 181329.84375, Val Loss: 188362.609375\n",
      "Epoch 103/3000, Loss: 181337.21875, Val Loss: 188362.703125\n",
      "Epoch 104/3000, Loss: 181345.171875, Val Loss: 188358.984375\n",
      "Epoch 105/3000, Loss: 181351.515625, Val Loss: 188356.40625\n",
      "Epoch 106/3000, Loss: 181328.640625, Val Loss: 188358.09375\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 107/3000, Loss: 181318.53125, Val Loss: 188360.375\n",
      "Epoch 108/3000, Loss: 181313.21875, Val Loss: 188357.390625\n",
      "Epoch 109/3000, Loss: 181337.75, Val Loss: 188351.53125\n",
      "Epoch 110/3000, Loss: 181318.3125, Val Loss: 188350.5625\n",
      "Epoch 111/3000, Loss: 181318.875, Val Loss: 188352.15625\n",
      "Epoch 112/3000, Loss: 181325.015625, Val Loss: 188351.6875\n",
      "Epoch 113/3000, Loss: 181322.265625, Val Loss: 188349.921875\n",
      "Epoch 114/3000, Loss: 181311.421875, Val Loss: 188347.453125\n",
      "Epoch 115/3000, Loss: 181326.75, Val Loss: 188348.421875\n",
      "Epoch 116/3000, Loss: 181334.0625, Val Loss: 188347.265625\n",
      "Epoch 117/3000, Loss: 181325.3125, Val Loss: 188347.578125\n",
      "Epoch 00117: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 118/3000, Loss: 181333.1875, Val Loss: 188348.890625\n",
      "Epoch 119/3000, Loss: 181319.0625, Val Loss: 188349.046875\n",
      "Epoch 120/3000, Loss: 181328.1875, Val Loss: 188345.21875\n",
      "Epoch 121/3000, Loss: 181322.359375, Val Loss: 188344.96875\n",
      "Epoch 122/3000, Loss: 181318.34375, Val Loss: 188345.046875\n",
      "Epoch 123/3000, Loss: 181310.25, Val Loss: 188347.53125\n",
      "Epoch 124/3000, Loss: 181318.015625, Val Loss: 188345.625\n",
      "Epoch 125/3000, Loss: 181326.046875, Val Loss: 188344.1875\n",
      "Epoch 126/3000, Loss: 181318.0, Val Loss: 188346.9375\n",
      "Epoch 127/3000, Loss: 181327.578125, Val Loss: 188345.328125\n",
      "Epoch 128/3000, Loss: 181326.390625, Val Loss: 188343.796875\n",
      "Epoch 00128: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 129/3000, Loss: 181305.96875, Val Loss: 188344.71875\n",
      "Epoch 130/3000, Loss: 181321.0, Val Loss: 188344.65625\n",
      "Epoch 131/3000, Loss: 181320.984375, Val Loss: 188345.125\n",
      "Epoch 132/3000, Loss: 181326.34375, Val Loss: 188346.4375\n",
      "Epoch 133/3000, Loss: 181327.5, Val Loss: 188347.09375\n",
      "Epoch 134/3000, Loss: 181325.15625, Val Loss: 188344.484375\n",
      "Epoch 135/3000, Loss: 181322.90625, Val Loss: 188343.96875\n",
      "Epoch 136/3000, Loss: 181313.640625, Val Loss: 188342.9375\n",
      "Epoch 137/3000, Loss: 181318.65625, Val Loss: 188342.640625\n",
      "Epoch 138/3000, Loss: 181330.09375, Val Loss: 188340.390625\n",
      "Epoch 139/3000, Loss: 181322.8125, Val Loss: 188341.265625\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 140/3000, Loss: 181303.15625, Val Loss: 188340.890625\n",
      "Epoch 141/3000, Loss: 181316.1875, Val Loss: 188341.40625\n",
      "Epoch 142/3000, Loss: 181309.484375, Val Loss: 188341.25\n",
      "Epoch 143/3000, Loss: 181324.65625, Val Loss: 188340.390625\n",
      "Epoch 144/3000, Loss: 181325.796875, Val Loss: 188339.6875\n",
      "Epoch 145/3000, Loss: 181314.484375, Val Loss: 188340.28125\n",
      "Epoch 146/3000, Loss: 181318.03125, Val Loss: 188343.0\n",
      "Epoch 147/3000, Loss: 181325.53125, Val Loss: 188341.28125\n",
      "Epoch 148/3000, Loss: 181317.359375, Val Loss: 188343.359375\n",
      "Epoch 149/3000, Loss: 181320.96875, Val Loss: 188338.8125\n",
      "Epoch 150/3000, Loss: 181324.453125, Val Loss: 188336.140625\n",
      "Epoch 00150: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 151/3000, Loss: 181319.234375, Val Loss: 188339.328125\n",
      "Epoch 152/3000, Loss: 181313.421875, Val Loss: 188341.484375\n",
      "Epoch 153/3000, Loss: 181323.5625, Val Loss: 188339.125\n",
      "Epoch 154/3000, Loss: 181304.265625, Val Loss: 188343.8125\n",
      "Epoch 155/3000, Loss: 181325.15625, Val Loss: 188343.71875\n",
      "Epoch 156/3000, Loss: 181314.046875, Val Loss: 188342.078125\n",
      "Epoch 157/3000, Loss: 181329.296875, Val Loss: 188339.375\n",
      "Epoch 158/3000, Loss: 181319.4375, Val Loss: 188339.875\n",
      "Epoch 159/3000, Loss: 181327.34375, Val Loss: 188338.8125\n",
      "Epoch 160/3000, Loss: 181313.640625, Val Loss: 188337.0\n",
      "Epoch 161/3000, Loss: 181309.8125, Val Loss: 188338.5625\n",
      "Epoch 00161: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 162/3000, Loss: 181311.53125, Val Loss: 188339.625\n",
      "Epoch 163/3000, Loss: 181318.359375, Val Loss: 188340.78125\n",
      "Epoch 164/3000, Loss: 181323.265625, Val Loss: 188341.1875\n",
      "Epoch 165/3000, Loss: 181330.171875, Val Loss: 188339.84375\n",
      "Epoch 166/3000, Loss: 181317.40625, Val Loss: 188339.125\n",
      "Epoch 167/3000, Loss: 181309.421875, Val Loss: 188344.53125\n",
      "Epoch 168/3000, Loss: 181323.5625, Val Loss: 188339.828125\n",
      "Epoch 169/3000, Loss: 181307.578125, Val Loss: 188342.203125\n",
      "Epoch 170/3000, Loss: 181328.125, Val Loss: 188339.578125\n",
      "Epoch 171/3000, Loss: 181326.0, Val Loss: 188338.515625\n",
      "Epoch 172/3000, Loss: 181318.0, Val Loss: 188337.4375\n",
      "Epoch 00172: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 173/3000, Loss: 181321.109375, Val Loss: 188338.078125\n",
      "Epoch 174/3000, Loss: 181315.875, Val Loss: 188338.71875\n",
      "Epoch 175/3000, Loss: 181310.640625, Val Loss: 188339.65625\n",
      "Epoch 176/3000, Loss: 181311.984375, Val Loss: 188342.09375\n",
      "Epoch 177/3000, Loss: 181327.390625, Val Loss: 188339.359375\n",
      "Epoch 178/3000, Loss: 181319.15625, Val Loss: 188340.3125\n",
      "Epoch 179/3000, Loss: 181336.703125, Val Loss: 188342.328125\n",
      "Epoch 180/3000, Loss: 181315.109375, Val Loss: 188341.84375\n",
      "Epoch 181/3000, Loss: 181315.78125, Val Loss: 188340.921875\n",
      "Epoch 182/3000, Loss: 181317.78125, Val Loss: 188342.0625\n",
      "Epoch 183/3000, Loss: 181313.65625, Val Loss: 188345.984375\n",
      "Epoch 00183: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 184/3000, Loss: 181317.375, Val Loss: 188346.75\n",
      "Epoch 185/3000, Loss: 181326.453125, Val Loss: 188346.09375\n",
      "Epoch 186/3000, Loss: 181315.578125, Val Loss: 188348.078125\n",
      "Epoch 187/3000, Loss: 181311.296875, Val Loss: 188349.09375\n",
      "Epoch 188/3000, Loss: 181320.03125, Val Loss: 188348.546875\n",
      "Epoch 189/3000, Loss: 181325.28125, Val Loss: 188347.734375\n",
      "Epoch 190/3000, Loss: 181315.046875, Val Loss: 188349.609375\n",
      "Epoch 191/3000, Loss: 181316.34375, Val Loss: 188348.5625\n",
      "Epoch 192/3000, Loss: 181326.515625, Val Loss: 188348.765625\n",
      "Epoch 193/3000, Loss: 181322.296875, Val Loss: 188346.125\n",
      "Epoch 194/3000, Loss: 181308.6875, Val Loss: 188346.671875\n",
      "Epoch 00194: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 195/3000, Loss: 181315.796875, Val Loss: 188347.46875\n",
      "Epoch 196/3000, Loss: 181322.109375, Val Loss: 188347.65625\n",
      "Epoch 197/3000, Loss: 181318.515625, Val Loss: 188347.359375\n",
      "Epoch 198/3000, Loss: 181310.3125, Val Loss: 188346.9375\n",
      "Epoch 199/3000, Loss: 181319.296875, Val Loss: 188344.90625\n",
      "Epoch 200/3000, Loss: 181327.921875, Val Loss: 188343.359375\n",
      "Epoch 201/3000, Loss: 181321.984375, Val Loss: 188343.34375\n",
      "Epoch 202/3000, Loss: 181325.484375, Val Loss: 188341.421875\n",
      "Epoch 203/3000, Loss: 181317.90625, Val Loss: 188343.8125\n",
      "Epoch 204/3000, Loss: 181310.875, Val Loss: 188344.65625\n",
      "Epoch 205/3000, Loss: 181325.46875, Val Loss: 188344.515625\n",
      "Epoch 00205: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 206/3000, Loss: 181313.265625, Val Loss: 188345.28125\n",
      "Epoch 207/3000, Loss: 181313.984375, Val Loss: 188345.984375\n",
      "Epoch 208/3000, Loss: 181320.53125, Val Loss: 188348.09375\n",
      "Epoch 209/3000, Loss: 181315.484375, Val Loss: 188347.90625\n",
      "Epoch 210/3000, Loss: 181318.34375, Val Loss: 188345.3125\n",
      "Epoch 211/3000, Loss: 181312.78125, Val Loss: 188346.5625\n",
      "Epoch 212/3000, Loss: 181326.875, Val Loss: 188348.75\n",
      "Epoch 213/3000, Loss: 181311.171875, Val Loss: 188345.140625\n",
      "Epoch 214/3000, Loss: 181316.953125, Val Loss: 188347.703125\n",
      "Epoch 215/3000, Loss: 181319.71875, Val Loss: 188348.015625\n",
      "Epoch 216/3000, Loss: 181323.15625, Val Loss: 188347.15625\n",
      "Epoch 00216: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch 217/3000, Loss: 181324.734375, Val Loss: 188346.390625\n",
      "Epoch 218/3000, Loss: 181314.46875, Val Loss: 188345.5\n",
      "Epoch 219/3000, Loss: 181334.140625, Val Loss: 188346.890625\n",
      "Epoch 220/3000, Loss: 181316.546875, Val Loss: 188344.890625\n",
      "Epoch 221/3000, Loss: 181323.109375, Val Loss: 188347.046875\n",
      "Epoch 222/3000, Loss: 181318.53125, Val Loss: 188348.875\n",
      "Epoch 223/3000, Loss: 181322.328125, Val Loss: 188349.28125\n",
      "Epoch 224/3000, Loss: 181326.109375, Val Loss: 188349.59375\n",
      "Epoch 225/3000, Loss: 181312.640625, Val Loss: 188347.234375\n",
      "Epoch 226/3000, Loss: 181314.6875, Val Loss: 188347.515625\n",
      "Epoch 227/3000, Loss: 181334.203125, Val Loss: 188347.34375\n",
      "Epoch 228/3000, Loss: 181318.65625, Val Loss: 188345.703125\n",
      "Epoch 229/3000, Loss: 181321.234375, Val Loss: 188345.6875\n",
      "Epoch 230/3000, Loss: 181311.15625, Val Loss: 188345.375\n",
      "Epoch 231/3000, Loss: 181316.96875, Val Loss: 188343.328125\n",
      "Epoch 232/3000, Loss: 181310.40625, Val Loss: 188345.25\n",
      "Epoch 233/3000, Loss: 181327.09375, Val Loss: 188341.984375\n",
      "Epoch 234/3000, Loss: 181328.75, Val Loss: 188339.953125\n",
      "Epoch 235/3000, Loss: 181324.625, Val Loss: 188342.59375\n",
      "Epoch 236/3000, Loss: 181318.375, Val Loss: 188346.46875\n",
      "Epoch 237/3000, Loss: 181325.3125, Val Loss: 188346.96875\n",
      "Epoch 238/3000, Loss: 181326.765625, Val Loss: 188348.03125\n",
      "Epoch 239/3000, Loss: 181314.078125, Val Loss: 188349.5625\n",
      "Epoch 240/3000, Loss: 181306.671875, Val Loss: 188351.1875\n",
      "Epoch 241/3000, Loss: 181308.328125, Val Loss: 188351.875\n",
      "Epoch 242/3000, Loss: 181316.25, Val Loss: 188350.453125\n",
      "Early stopping triggered!\n",
      "Validation RMSE: 433.5791313886556\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# One-hot encoding\n",
    "train_encoded = pd.get_dummies(train_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "X = train_encoded.drop(columns=[\"ID\", \"수요량\"]).values\n",
    "y = train_encoded[\"수요량\"].values\n",
    "\n",
    "# Feature Engineering: Add interaction between some features (as an example)\n",
    "X = np.hstack((X, X[:, 0:1] * X[:, 1:2]))  # Example interaction between first and second feature\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_val_standard = scaler.transform(X_val)\n",
    "\n",
    "# Data Augmentation: Add Gaussian noise\n",
    "noise_factor = 0.05\n",
    "X_train_standard += noise_factor * np.random.randn(*X_train_standard.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_standard)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val_standard)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# Neural Network architecture with dropout and batch normalization\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.layer5 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.layer1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm2(self.layer2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm3(self.layer3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm4(self.layer4(x)))\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet(X_train_tensor.shape[1])\n",
    "\n",
    "# Loss, optimizer with weight decay, and learning rate scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Training with early stopping and learning rate warm-up\n",
    "num_epochs = 3000\n",
    "patience = 200\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "warm_up_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Learning rate warm-up\n",
    "    if epoch < warm_up_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001 * (epoch + 1) / warm_up_epochs\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor).squeeze()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Early stopping and learning rate reduction on plateau\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# ...\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# RMSE on validation set\n",
    "val_predictions = model(X_val_tensor).squeeze().detach().numpy()\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "print(f\"Validation RMSE: {rmse}\")\n",
    "\n",
    "# Preprocessing test data\n",
    "test_encoded = pd.get_dummies(test_data, columns=[\"쇼핑몰 구분\", \"도시 유형\", \"지역 유형\", \"쇼핑몰 유형\", \"선물 유형\"])\n",
    "\n",
    "# Feature Engineering for test data (same as train data)\n",
    "X_test = test_encoded.drop(columns=[\"ID\"]).values\n",
    "X_test = np.hstack((X_test, X_test[:, 0:1] * X_test[:, 1:2]))  # Example interaction between first and second feature\n",
    "\n",
    "X_test_standard = scaler.transform(X_test)\n",
    "X_test_tensor = torch.FloatTensor(X_test_standard)\n",
    "\n",
    "# Predict on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "# Submission file\n",
    "submission_dl = pd.DataFrame({'ID': test_data[\"ID\"], '수요량': test_predictions})\n",
    "submission_dl.to_csv(\"./data/submission_d2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b18719-492e-49e0-af63-7c260fd2e69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
